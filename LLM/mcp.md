---
title: "大模型相关概念"
weight: 2
catalog: true
date: 2025-6-10 10:50:57
subtitle:
header-img:
tags:
- 大模型
catagories:
- 大模型
---

> 本文主要介绍大模型领域常用的名词概念等。

# MCP

## MCP的概念

MCP的全称是`Model Context Protocol`，即模型上下文协议。根据官网的解释，MCP 是一个开放协议，它规范了应用程序向 LLM 提供`上下文`的方式。MCP 就像 AI 应用程序的 `USB-C `端口一样。正如 USB-C 提供了一种标准化的方式将您的设备连接到各种外围设备和配件一样，MCP 也提供了一种标准化的方式将 AI 模型连接到不同的数据源和工具。

简单理解`模型上下文协议`，可以把它拆成三个部分：

- `模型`：协议的使用方即大模型工具。

- `上下文`：提供存储上下文的功能，类似于存储大模型历史交互的记忆。

- `协议`：本质是一种标准，类比与TCP协议，http协议，而MCP可以类比于http之上的业务层协议。

它的目的是 **定义模型上下文的结构、状态传递、缓存方式、状态更新机制等内容**，以支持复杂推理任务中的“状态保持”和“多轮交互”。

## 为什么需要MCP

在大模型推理中，尤其是以下场景：

- 多轮对话（如 ChatGPT）

- Streaming 推理

- 长上下文处理（如 RAG、LLM agent）

- 分布式推理流水线（分stage执行）

我们需要在模型之间、服务组件之间传递“上下文状态”（如 past key value、token buffer、history、session id）。这个时候，就需要一套 **标准的协议** 来描述和控制这些状态 —— MCP 就是为此而生。

可以将 MCP 看作在 HTTP 或 gRPC 上层的“**业务语义协议**”：

```
┌──────────────────────────────────────────────┐
│                Application (LLM API)         │
│     ┌─────────────┐                          │
│     │   MCP 协议   │  <-- 管理模型上下文状态   │
│     └─────────────┘                          │
│         ↑ 使用 protobuf / JSON               │
│     ┌─────────────┐                          │
│     │   HTTP/gRPC  │  <-- 实际传输协议        │
│     └─────────────┘                          │
│         ↑ 使用 TCP/IP                        │
└──────────────────────────────────────────────┘
```

## 类比

**MCP 是“大脑助手的记忆本”**

想象你在和一个 AI 助手（比如 ChatGPT）对话，它像一个演员在扮演各种角色。

**如果没有 MCP：**

助手每次都“失忆”，你要一遍遍重复之前的内容，它无法记得你是谁、想干嘛、聊过啥。

**如果有 MCP：**

就像它有了一本“**记忆本**”：

- 每次你对它说话，它都记下“对话历史”、“你正在聊的主题”、“你的偏好”

- 它能从记忆本中取出之前你说的话，继续展开对话

- 这个“记忆本”就由 **MCP 协议定义结构、内容和传递方式**

🧠 **形象理解**：

> MCP 就像是 AI 模型的大脑助手，负责“记住你们聊过什么”，以及“接下来该怎么回复你”。

# MCP server

MCP server是模型上下文协议（Model Context Protocol）中的一个组件，它扮演着AI模型与外部数据、工具和功能之间连接的桥梁。它就像一个工具箱，为AI模型提供各种功能，比如访问文件、调用API、执行计算等，可以实现更复杂的任务。﻿

更详细地说，MCP server是一种轻量级服务程序，负责提供数据、工具和提示，帮助AI模型理解和执行任务。它通过标准化的接口和协议，将外部能力暴露给AI模型，让AI模型能够访问各种外部资源，例如数据库、API、文件等。

mcp server 链接：

- https://mcp.ad/

# Prompt

> Prompt 就是你给大模型的“指令 + 提示 +上下文”，用来引导它输出你想要的结果。

**Prompt 是 MCP 协议中的核心内容之一**，而 **MCP 是管理 Prompt 的“协议和系统”**，用来组织、封装、传输、复用 prompt（以及上下文）以便模型推理使用。

**Prompt 是 MCP 协议中的核心内容之一**，而 **MCP 是管理 Prompt 的“协议和系统”**，用来组织、封装、传输、复用 prompt（以及上下文）以便模型推理使用。

| 维度       | Prompt                  | MCP（Model Context Protocol）   |
| -------- | ----------------------- | ----------------------------- |
| 本质       | 一段输入文本（或 token 序列）      | 一种上下文协议，组织模型输入/状态/上下文的格式与流程   |
| 作用       | 引导模型生成内容                | 管理 Prompt 和上下文的结构、状态、生命周期     |
| 是否直接传给模型 | ✅ 是模型输入的一部分             | ✅ 会生成 prompt，并交给模型            |
| 是谁的子集？   | Prompt 是 MCP 的**一部分字段** | MCP 是对 Prompt 及其上下文的**结构化封装** |
| 类比关系     | 你要说的一句话                 | 你整场对话的“聊天记录 + 状态 + 上下文管理系统”   |

示例：

Prompt：

> “你是一个翻译专家，请将下面这段话翻译为英文：‘我爱编程’”

最终形成一个这样的 MCP payload：

```json
{
"session_id": "abc123",
  "stage": "prompt",
  "inputs": {
    "prompt": "你是一个翻译专家，请将下面这段话翻译为英文：‘我爱编程’",
    "history": [...],
    "role": "user"
  },
  "generation_config": {
    "temperature": 0.8,
    "top_p": 0.95
  },
  "kv_cache_refs": [],
  "position": 128
}
```

# Token

> **Token 是大模型处理语言时的最小单位**，相当于模型的“语言颗粒度”。它可能是一个词、一个子词、一个字符，甚至是一部分单词。

人说话是用句子、单词，大模型不能直接理解这些自然语言，它必须先**把文本切割成小块（token）**，再转成数字（embedding）才能理解。

比如下面这句话：`你好，世界！`

对人来说是 几 个字，但对模型来说，它可能被切分成 **2~4 个 token**，取决于使用的 tokenizer（分词器）。

**为什么 token 很重要？**

- **计费单位**（OpenAI/Claude/其他 API）是按 token 收费的

- **上下文长度限制** 是按 token 算的，不是按“字”算的

- **模型性能 & 精度** 和 token 分布、长度密切相关

- **Prompt 工程** 就是用尽可能少的 token 实现更强的引导效果




参考：

- [Introduction - Model Context Protocol](https://modelcontextprotocol.io/introduction)
