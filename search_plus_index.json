{"./":{"url":"./","title":"序言","keywords":"","body":"Linux 学习笔记目录计算存储网络程序运维工具Git管理数据库网络工具工具技巧赞赏Linux 学习笔记 本系列是 Linux 学习笔记 更多的学习笔记请参考： Kubernetes 学习笔记 Golang 学习笔记 Linux 学习笔记 数据结构学习笔记 个人博客：www.huweihuang.com 目录 计算 CPU 内存 存储 Linux 文件系统 Linux 介绍 文件系统 文件存储结构 文件权限 磁盘 LVM的使用 磁盘命令 网络 TCP/IP TCP/IP基础 IP协议 TCP与UDP协议 Http Http基础 Http报文 Http状态码 网络命令 iptables 程序 进程 Shell 脚本 Shell简介 Shell变量 Shell运算符 Shell数组 Shell echo命令 Shell判断语句 Shell循环语句 Shell函数 Shell重定向 运维工具 Ansible的使用 Supervisor的使用 Confd的使用 NFS的使用 ceph-fuse的使用 ssh tips Git管理 Git 介绍 Git 常用命令 Git 命令分类 Git commit规范 Git 命令别名 数据库 Mysql 系统管理 数据表操作 表内容操作 Redis Redis介绍 Redis集群模式部署 Redis主从及哨兵模式部署 Redis配置详解(中文版) Redis配置详解(英文版) Memcached Memcached的使用 Memcached命令 网络工具 Nginx Nginx安装与配置 Nginx作为反向代理 Nginx http服务器 Keepalived Keepalived简介 Keepalived的安装与配置 Keepalived的相关操作 Keepalived的配置详解 工具技巧 快捷键 vscode快捷键 eclipse快捷键 chrome快捷键 tmux快捷键 iterm2 rzsz的使用 vim vim命令 vimrc配置 basic vimrc 赞赏 如果觉得文章有帮助的话，可以打赏一下，谢谢！ Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2022-08-01 10:03:40 "},"file/linux-introduction.html":{"url":"file/linux-introduction.html","title":"Linux 介绍","keywords":"","body":"1. Linux简介2. Linux体系结构3. 系统操作3.1. 登录Linux3.2. 修改密码3.3. 查看当前用户3.4. 关闭系统1. Linux简介 严格来讲，Linux（内核）是计算机软件与硬件通信之间的平台，不是真正意义上的操作系统，而一些厂家将Linux内核和GNU软件（系统软件和工具）整合起来，并提供一些安装界面和系统设定与管理工具，就构成一些发行套件（系统），例如：Ubuntu、CentOS、Red Hat、Debian等。 Linux内核版本 Linux内核版本一般格式为：x.y.zz-www，例如：Kernel2.6.15 x.y：Linux内核主版本号，y若为奇数则表示是测试版 zz：次版本好 www：代表发行号 2. Linux体系结构 Linux体系结构如下： 几个重要概念： 内核：内核是操作系统的核心。内核直接与硬件交互，并处理大部分较低层的任务，如内存管理、进程调度、文件管理等。 Shell：Shell是一个处理用户请求的工具，它负责解释用户输入的命令，调用用户希望使用的程序。 命令和工具：日常工作中，你会用到很多系统命令和工具，如cp、mv、cat和grep等。 文件和目录：Linux系统中所有的数据都被存储到文件中，这些文件被分配到各个目录，构成文件系统。 3. 系统操作 3.1. 登录Linux 登录需要输入用户名和密码，用户名和密码是区分大小写。 login : amrood amrood's password: Last login: Sun Jun 14 09:32:32 2009 from 62.61.164.73 $ 3.2. 修改密码 输入password命令后，输入原密码和新密码，确认密码即可。 $ passwd Changing password for amrood (current) Linux password:****** New Linux password:******* Retype new Linux password:******* passwd: all authentication tokens updated successfully 3.3. 查看当前用户 1、查看自己的用户名 $ whoami amrood 2、查看当前在线用户 可以使用users 、who、w命令。 $ users amrood bablu qadir $ who amrood ttyp0 Oct 8 14:10 (limbo) bablu ttyp2 Oct 4 09:08 (calliope) qadir ttyp4 Oct 8 12:09 (dent) $ w 13:58:53 up 158 days, 22:07, 3 users, load average: 0.72, 0.99, 1.11 USER TTY FROM LOGIN@ IDLE JCPU PCPU WHAT root pts/1 172.16.20.65 13:40 0.00s 0.22s 0.02s w root pts/2 172.16.20.65 Fri15 43:17m 1.04s 1.04s -bash 3.4. 关闭系统 关闭系统可以使用以下命令 命令 说明 halt 直接关闭系统 init 0 使用预先定义的脚本关闭系统，关闭前可以清理和更新有关信息 init 6 重新启动系统 poweroff 通过断电来关闭系统 reboot 重新启动系统 shutdown 安全关闭系统 一般只有root有关闭系统的权限，普通用户被赋予相应权限也可以关闭系统。 Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2023-03-06 10:57:30 "},"file/linux-file-system.html":{"url":"file/linux-file-system.html","title":"文件系统","keywords":"","body":"1. 文件系统2. 分区与目录3. 常用文件管理命令3.1. df命令3.2. du 命令4. 挂载文件系统5. 用户和群组配额1. 文件系统 文件系统就是分区或磁盘上的所有文件的逻辑集合。文件系统不仅包含着文件中的数据而且还有文件系统的结构，所有Linux 用户和程序看到的文件、目录、软连接及文件保护信息等都存储在其中。 不同Linux发行版本之间的文件系统差别很少，主要表现在系统管理的特色工具以及软件包管理方式的不同，文件目录结构基本上都是一样的。 ext2 ： 早期linux中常用的文件系统； ext3 ： ext2的升级版，带日志功能； RAMFS ： 内存文件系统，速度很快； iso9660：光盘或光盘镜像； NFS ： 网络文件系统，由SUN发明，主要用于远程文件共享； MS-DOS ： MS-DOS文件系统； FAT ： Windows XP 操作系统采用的文件系统； NTFS ： Windows NT/XP 操作系统采用的文件系统。 2. 分区与目录 文件系统位于磁盘分区中；一个硬盘可以有多个分区，也可以只有一个分区；一个分区只能包含一个文件系统。 Linux文件系统与Windows有较大的差别： Windows的文件结构是多个并列的树状结构，最顶部的是不同的磁盘（分区），如 C、D、E、F等。 Linux的文件结构是单个的树状结构，根目录是“/”，其他目录都要位于根目录下。 每次安装系统的时候我们都会进行分区，Linux下磁盘分区和目录的关系如下： 任何一个分区都必须对应到某个目录上，才能进行读写操作，称为“挂载”。 被挂载的目录可以是根目录，也可以是其他二级、三级目录，任何目录都可以是挂载点。 目录是逻辑上的区分。分区是物理上的区分。 根目录是所有Linux的文件和目录所在的地方，需要挂载上一个磁盘分区。 下图是常见的目录和分区的对应关系： () 为什么要分区，如何分区？ 可以把不同资料，分别放入不同分区中管理，降低风险。 大硬盘搜索范围大，效率低。 /home、/var、/usr/local 经常是单独分区，因为经常会操作，容易产生碎片。 为了便于定位和查找，Linux中的每个目录一般都存放特定类型的文件，下表列出了各种Linux发行版本的常见目录： 目录 说明 / 根目录，只能包含目录，不能包含具体文件。 /bin 存放可执行文件。很多命令就对应/bin目录下的某个程序，例如 ls、cp、mkdir。/bin目录对所有用户有效。 /dev 硬件驱动程序。例如声卡、磁盘驱动等，还有如 /dev/null、/dev/console、/dev/zero、/dev/full 等文件。 /etc 主要包含系统配置文件和用户、用户组配置文件。 /lib 主要包含共享库文件，类似于Windows下的DLL；有时也会包含内核相关文件。 /boot 系统启动文件，例如Linux内核、引导程序等。 /home 用户工作目录（主目录），每个用户都会分配一个目录。 /mnt 临时挂载文件系统。这个目录一般是用于存放挂载储存设备的挂载目录的，例如挂载CD-ROM的cdrom目录。 /proc 操作系统运行时，进程（正在运行中的程序）信息及内核信息（比如cpu、硬盘分区、内存信息等）存放在这里。/proc目录伪装的文件系统proc的挂载目录，proc并不是真正的文件系统。 /tmp 临时文件目录，系统重启后不会被保存。 /usr /user目下的文件比较混杂，包含了管理命令、共享文件、库文件等，可以被很多用户使用。 /var 主要包含一些可变长度的文件，会经常对数据进行读写，例如日志文件和打印队列里的文件。 /sbin 和 /bin 类似，主要包含可执行文件，不过一般是系统管理所需要的，不是所有用户都需要。 3. 常用文件管理命令 你可以通过下面的命令来管理文件： Command Description cat filename 查看文件内容。 cd dirname 改变所在目录。 cp file1 file2 复制文件或目录。 file filename 查看文件类型(binary, text, etc)。 find filename dir 搜索文件或目录。 head filename 显示文件的开头，与tail命令相对。 less filename 查看文件的全部内容，可以分页显示，比more命令要强大。 ls dirname 遍历目录下的文件或目录。 mkdir dirname 创建目录。 more filename 查看文件的全部内容，可以分页显示。 mv file1 file2 移动文件或重命名。 pwd 显示用户当前所在目录。 rm filename 删除文件。 rmdir dirname 删除目录。 tail filename 显示文件的结尾，与head命令相对。 touch filename 文件不存在时创建一个空文件，存在时修改文件时间戳。 whereis filename 查看文件所在位置。 which filename 如果文件在环境变量PATH中有定义，那么显示文件位置。 3.1. df命令 管理磁盘分区时经常会使用 df (disk free) 命令，df -k 命令可以用来查看磁盘空间的使用情况（以千字节计），例如： $df -k Filesystem 1K-blocks Used Available Use% Mounted on /dev/vzfs 10485760 7836644 2649116 75% / /devices 0 0 0 0% /devices 每一列的含义如下： 列 说明 Filesystem 代表文件系统对应的设备文件的路径名（一般是硬盘上的分区）。 kbytes 分区包含的数据块（1024字节）的数目。 used 已用空间。 avail 可用空间。 capacity 已用空间的百分比。 Mounted on 文件系统挂载点。 某些目录（例如 /devices）的 kbytes、used、avail 列为0，use列为0%，这些都是特殊（或虚拟）文件系统，即使位于根目录下，也不占用硬盘空间。 你可以结合 -h (human readable) 选项将输出信息格式化，让人更易阅读。 3.2. du 命令 du (disk usage) 命令可以用来查看特定目录的空间使用情况。 du 命令会显示每个目录所占用数据块。根据系统的不同，一个数据块可能是 512 字节或 1024 字节。举例如下： $du /etc 10 /etc/cron.d 126 /etc/default 6 /etc/dfs ... 结合 -h 选项可以让信息显示的更加清晰： $du -h /etc 5k /etc/cron.d 63k /etc/default 3k /etc/dfs ... 4. 挂载文件系统 挂载是指将一个硬件设备（例如硬盘、U盘、光盘等）对应到一个已存在的目录上。 若要访问设备中的文件，必须将文件挂载到一个已存在的目录上， 然后通过访问这个目录来访问存储设备。 这样就为用户提供了统一的接口，屏蔽了硬件设备的细节。Linux将所有的硬件设备看做文件，对硬件设备的操作等同于对文件的操作。 注意：挂载目录可以不为空，但挂载后这个目录下以前的内容将不可用。 需要知道的是，光盘、软盘、其他操作系统使用的文件系统的格式与linux使用的文件系统格式是不一样的，挂载需要确认Linux是否支持所要挂载的文件系统格式。 查看当前系统所挂载的硬件设备可以使用 mount 命令： $ mount /dev/vzfs on / type reiserfs (rw,usrquota,grpquota) proc on /proc type proc (rw,nodiratime) devpts on /dev/pts type devpts (rw) 一般约定，/mnt 为临时挂载目录，例如挂载CD-ROM、远程网络设备、软盘等。 也可以通过mount命令来挂载文件系统，语法为： mount -t file_system_type device_to_mount directory_to_mount_to 例如： 将 CD-ROM 挂载到 /mnt/cdrom 目录。 $ mount -t iso9660 /dev/cdrom /mnt/cdrom 注意：file_system_type用来指定文件系统类型，通常可以不指定，Linux会自动正确选择文件系统类型。 挂载文件系统后，就可以通过 cd、cat 等命令来操作对应文件。 可以通过 umount 命令来卸载文件系统。例如，卸载 cdrom： $ umount /dev/cdrom 不过，大部分现代的Linux系统都有自动挂载卸载功能，unmount 命令较少用到。 5. 用户和群组配额 用户和群组配额可以让管理员为每个用户或群组分配固定的磁盘空间。 管理员有两种方式来分配磁盘空间： 软限制：如果用户超过指定的空间，会有一个宽限期，等待用户释放空间。 硬限制：没有宽限期，超出指定空间立即禁止操作。 下面的命令可以用来管理配额： 命令 说明 quota 显示磁盘使用情况以及每个用户组的配额。 edquota 编辑用户和群组的配额。 quotacheck 查看文件系统的磁盘使用情况，创建、检查并修复配额文件。 setquota 设置配额。 quotaon 开启用户或群组的配额功能。 quotaoff 关闭用户或群组的配额功能。 repquota 打印指定文件系统的配额。 参考： http://c.biancheng.net/cpp/linux/ Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2023-03-06 10:57:30 "},"file/linux-file-storage.html":{"url":"file/linux-file-storage.html","title":"文件存储结构","keywords":"","body":"文件存储结构1. inode1.1. inode的大小1.2. inode号码3. 目录项4. 硬链接和软链接4.1. 硬链接4.2. 软链接文件存储结构 大部分的Linux文件系统（如ext2、ext3）规定，一个文件由目录项、inode和数据块组成 目录项：包括文件名和inode节点号。 Inode：又称文件索引节点，包含文件的基础信息以及数据块的指针。 数据块：包含文件的具体内容。 1. inode 理解inode，要从文件储存说起。文件储存在硬盘上，硬盘的最小存储单位叫做\"扇区\"（Sector），每个扇区储存512字节（相当于0.5KB）。 操作系统读取硬盘的时候，不会一个扇区一个扇区地读取，这样效率太低，而是一次性连续读取多个扇区，即一次性读取一个\"块\"（block）。这种由多个扇区组成的\"块\"，是文件存取的最小单位。，即连续八个 sector组成一个 block。 文件数据都储存在\"块\"中，那么很显然，我们还必须找到一个地方储存文件的元信息，比如文件的创建者、文件的创建日期、文件的大小等等。这种储存文件元信息的区域就叫做inode，中文译名为\"索引节点\"。 inode包含文件的元信息，具体来说有以下内容： 文件的字节数。 文件拥有者的User ID。 文件的Group ID。 文件的读、写、执行权限。 文件的时间戳，共有三个：ctime指inode上一次变动的时间，mtime指文件内容上一次变动的时间，atime指文件上一次打开的时间。 链接数，即有多少文件名指向这个inode。 文件数据block的位置。 可以用stat命令，查看某个文件的inode信息： stat demo.txt 总之，除了文件名以外的所有文件信息，都存在inode之中。 当查看某个文件时，会先从inode表中查出文件属性及数据存放点，再从数据块中读取数据。 请看文件存储结构示意图： 图片 - img 1.1. inode的大小 inode也会消耗硬盘空间，所以硬盘格式化的时候，操作系统自动将硬盘分成两个区域。一个是数据区，存放文件数据；另一个是inode区（inode table），存放inode所包含的信息。 每个inode节点的大小，一般是128字节或256字节。inode节点的总数，在格式化时就给定，一般是每1KB或每2KB就设置一个inode。假定在一块1GB的硬盘中，每个inode节点的大小为128字节，每1KB就设置一个inode，那么inode table的大小就会达到128MB，占整块硬盘的12.8%。 查看每个硬盘分区的inode总数和已经使用的数量，可以使用df -i 命令。 查看每个inode节点的大小，可以用如下命令： sudo dumpe2fs -h /dev/hda | grep \"Inode size\" 由于每个文件都必须有一个inode，因此有可能发生inode已经用光，但是硬盘还未存满的情况。这时，就无法在硬盘上创建新文件。 1.2. inode号码 每个inode都有一个号码，操作系统用inode号码来识别不同的文件。 Linux系统内部不使用文件名，而使用inode号码来识别文件。对于系统来说，文件名只是inode号码便于识别的别称或者绰号。表面上，用户通过文件名，打开文件。实际上，系统内部这个过程分成三步：首先，系统找到这个文件名对应的inode号码；其次，通过inode号码，获取inode信息；最后，根据inode信息，找到文件数据所在的block，读出数据。 使用ls -i命令，可以看到文件名对应的inode号码，例如： ls -i demo.txt 3. 目录项 Linux系统中，目录（directory）也是一种文件。打开目录，实际上就是打开目录文件。 目录文件的结构非常简单，就是一系列目录项（dirent）的列表。每个目录项，由两部分组成：所包含文件的文件名，以及该文件名对应的inode号码。 ls命令只列出目录文件中的所有文件名： ls /etc ls -i命令列出整个目录文件，即文件名和inode号码： ls -i /etc 如果要查看文件的详细信息，就必须根据inode号码，访问inode节点，读取信息。ls -l命令列出文件的详细信息。 ls -l /etc 4. 硬链接和软链接 4.1. 硬链接 一般情况下，文件名和inode号码是\"一一对应\"关系，每个inode号码对应一个文件名。但是，Linux系统允许，多个文件名指向同一个inode号码。这意味着，可以用不同的文件名访问同样的内容；对文件内容进行修改，会影响到所有文件名；但是，删除一个文件名，不影响另一个文件名的访问。这种情况就被称为\"硬链接\"（hard link）。 ln命令可以创建硬链接 ln source_file target_file 运行上面这条命令以后，源文件与目标文件的inode号码相同，都指向同一个inode。inode信息中有一项叫做\"链接数\"，记录指向该inode的文件名总数，这时就会增加1。反过来，删除一个文件名，就会使得inode节点中的\"链接数\"减1。当这个值减到0，表明没有文件名指向这个inode，系统就会回收这个inode号码，以及其所对应block区域。 这里顺便说一下目录文件的\"链接数\"。创建目录时，默认会生成两个目录项：\".\"和\"..\"。前者的inode号码就是当前目录的inode号码，等同于当前目录的\"硬链接\"；后者的inode号码就是当前目录的父目录的inode号码，等同于父目录的\"硬链接\"。所以，任何一个目录的\"硬链接\"总数，总是等于2加上它的子目录总数（含隐藏目录）,这里的2是父目录对其的“硬链接”和当前目录下的\".硬链接“。 4.2. 软链接 除了硬链接以外，还有一种特殊情况。文件A和文件B的inode号码虽然不一样，但是文件A的内容是文件B的路径。读取文件A时，系统会自动将访问者导向文件B。因此，无论打开哪一个文件，最终读取的都是文件B。这时，文件A就称为文件B的\"软链接\"（soft link）或者\"符号链接（symbolic link）。 这意味着，文件A依赖于文件B而存在，如果删除了文件B，打开文件A就会报错：\"No such file or directory\"。这是软链接与硬链接最大的不同：文件A指向文件B的文件名，而不是文件B的inode号码，文件B的inode\"链接数\"不会因此发生变化。 ln -s命令可以创建软链接 ln -s source_file target_file 参考： http://c.biancheng.net/cpp/linux/ Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2023-03-06 10:57:30 "},"file/linux-file-permission.html":{"url":"file/linux-file-permission.html","title":"文件权限","keywords":"","body":"1. Linux文件管理1.1. 文件类型1.2. 文件属性1.3. 文件的操作1.4. 标准的Linux流2. 文件权限和访问模式2.1. 查看文件权限2.2. 访问模式2.2.1. 文件访问模式2.2.2. 目录访问模式2.3. 权限的操作2.3.1. chmod2.3.2. chown2.3.3. chgrp2.4. SUID和SGID位1. Linux文件管理 Linux中的所有数据都被保存在文件中，所有的文件被分配到不同的目录。目录是一种类似于树的结构，称为文件系统。 1.1. 文件类型 1、普通文件 普通文件是以字节为单位的数据流，包括文本文件、源码文件、可执行文件等。文本和二进制对Linux来说并无区别，对普通文件的解释由处理该文件的应用程序进行。 2、目录 目录可以包含普通文件和特殊文件，目录相当于Windows和Mac OS中的文件夹。 3、设备文件 Linux 与外部设备（例如光驱，打印机，终端，modern等）是通过一种被称为设备文件的文件来进行通信。Linux 输入输出到外部设备的方式和输入输出到一个文件的方式是相同的。Linux 和一个外部设备通讯之前，这个设备必须首先要有一个设备文件存在。 设备文件和普通文件不一样，设备文件中并不包含任何数据。 设备文件有两种类型：字符设备文件和块设备文件。 字符设备文件以字母\"c\"开头。字符设备文件向设备传送数据时，一次传送一个字符。典型的通过字符传送数据的设备有终端、打印机、绘图仪、modern等。字符设备文件有时也被称为\"raw\"设备文件。 块设备文件以字母\"b\"开头。块设备文件向设备传送数据时，先从内存中的buffer中读或写数据，而不是直接传送数据到物理磁盘。磁盘和CD-ROMS既可以使用字符设备文件也可以使用块设备文件。 1.2. 文件属性 可以使用ls -al来查看当前目录下的所有文件列表。 [root@www ~]# ls -al total 156 drwxr-x--- 4 root root 4096 Sep 8 14:06 . # 当前目录 drwxr-xr-x 23 root root 4096 Sep 8 14:21 .. # 父目录 -rw------- 1 root root 1474 Sep 4 18:27 anaconda-ks.cfg -rw------- 1 root root 199 Sep 8 17:14 .bash_history -rw-r--r-- 1 root root 24 Jan 6 2007 .bash_logout -rw-r--r-- 1 root root 191 Jan 6 2007 .bash_profile -rw-r--r-- 1 root root 176 Jan 6 2007 .bashrc -rw-r--r-- 1 root root 100 Jan 6 2007 .cshrc drwx------ 3 root root 4096 Sep 5 10:37 .gconf drwx------ 2 root root 4096 Sep 5 14:09 .gconfd -rw-r--r-- 1 root root 42304 Sep 4 18:26 install.log -rw-r--r-- 1 root root 5661 Sep 4 18:25 install.log.syslog [ 1 ] [ 2 ][ 3 ][ 4 ] [ 5 ] [ 6 ] [ 7 ] [ 权限 ][文件数][所有者] [用户组][文件容量][ 修改日期 ] [ 文件名 ] 每列含义说明： 第一列：文件类型。 第二列：表示文件个数。如果是文件，那么就是1；如果是目录，那么就是该目录中文件的数目。 第三列：文件的所有者，即文件的创建者。 第四列：文件所有者所在的用户组。在Linux中，每个用户都隶属于一个用户组。 第五列：文件大小（以字节计）。 第六列：文件被创建或上次被修改的时间。 第七列：文件名或目录名。 文件类型字符 前缀 描述 - 普通文件。如文本文件、二进制可执行文件、源代码等。 b 块设备文件。硬盘可以使用块设备文件。 c 字符设备文件。硬盘也可以使用字符设备文件。 d 目录文件。目录可以包含文件和其他目录。 l 符号链接（软链接）。可以链接任何普通文件，类似于 Windows 中的快捷方式。 p 具名管道。管道是进程间的一种通信机制。 s 用于进程间通信的套接字。 隐藏文件 隐藏文件的第一个字符为英文句号或点号(.)，Linux程序（包括Shell）通常使用隐藏文件来保存配置信息。可以通过ls -a来查看所有文件，即包含隐藏文件。 常见的隐藏文件： .profile：Bourne shell (sh) 初始化脚本 .kshrc：Korn shell (ksh) 初始化脚本 .cshrc：C shell (csh) 初始化脚本 .rhosts：Remote shell (rsh) 配置文件 1.3. 文件的操作 操作 命令 创建 touch filename 编辑 vi filename 查看 cat filename 复制 cp filename copyfile 重命名 mv filename newfile 删除 rm filename filename2 统计词数 wc filename 1.4. 标准的Linux流 一般情况下，每个Linux程序运行时都会创建三个文件流（三个文件）： 标准输入流(stdin)：stdin的文件描述符为0，Linux程序默认从stdin读取数据。 标准输出流(stdout)：stdout 的文件描述符为1，Linux程序默认向stdout输出数据。 标准错误流(stderr)：stderr的文件描述符为2，Linux程序会向stderr流中写入错误信息。 2. 文件权限和访问模式 2.1. 查看文件权限 Linux每个文件都有三类权限： 所有者权限(user)：文件所有者能够进行的操作 组权限(group)：文件所属用户组能够进行的操作 外部权限（other）：其他用户可以进行的操作。 通过ls -l的命令可以查看文件权限信息。 $ls -l /home/amrood -rwxr-xr-- 1 amrood users 1024 Nov 2 00:10 myfile drwxr-xr--- 1 amrood users 1024 Nov 2 00:10 mydir 第一列-rwxr-xr--包含了文件或目录的权限。 除了第一个字符-或d分别用来表示文件或目录外，其他的九个字符可以分为三组，分别对应所有者权限，用户组权限，其他用户权限，即-|user|group|other。 每组的权限又可分为三类： 读取（r），对应权限数字4 写入（w），对应权限数字2 执行（x），对应权限数字1 使用数字表示权限： 数字 说明 权限 0 没有任何权限 --- 1 执行权限 --x 2 写入权限 -w- 3 执行权限和写入权限：1 (执行) + 2 (写入) = 3 -wx 4 读取权限 r-- 5 读取和执行权限：4 (读取) + 1 (执行) = 5 r-x 6 读取和写入权限：4 (读取) + 2 (写入) = 6 rw- 7 所有权限: 4 (读取) + 2 (写入) + 1 (执行) = 7 rwx 2.2. 访问模式 2.2.1. 文件访问模式 基本的权限有读取(r)、写入(w)和执行(x)： 读取：用户能够读取文件信息，查看文件内容。 写入：用户可以编辑文件，可以向文件写入内容，也可以删除文件内容。 执行：用户可以将文件作为程序来运行。 2.2.2. 目录访问模式 目录的访问模式和文件类似，但是稍有不同： 读取：用户可以查看目录中的文件 写入：用户可以在当前目录中删除文件或创建文件 执行：执行权限赋予用户遍历目录的权利，例如执行 cd 和 ls 命令。 2.3. 权限的操作 2.3.1. chmod chmod (change mode) 命令来改变文件或目录的访问权限，权限可以使用符号或数字来表示。 1、通过符号方式 可以使用符号来改变文件或目录的权限，你可以增加(+)和删除(-)权限，也可以指定特定权限(=)。 指定权限范围 u (user)：所有者权限 g(group)：所属用户组权限 o(other)：其他用户权限 符号 说明 + 为文件或目录增加权限 - 删除文件或目录的权限 = 设置指定的权限 示例 # 查看权限 $ls -l testfile -rwxrwxr-- 1 amrood users 1024 Nov 2 00:10 testfile # 增加权限 $chmod o+wx testfile $ls -l testfile -rwxrwxrwx 1 amrood users 1024 Nov 2 00:10 testfile # 删除权限 $chmod u-x testfile $ls -l testfile -rw-rwxrwx 1 amrood users 1024 Nov 2 00:10 testfile # 指定权限 $chmod g=rx testfile $ls -l testfile -rw-r-xrwx 1 amrood users 1024 Nov 2 00:10 testfile # 同时使用多个符号 $chmod o+wx,u-x,g=rx testfile $ls -l testfile -rw-r-xrwx 1 amrood users 1024 Nov 2 00:10 testfile 2、通过数字权限方式 数字权限依照2.1的权限说明。 示例 $ls -l testfile -rwxrwxr-- 1 amrood users 1024 Nov 2 00:10 testfile $ chmod 755 testfile $ls -l testfile -rwxr-xr-x 1 amrood users 1024 Nov 2 00:10 testfile 2.3.2. chown chown 命令是\"change owner\"的缩写，用来改变文件的所有者。 # user可以是用户名或用户ID $ chown user filelist # 例如： $ chown amrood testfile 超级用户 root 可以不受限制的更改文件的所有者和用户组，但是普通用户只能更改所有者是自己的文件或目录。 2.3.3. chgrp chgrp 命令是\"change group\"的缩写，用来改变文件所在的群组。 # group可以是用户组名或用户组ID $ chgrp group filelist # 例如： $ chgrp special testfile 2.4. SUID和SGID位 在Linux中，一些程序需要特殊权限才能完成用户指定的操作。例如密码文件/etc/shadow。 Linux 通过给程序设置SUID(Set User ID)和SGID(Set Group ID)位来赋予普通用户特殊权限。当我们运行一个带有SUID位的程序时，就会继承该程序所有者的权限；如果程序不带SUID位，则会根据程序使用者的权限来运行。 例如： $ ls -l /usr/bin/passwd -r-sr-xr-x 1 root bin 19031 Feb 7 13:47 /usr/bin/passwd* 上面第一列第四个字符不是'x'或'-'，而是's'，说明 /usr/bin/passwd 文件设置了SUID位，这时普通用户会以root用户的权限来执行passwd程序。 小写字母's'说明文件所有者有执行权限(x)，大写字母'S'说明程序所有者没有执行权限(x)。 为一个目录设置SUID和SGID位可以使用下面的命令： $ chmod ug+s dirname $ ls -l drwsr-sr-x 2 root root 4096 Jun 19 06:45 dirname Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2023-03-06 10:57:30 "},"disk/lvm-usage.html":{"url":"disk/lvm-usage.html","title":"LVM的使用","keywords":"","body":"1. LVM简介2. LVM核心概念3. LVM原理4. 格式化为LVM盘4.1. fdisk格式化2T以下磁盘4.2. parted格式化2T以上磁盘5. LVM操作5.1. 创建物理卷（PV）5.2. 创建卷组（VG）5.3. 创建逻辑卷（LV）5.4. 格式化文件系统及挂载5.5. LVM扩容6. LVM相关命令6.1. 管理 PV6.2. 管理 VG6.3. 管理 LV 本文由网络内容整理而成的笔记 1. LVM简介 LVM是逻辑盘卷管理（Logical Volume Manager）的简称，它是Linux环境下对磁盘分区进行管理的一种机制，LVM是建立在硬盘和分区之上的一个逻辑层，来提高磁盘分区管理的灵活性。 优点： 可以灵活分配和管理磁盘空间 可以对分区进行动态的扩容 可以增加新的磁盘到lvm中 2. LVM核心概念 LVM概念图： PV（Physical Volume）物理卷 磁盘分区后（还未格式化为文件系统）使用 pvcreate 命令可以将硬盘分区创建为 pv，此分区的 systemID 为8e，即为 LVM 格式的系统标识符。 VG（Volume Group）卷组 将多个 PV 组合起来，使用 vgcreate 命令创建成卷组。卷组包含了多个 PV，相当于重新整合了多个分区后得到的硬盘。虽然 VG 整合了多个 PV，但是创建 VG 时会将所有空间根据指定 PE 大小划分为多个 PE，在 LVM 模式下的存储都是以 PE 为单元，类似于文件系统的 Block。 PE（Physical Extend）物理存储单元 PE 是 VG 中的存储单元。实际存储的数据都是在 PE 存储。 LV（Logical Volume）逻辑卷 如果说VG是整合分区为硬盘，那么 LV 就是把这个硬盘重新的分区，只不过该分区是通过 VG 来划分的。VG 中有很多 PE 单元，可以指定将多少 PE 划分给一个 LV，也可以直接指定大小来划分。划分 LV 后就相当于划分了分区，只需要对 LV 进行格式化即可变成普通的文件系统。 LE（Logical extent）逻辑存储单元 LE 则是逻辑存储单元，即 LV 中的逻辑存储单元，和 PE 的大小一样。从 VG 中划分 LV，实际上是从 VG 中划分 VG 中的 PE，只不过划分 LV 后它不在称为 PE，而是 LE。 3. LVM原理 LVM 之所以能够伸缩容量，实现的方法就是讲 LV 里空闲的 PE 移出，或向 LV 中添加空闲的 PE。 4. 格式化为LVM盘 4.1. fdisk格式化2T以下磁盘 # 使用fdisk进行盘的格式化 fdisk /dev/vdb # 以下是交互输出结果 Welcome to fdisk (util-linux 2.23.2). Changes will remain in memory only, until you decide to write them. Be careful before using the write command. Device does not contain a recognized partition table Building a new DOS disklabel with disk identifier 0xadfbfcb4. Command (m for help): n # 新建分区 Partition type: p primary (0 primary, 0 extended, 4 free) e extended Select (default p): p # 待定主分区 Partition number (1-4, default 1): 1 # 序号 First sector (2048-1048575999, default 2048): # 直接回车 Using default value 2048 Last sector, +sectors or +size{K,M,G} (2048-1048575999, default 1048575999): # 直接回车 Using default value 1048575999 Partition 1 of type Linux and of size 500 GiB is set Command (m for help): p # 确认分区情况 Disk /dev/vdb: 536.9 GB, 536870912000 bytes, 1048576000 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk label type: dos Disk identifier: 0xadfbfcb4 Device Boot Start End Blocks Id System /dev/vdb1 2048 1048575999 524286976 83 Linux Command (m for help): t # 选择系统id Selected partition 1 Hex code (type L to list all codes): 8e # 8e指定的是使用LVM Changed type of partition 'Linux' to 'Linux LVM' Command (m for help): w # 保存 The partition table has been altered! Calling ioctl() to re-read partition table. Syncing disks. 4.2. parted格式化2T以上磁盘 # parted /dev/sdk GNU Parted 3.1 使用 /dev/sdk Welcome to GNU Parted! Type 'help' to view a list of commands. (parted) mktable 新的磁盘标签类型？ gpt (parted) p Model: ATA ST4000NM0035-1V4 (scsi) Disk /dev/sdk: 4001GB Sector size (logical/physical): 512B/512B Partition Table: gpt Disk Flags: Number Start End Size File system Name 标志 (parted) mkpart 分区名称？ []? 文件系统类型？ [ext2]? 起始点？ 0g 结束点？ 4000G (parted) p Model: ATA ST4000NM0035-1V4 (scsi) Disk /dev/sdk: 4001GB Sector size (logical/physical): 512B/512B Partition Table: gpt Disk Flags: Number Start End Size File system Name 标志 1 1049kB 4000GB 4000GB (parted) toggle 1 lvm (parted) p Model: ATA ST4000NM0035-1V4 (scsi) Disk /dev/sdk: 4001GB Sector size (logical/physical): 512B/512B Partition Table: gpt Disk Flags: Number Start End Size File system Name 标志 1 1049kB 4000GB 4000GB lvm (parted) quit 信息: You may need to update /etc/fstab. 5. LVM操作 # pvcreate如果提示命令不存在，则需要安装lvm2 yum install lvm2 -y 5.1. 创建物理卷（PV） # pvcreate /dev/nvme1n1p1 /dev/nvme2n1p1 Physical volume \"/dev/nvme1n1p1\" successfully created. Physical volume \"/dev/nvme2n1p1\" successfully created. # 使用pvs或者 pvdisplay 查看结果 # pvs PV VG Fmt Attr PSize PFree /dev/nvme1n1p1 lvm2 --- 931.51g 931.51g /dev/nvme2n1p1 lvm2 --- 931.51g 931.51g 5.2. 创建卷组（VG） # vgcreate vgdata /dev/nvme1n1p1 /dev/nvme2n1p1 Volume group \"vgdata\" successfully created # 使用vgs 查看vg, vgdisplay的信息 # lsblk查看 # lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT nvme0n1 259:0 0 931.5G 0 disk /pcdn_data/storage1_ssd nvme2n1 259:2 0 931.5G 0 disk └─nvme2n1p1 259:5 0 931.5G 0 part └─vgdata-data 251:2 0 1.8T 0 lvm /vgdata nvme1n1 259:1 0 931.5G 0 disk └─nvme1n1p1 259:4 0 931.5G 0 part └─vgdata-data 251:2 0 1.8T 0 lvm /vgdata 5.3. 创建逻辑卷（LV） # lvcreate -L 后面是大小， -n 后面是逻辑卷名称， vgdata对应上面的卷组 # lvcreate -L 1.8T -n data vgdata Rounding up size to full physical extent 1.80 TiB Logical volume \"data\" created. # 使用lvdisplay 查看结果 5.4. 格式化文件系统及挂载 # 查看磁盘信息 # fdisk -l 磁盘 /dev/mapper/vgdata-data：1979.1 GB, 1979124285440 字节，3865477120 个扇区 Units = 扇区 of 1 * 512 = 512 bytes 扇区大小(逻辑/物理)：512 字节 / 512 字节 I/O 大小(最小/最佳)：512 字节 / 512 字节 # 格式化成xfs, /dev/vgdata/data为上面 LV Path mkfs.xfs /dev/vgdata/data # mount mkdir -p /data mount /dev/vgdata/data /data 5.5. LVM扩容 LVM最大的优势就是其可伸缩性，伸缩性有更加偏重与扩容。扩容的实质是将 VG 中的空闲 PE 添加到 LV 中，所以只要 VG 中有空闲的 PE，就可以进行扩容。即使没有空闲 PE，也可以添加PV，将PV加入到VG中增加空闲PE。 扩容的两个关键步骤： （1）使用 lvextend 或 lvresize 添加更多的 PE 或容量到 LV （2）使用 resize2fs命令（xfs 使用 xfs_growfs）将 LV 增加后的容量添加到对应的文件系统中(此过程是修改文件系统而非LVM内容) 6. LVM相关命令 6.1. 管理 PV 功能 命令 创建 PV pvcreate 扫描并列出所有 PV pvscan 列出 PV 属性 pvdisplay {name\\ size} 移除 PV pvremove 移动 PV 中的数据 pvmove 6.2. 管理 VG 功能 命令 创建 VG vgcreate 扫描并列出所有 VG vgscan 列出 VG 属性信息 vgdisplay 移除（删除）VG vgremove 从 VG 中移除 PV vgreduce 将 PV 添加到 VG 中 vgextend 修改 VG 属性 vgchange 6.3. 管理 LV 功能 命令 创建 LV lvcreate 扫描并列出所有 LV lvscan 列出 LV 属性信息 lvdisplay 移除 LV lvremove 缩小 LV 容量 lvreduce/lvresize 增大 LV 容量 lvextend/lvresize 调整 LV 容量 lvresize lvcreate命令 一般用法：lvcreate [-L size(M/G) | -l PEnum] -n lv_name vg_name 选项： -L：根据大小创建 LV，即分配多少空间给此 LV -l：根据 PE 的数量来创建 LV，即分配多少个 PE 给此 LV -n：指定 LV 名称 参考： Linux下使用lvm将多块盘合并 | Z.S.K.'s Records 100个Linux命令(5)-LVM - 云+社区 - 腾讯云 LVM数据卷 - 容器服务 ACK - 阿里云 Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2023-03-06 10:57:30 "},"disk/disk-command.html":{"url":"disk/disk-command.html","title":"磁盘命令","keywords":"","body":"1. 判断磁盘是SSD或HDD盘2. 解决umount target is busy挂载盘卸载不掉问题1. 判断磁盘是SSD或HDD盘 1、没有使用raid方案 lsblk -d -o name,rota命令，0表示SSD，1表示HDD # lsblk -d -o name,rota NAME ROTA sda 0 sdb 1 sdc 1 2、使用raid方案 下载工具 wget https://raw.githubusercontent.com/eLvErDe/hwraid/master/wrapper-scripts/megaclisas-status 执行检测命令 $ megaclisas-status -- Controller information -- -- ID | H/W Model | RAM | Temp | BBU | Firmware c0 | SAS3508 | 2048MB | 55C | Good | FW: 50.6.3-0109 -- Array information -- -- ID | Type | Size | Strpsz | Flags | DskCache | Status | OS Path | CacheCade |InProgress c0u0 | RAID-1 | 1089G | 256 KB | RA,WB | Default | Optimal | /dev/sda | None |None c0u1 | RAID-5 | 2616G | 256 KB | RA,WB | Default | Optimal | /dev/sdb | None |None -- Disk information -- -- ID | Type | Drive Model | Size | Status | Speed | Temp | Slot ID | LSI ID c0u0p0 | HDD | TOSHIBA AL15SEB120N 080710R0A0LJFDWG | 1.089 TB | Online, Spun Up | 12.0Gb/s | 27C | [134:4] | 0 c0u0p1 | HDD | TOSHIBA AL15SEB120N 080710S0A10SFDWG | 1.089 TB | Online, Spun Up | 12.0Gb/s | 28C | [134:5] | 5 c0u1p0 | SSD | HUAWEI HWE52SS3960L005N3248033GSN10L5002816 | 893.1 Gb | Online, Spun Up | 12.0Gb/s | 29C | [134:0] | 2 c0u1p1 | SSD | HUAWEI HWE52SS3960L005N3248033GSN10L5002799 | 893.1 Gb | Online, Spun Up | 12.0Gb/s | 30C | [134:1] | 4 c0u1p2 | SSD | HUAWEI HWE52SS3960L005N3248033GSN10L5002805 | 893.1 Gb | Online, Spun Up | 12.0Gb/s | 29C | [134:2] | 1 c0u1p3 | SSD | HUAWEI HWE52SS3960L005N3248033GSN10L5002797 | 893.1 Gb | Online, Spun Up | 12.0Gb/s | 29C | [134:3] | 3 2. 解决umount target is busy挂载盘卸载不掉问题 问题描述: 由于有进程占用目录，因此无法umount目录，需要先将占用进程杀死，再umount目录。 $ umount /data umount: /data: target is busy. 查看目录占用进程： # fuser -mv /mnt/ USER PID ACCESS COMMAND /mnt: root kernel mount /mnt root 13830 ..c.. bash 杀死目录占用进程 # fuser -kv /mnt/ USER PID ACCESS COMMAND /mnt: root kernel mount /mnt root 13830 ..c.. bash # 检查目录占用进程 # fuser -mv /mnt/ # umount /mnt fuser命令参数说明 -k,--kill kill 　　processes accessing the named file -m,--mount 　　 show all processes using the named filesystems or block device -v,--verbose 　　 verbose output Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2023-03-06 10:57:30 "},"tcpip/tcpip-basics.html":{"url":"tcpip/tcpip-basics.html","title":"TCP/IP基础","keywords":"","body":"1. 基础知识1.1. 协议1.2. 地址1.3. 网络构成2. OSI与TCP/IP参考模型2.1. OSI与TCP/IP参考模型图2.2. OSI参考模型分层说明2.3. OSI参考模型通信过程2.4. TCP/IP应用层协议2.4.1. 通信模型2.4.2. 应用层协议说明3. TCP/IP通信过程3.1. 数据包结构3.2. 数据打包和解包过程3.2.1. 包的封装3.2.2. 发送与接收3.3. 数据包传输过程1. 基础知识 1.1. 协议 计算机与网络设备要相互通信，必须基于相同的方法。比如，如何探测到通信目标，使用哪种语言通信，如何结束通信等规则要事先确定。 不同硬件，操作系统之间的通信都需要一种规则，我们将这种事先约定好的规则称之为协议。 1.2. 地址 地址：在某一范围内确认的唯一标识符，即数据包传到某一个范围，需要有一个明确唯一的目标地址。 类型 层 地址 说明 端口号 传输层 程序地址 同一个计算机中不同的应用程序 IP地址 网络层 主机地址 识别TCP/IP网络中不同的主机或路由器 MAC地址 数据链路层 物理地址 在同一个数据链路中识别不同的计算机 1.3. 网络构成 构成要素 说明 网卡 连入网络必须使用网卡，又称网络接口卡。 中继器 OSI第1层，物理层上延长网络的设备，将电缆的信号放大传给另一个电缆。 网桥/2层交换机 OSI第2层，数据链路层面上连接两个网络的设备，识别数据帧的内容并转发给相邻的网段，根据MAC地址进行处理。 路由器/3层交换机 OSI第3层，网络层面连接两个网络并对分组报文进行转发，根据IP进行处理。 4-7层交换机 传输层到应用层，以TCP等协议分析收发数据，负载均衡器就是其中一种。 网关 对传输层到应用层的数据进行转换和转发的设备，通常会使用表示层或应用层的网关来处理不同协议之间的翻译和通信，代理服务器（proxy）就是应用网关的一种。 2. OSI与TCP/IP参考模型 2.1. OSI与TCP/IP参考模型图 2.2. OSI参考模型分层说明 2.3. OSI参考模型通信过程 1、打包数据时，每一层在处理上一层传过来的数据时，会在数据上附上当前层的首部信息后传给下一层； 2、解包数据时，每一层在处理下一层传过来的数据时，会将当前层的首部信息与数据分开，将数据传给上一层。 3、数据通信过程 分层 每层的操作 应用层 在数据前面加首部，首部包括数据内容、源地址和目标地址，同时也会处理异常的反馈信息。 表示层 将特有的数据格式转换为通用的数据格式，同时也会加上表示层的首部信息以供解析。 会话层 对何时连接，以何种方式连接，连接多久，何时断开等做记录。同时也会加会话层的首部信息。 传输层 建立连接，断开连接，确认数据是否发送成功和执行失败重发任务。 网络层 负责将数据发到目标地址，也包含首部信息。 数据链路层 通过物理的传输介质实现数据的传输。 物理层 将0/1转换成物理的传输介质，通过MAC地址进行传输。 2.4. TCP/IP应用层协议 2.4.1. 通信模型 2.4.2. 应用层协议说明 应用类型 协议 协议说明 WWW HTTP,HTML 电子邮件 SMTP，MIME 文件传输 FTP 远程登录 TELNET,SSH 网络管理 SNMP,MIB 3. TCP/IP通信过程 3.1. 数据包结构 3.2. 数据打包和解包过程 3.2.1. 包的封装 3.2.2. 发送与接收 3.3. 数据包传输过程 文章： 《图解TCP/IP》 Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2023-03-06 10:57:30 "},"tcpip/ip.html":{"url":"tcpip/ip.html","title":"IP协议","keywords":"","body":"1. IP基础1.1. 网络层与数据链路层的关系1.2. IP寻址1.3. 路由控制1.3.1. 路由控制表1.4. 数据链路的抽象化1.5. IP是面向无连接型2. IP地址2.1. IP地址的定义2.2. IP地址由网络和主机两部分标识组成2.3. IP地址的分类2.4. 子网隐码2.4.1. 子网隐码的表示方法3. 路由控制3.1. IP地址和路由控制3.1.1. 默认路由3.1.2. 主机路由3.1.3. 环回地址3.2. 路由控制表的聚合4. IP首部信息4.1. IPv4首部4.2. IPv6首部1. IP基础 TCP/IP的心脏是互联网层，这一层主要有IP和ICMP两个协议组成，在OSI参考模型中为第三层（网络层）。网络层的主要作用是实现终端节点之间的通信（点对点通信）。 1.1. 网络层与数据链路层的关系 图片 - 这里写图片描述 1.2. IP寻址 IP地址用于在“连接到网络中的所有主机中识别出进行通信的目标地址”。因此TCP/IP通信中所有主机或路由器必须设定自己的IP地址（每块网卡至少配置一个或以上的IP地址）。 图片 - 这里写图片描述 1.3. 路由控制 路由控制是指将分组数据发送到最终目标地址的功能。 图片 - 这里写图片描述 IP数据包类似快递中的包裹，送货车类似数据链路，包裹依赖送货车承载转运，而一辆送货车只能将包裹送到某个区间内，由新的快递点安排新的送货车来进行下一区间的运输。 图片 - 这里写图片描述 1.3.1. 路由控制表 为了将数据包发给目标主机，所有主机都维护一张路由控制表（Routing Table）,该表记录IP数据在下一步应该发给哪个路由器。IP包根据这个路由表在各个数据链路上传输。 图片 - 这里写图片描述 1.4. 数据链路的抽象化 IP是实现多个数据链路之间通信的协议。对不同数据链路的相异特性进行抽象化也是IP的重要作用之一。不同数据链路最大的区别在于它们各自的最大传输单位（MTU）不同，类似快递包裹有各自的大小限制。当数据包过大时，IP进行分片处理，即将大的IP包分成多个较小的IP包，当到目标地址后再被组合起来传给上一层。 1.5. IP是面向无连接型 IP发包之前不需要提前与目标建立连接。采用面向无连接的原因：为了简化和提速。面向连接型需要提前建立连接会降低处理速度。IP只负责将数据发给目标主机，但途中可能会发生丢包、错位、数据量翻倍等问题。TCP则是面向连接的协议，负责保证对端主机确实收到数据。 2. IP地址 在TCP/IP通信中，用IP地址识别主机和路由器。 2.1. IP地址的定义 IP地址（IPv4地址）由32位正整数来表示。IP地址在计算机内部以二进制方式被处理，但习惯将32位的IP地址以8位为一组，分成4组，每组以“.”隔开，转换成10进制来表示。IPv4地址为32位，最多允许43亿台计算机连接网络。 实际上，IP地址并非根据主机台数来分配而是每一台主机上的每一块网卡都得设置IP地址，一块网卡可以设置一个或以上个IP,路由器通常会配置两个以上的网卡。 2.2. IP地址由网络和主机两部分标识组成 IP地址由“网络地址”和“主机地址”两部分组成。 网络标识在数据链路的每个段配置不同的值，必须保证相互连接的每个段的地址不重复，相同段内连接的主机必须有相同的网络地址。主机标识则不允许同一个网段内重复出现。在某一范围内，IP地址需具有唯一性。 图片 - 这里写图片描述 IP包被转发到某个路由器时，是利用目标IP地址的网络标识进行路由，即使不看主机地址，由网络地址则可判断是否是该网段内的主机。 图片 - 这里写图片描述 2.3. IP地址的分类 IP地址分为A、B、C、D四类。 IP地址类别 地址开头 网络地址 主机地址 范围 一个网段内主机地址个数 备注 A类地址 0 第1-8位 后24位 0.0.0.0~127.0.0.0 2^24-2=16777214 B类地址 10 第1-16位 后16位 128.0.0.0~191.255.0.0 2^16-2=65534 C类地址 110 第1-24位 后8位 192.0.0.0~239.255.255.0 2^8-2=254 D类地址 1110 第1-32位 没有主机地址 224.0.0.0~239.255.255.255 常用于多播 注意：同一个网段中的主机地址分配，主机地址全为0表示对应的网络地址，主机地址全为1通常用于广播地址。因此一个网段内主机的个数去掉2个（例如2^8-2=254）。 图片 - 这里写图片描述 2.4. 子网隐码 用1表示网络地址的范围，用0表示主机地址的访问。因此A、B、C类可表示为 IP类别 表示 A类 255.0.0.0 B类 255.255.0.0 C类 255.255.255.0 按照以上的组合方式IP有点浪费，因此产生子网隐码的分类方法减少这种浪费。 引入子网后，IP地址由两种识别码组成：IP地址本身+表示网络地址的子网隐码。即将A,B,C类中的主机地址拆成网络部分和主机部分，重新分配网络地址和主机地址。子网隐码同样是用1表示网络地址的范围，用0表示主机地址的访问。 2.4.1. 子网隐码的表示方法 表示方法 地址 子网隐码 备注 数字 IP地址 172.20.100.52 255.255.255.192 网络地址 172.20.100.0 255.255.255.192 广播地址 172.20.100.63 255.255.255.192 “/26”，表示前26位为网络地址 IP地址 172.20.100.52/26 网络地址 172.20.100.0/26 广播地址 172.20.100.63/26 图片 - 这里写图片描述 3. 路由控制 发送数据包除了有目标IP地址外，还需要指明路由器和主机的信息，即路由控制表。 路由控制表的形成方式有两种： 1、静态路由 由管理员手动设置 2、动态路由 路由器与其他路由器相互交互信息时自动刷新。为了让动态路由及时刷新路由表，在网络上互联的路由器之间需设置路由协议，保证正常读取路由控制信息。 3.1. IP地址和路由控制 图片 - 这里写图片描述 IP地址的网络地址部分用于进行路由控制。路由控制表中记录着网络地址与下一步应该发送至路由器的地址。在发送IP包时，首先确认IP包首部中的目标地址，再从路由控制表中找到与该地址具有相同网络地址的记录，根据该记录将IP包转发给相应的下一个路由器。如果存在多条相同网络地址的记录，则选择最为吻合的网络地址（相同位数最多）。例如：172.20.100.52的网络地址与172.20.0/16和172.20.100.0/24都匹配，则选择匹配最长的172.20.100.0/24。 3.1.1. 默认路由 默认路由一般标记为0.0.0.0/0或default。当路由表中没有任何一个地址与之匹配的记录，则使用默认路由。 3.1.2. 主机路由 “IP地址/32”也被称为主机路由，即整个IP地址的所有位都参与路由。进行主机路由意味着基于主机上网卡配置的IP地址本身而不是基于该地址的网络地址进行路由。一般用于不希望通过网络地址路由的情况。使用主机路由会导致路由表膨大，路由负荷增加，网络性能下降。 3.1.3. 环回地址 环回地址是在同一台计算机上程序之间进行网络通信时所使用的一个默认地址。即IP地址为127.0.0.1，主机名为localhost。 3.2. 路由控制表的聚合 路由信息的聚合可以有效的减少路由表的条目。路由表越大，管理它所需要的内存和CPU就越多，查找路由表的时间越长，导致转发IP包性能下降。要构建高性能网络就需要尽可能减少路由表的大小。 图片 - 这里写图片描述 4. IP首部信息 IP进行通信时，需要在数据前面加入IP首部信息，IP首部包含着用于IP协议进行发包控制时所有的必要信息。 4.1. IPv4首部 图片 - 这里写图片描述 字段 说明 大小 版本 标识IP首部的版本号，IPv4，即版本号为4 4比特 首部长度 表示IP首部的大小，单位为4字节 4比特 区分服务 表示服务质量 8比特 DSCP段与ECN段 DSCP用来进行质量控制，值越大优先度越高；ECN用来报告网络拥堵情况 2比特 总长度 表示IP首部与数据部分合起来的总字节数 16比特 标识 用于分片重组，同一个分片标识值相同，不同分片的标识值不同 16比特 标志 表示包被分片的相关信息 3比特 片偏移 用来标识被分片的每一个分段相对于原始数据的位置。 13比特 生存时间（TTL） 本意为包的生存期限，一般表示可以中转多少个路由器，每经过一个路由器TTL减1，直到变为0则丢弃该包 8比特 协议 表示IP首部的下一个首部隶属于哪个协议。 8比特 首部校验和 IP首部校验和，用来确保IP数据报不被破坏。 16比特 源地址 发送端IP地址 32比特 目标地址 接收端IP地址 32比特 可选项 安全级别、源路径、路径记录、时间戳 填充 填补物，调整大小使用 数据 存入数据 4.2. IPv6首部 图片 - 这里写图片描述 字段 说明 版本 IPv6,版本为6 通信量类 相当于IPv4的TOS（Type Of Service）字段 流标号 用于服务质量控制 有效载荷长度 包的数据部分 下一个首部 相当于IPv4的协议字段 跳数限制 Hop Limit，同IPv4的TTL,表示可通过的路由器个数 源地址 发送端的IP地址 目标地址 接收端的IP地址 参考 《图解TCP/IP》 Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2023-03-06 10:57:30 "},"tcpip/tcp-udp.html":{"url":"tcpip/tcp-udp.html","title":"TCP与UDP协议","keywords":"","body":"1. 传输层的作用1.1. 传输层的定义1.2. 通信处理1.3. TCP和UDP1.3.1. TCP1.3.2. UDP1.3.3. 套接字2. 端口号2.1. 端口号的定义2.2. 根据端口号识别应用2.3. 通过IP地址、端口号、协议号进行通信2.4. 端口号如何确定2.4.1. 标准既定的端口号2.4.2. 时序分配法3. UDP4. TCP1. 传输层的作用 1.1. 传输层的定义 IP首部有个协议字段，用来标识传输层协议，识别数据是TCP的内容还是UDP的内容。同样，传输层，为了识别数据应该发给哪个应用也设定了这样的编号，即端口。 图片 - 这里写图片描述 1.2. 通信处理 应用协议大多以C/S形式运行，即服务端需提前启动服务，监听某个端口，当客户端往该端口发送数据时，可以及时处理请求。 服务端程序在UNIX系统中称为守护进程，例如HTTP的服务端程序为httpd；ssh的服务端程序为sshd。UNIX中不必要逐个启动这些守护进程，而是由超级守护进程inetd(互联网守护进程)启动，当收到客户端请求时会创建（fork）新的进程并转换（exec）为httpd等各个守护进程。根据请求端口分配到对应的服务端守护进程上处理。 图片 - 这里写图片描述 1.3. TCP和UDP 1.3.1. TCP TCP是面向连接、可靠的数据流。流就是不间断的数据结构，可理解为水管中的水流。虽然可以保证发送顺序，但犹如没有间隔的发送数据流给接收端。例如：发送10次100字节的消息，接收端可能会收到一个1000字节连续不断的数据。TCP为实现可靠传输，实行“顺序控制”和“重发控制”；还具备“流量控制”、“拥塞控制”、提高网络利用率等。TCP可以类比为“打电话”，有去有回。 1.3.2. UDP UDP是不具备可靠性的数据报协议，可以确保发送消息的大小，但不能保证消息一定到达，应用有时会根据自己的需要进行重发处理。UDP可以类比“发短信”，有去无回。 1.3.3. 套接字 应用在使用TCP或UDP时会用到系统提供的类库，即API（应用编程接口），通信时会用到套接字（socket）的API。应用程序利用套接字，可以设置对端的IP地址、端口号，并实现数据的发送与接收。 图片 - 这里写图片描述 2. 端口号 2.1. 端口号的定义 类别 地址 层 说明 端口号 程序地址 传输层 同一个计算机中不同的应用程序 IP地址 主机地址 网络层 识别TCP/IP网络中不同的主机或路由器 MAC地址 物理地址 数据链路层 在同一个数据链路中识别不同的计算机 把数据传输比作快递传递；IP地址就像你的家庭地址；那么端口号相当于你家具体的收件人；知道了家庭地址和收件人才能将快递准确送达。 2.2. 根据端口号识别应用 图片 - 这里写图片描述 图片 - 这里写图片描述 2.3. 通过IP地址、端口号、协议号进行通信 5个信息唯一标识一个通信：源地址IP、目标地址IP、协议号、源端口号、目标端口号。 图片 - 这里写图片描述 2.4. 端口号如何确定 2.4.1. 标准既定的端口号 该方法也叫静态方法，是指每个应用程序都有其指定的端口号。例如HTTP、FTP等应用协议使用的端口号，这类端口号称为知名端口号，一般由0-1023的数字分配而成。除知名端口号外，还有一些端口号也被正式注册，分布在1024-49151的数字之间。这些端口可用于任何通信用途。 2.4.2. 时序分配法 该方法也叫动态分配法，服务端有必要确定监听端口号，但接受服务的客户端没必要确定端口号。客户端可以不用自己设置端口号，由操作系统进行分配。操作系统为每个应用程序分配互不冲突的端口号。例如，新增一个端口号则在之前的端口号上加1，动态分配的端口号取值范围：49152-65535。 3. UDP UDP:User Datagram Protocol的缩写，提供面向无连接的通信服务，在应用程序发来数据收到那一刻则立即原样发送到网络上。即使出现丢包也不负责重发，包出现乱序也不能纠正。 UDP可以随时发送数据，本身处理简单高效，但不具备可靠性，适合以下场景： 包总量较少的通信（DNS、SNMP等） 视频、音频等多媒体通信（即使通信） 限定于LAN等特定网络中的应用通信 广播通信（广播、多播） 4. TCP TCP:Transmission Control Protocol (传输控制协议)，TCP实现了数据传输时的各种控制功能，可以进行丢包重发，乱序纠正，控制通信流量的浪费。 待续。。。 参考： 《图解TCP/IP》 Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2023-03-06 10:57:30 "},"tcpip/http-basics.html":{"url":"tcpip/http-basics.html","title":"Http基础","keywords":"","body":"1. web及网络基础1.1. 通过HTTP访问web[C/S]1.2. TCP/IP四层模型1.2.1. 数据包的封装1.3. TCP/IP协议族1.3.1. 负责传输的IP协议1.3.2. 确保可靠的TCP协议1.3.3. 负责域名解析的DNS服务1.3.4. 各协议与HTTP的关系1.4. URI与URL1.4.1. URI的格式1.4.2. URI的示例2. HTTP协议2.1. 通过请求和响应的交换达成通信2.1.1. 请求报文2.1.2. 响应报文2.2. HTTP请求方法2.2.1. GET:获取资源2.2.2. POST:传输实体主体2.2.3. PUT:传输文件2.2.4. HEAD:获取报文头部2.2.5. DELETE:删除文件2.2.6. OPTIONS:询问支持的方法2.2.7. TRACE:追踪路径2.2.8. CONNECT:要求用隧道协议连接代理2.3. 持久连接2.3.1. keep-alive2.3.2. 管线化2.3.3. 使用cookie的状态管理1. web及网络基础 1.1. 通过HTTP访问web[C/S] 1.2. TCP/IP四层模型 1.2.1. 数据包的封装 1.3. TCP/IP协议族 1.3.1. 负责传输的IP协议 使用ARP协议凭借MAC地址通信 1.3.2. 确保可靠的TCP协议 1.3.3. 负责域名解析的DNS服务 1.3.4. 各协议与HTTP的关系 1.4. URI与URL URI(Uniform Resource Identifier):统一资源标识符 URL(Uniform Resource Locator):统一资源定位符；URL是URI的子集 1.4.1. URI的格式 字段 说明 协议 http/https 登录信息（认证） user:pass@(一般没有) 服务器地址 域名或IP 服务器端口号 服务端口号，省略则取默认端口号 带层次的文件路径 指定服务器上的文件路径来定位特指的资源 查询字符串 使用查询字符串传入参数 片段标识符 标记以获取资源中的子资源（文档内的某个位置） 1.4.2. URI的示例 2. HTTP协议 2.1. 通过请求和响应的交换达成通信 2.1.1. 请求报文 2.1.2. 响应报文 2.2. HTTP请求方法 2.2.1. GET:获取资源 2.2.2. POST:传输实体主体 2.2.3. PUT:传输文件 PUT方法用来传输文件，像FTP协议一样，要求在请求报文的主体中包含文件内容，然后保存到请求URI指定的位置。 因为自身不带验证机制，有安全问题，因此一般不采用。若配合验证机制或者REST标准则可使用。 2.2.4. HEAD:获取报文头部 HEAD和GET一样但不返回报文主体部分，用于确认URI的有效性及资源的更新时间等。 2.2.5. DELETE:删除文件 DELETE与PUT作用相反，但不带安全验证机制一般不采用。 2.2.6. OPTIONS:询问支持的方法 OPTIONS用来查询针对请求URI指定的资源支持的方法 2.2.7. TRACE:追踪路径 TRACE用来查询发送出去的请求是怎样被加工修改/篡改的，因为易引发XST（跨站追踪）攻击，一般不使用。 2.2.8. CONNECT:要求用隧道协议连接代理 CONNECT要求在与代理服务器通信时建立隧道，实现用隧道协议进行TCP通信。主要使用SSL（Source Sockets Layer:安全套接字）和TLS（Transport Layer Security:传输层安全）协议把通信内容加密后经网络隧道传输。 方法格式如下： 2.3. 持久连接 2.3.1. keep-alive 为解决每进行一次HTTP通信就要断开一次TCP连接，增加了通信量的开销，HTTP/1.1通过keep-alive持久连接，只要任意一端没有明确提出断开连接，则保持TCP连接状态。 持久连接减少了TCP连接的重复建立和断开所造成的额外开销，减轻了服务器端的负载。 2.3.2. 管线化 持续连接使得多数请求以管线化（pipelining）方式发送成为可能。管线化即同时并行发送多个请求，而不需要一个接一个等待响应。管线化技术比持续连接速度快，请求数越多越明显。 2.3.3. 使用cookie的状态管理 HTTP是无状态协议，不对之前发生过的请求和响应的状态进行管理，即无法根据之前的状态进行本次的请求处理。无状态协议的优点在于不必保存状态，减少服务器CPU及内存资源的消耗。 cookie技术通过在请求和响应报文中写入cookie信息来控制客户端的状态。cookie会根据从服务端发送的响应报文内的一个叫做Set-Cookie的首部字段通知客户端保存Cookie；当客户端再往服务端发送请求时，客户端自动在请求报文中加入Cookie值后发送出去。服务器发现Cookie后会检查从哪个客户端发送来的连接请求，对比服务器上的记录，最后得到之前的状态信息。 参考： 《图解HTTP》 Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2023-03-06 10:57:30 "},"tcpip/http-message.html":{"url":"tcpip/http-message.html","title":"Http报文","keywords":"","body":"3. HTTP报文3.1. HTTP报文3.2. 报文结构3.3. 编码提升传输速率3.3.1. 报文主体和实体主体的差异3.3.2. 压缩传输的内容编码3.3.3. 分块传输编码3.4. 发送多种数据的多部分对象集合3.5. 获取部分内容的范围请求3.6. 内容协商返回最合适的内容3. HTTP报文 3.1. HTTP报文 用于HTTP协议交互的信息被称为HTTP报文，客户端的HTTP报文叫做请求报文，服务端的叫做响应报文。报文大致分为报文首部和报文主体，但并不一定要有报文主体。 图片 - img 3.2. 报文结构 图片 - img 图片 - img 字段 说明 请求行 请求方法，请求URI和HTTP版本 状态行 响应结果的状态码，原因短语和HTTP版本 首部字段 请求和响应的各种条件和属性的各类首部：通用首部、请求首部、响应首部、实体首部 其他 HTTP的RFC里未定义的首部（Cookie等） 3.3. 编码提升传输速率 HTTP在传输数据时可以按照数据原貌直接传输也可以在传输过程中编码提升传输速率；通过编码可以处理大量请求但会消耗更多的CPU等资源。 3.3.1. 报文主体和实体主体的差异 报文：是HTTP通信中的基本单位，由8位组字节流组成，通过HTTP通信传输。 实体：作为请求或响应的有效载荷数据被传输，其内容由实体首部和实体主体组成。 通常报文主体等于实体主体，但当传输中进行编码时，实体主体的内容发生变化才会与报文主体产生差异。 3.3.2. 压缩传输的内容编码 HTTP中的内容编码指明应用在实体内容上的编码格式，并保持实体信息原样压缩，内容编码后的实体由客户端接收并负责解码。 图片 - img 常用的内容编码： gzip(GNU ZIP) compress(UNIX系统的标准压缩) deflate(zlib) identity(不进行编码) 3.3.3. 分块传输编码 分块传输编码会将实体主体分成多个块，每一块都会用十六进制来标记快的大小，而实体的最后一块会使用“0（CR+LF）”来标记。 由接收的客户端负责解码，回复到编码前的实体主体。 图片 - img 3.4. 发送多种数据的多部分对象集合 HTTP中的多部分对象集合即发送一份报文主体内可含有多类型实体，通常是图片或文本文件上传等。 多部分对象集合包含的对象： multipart/form-data:在web表单文件上传时使用 multipart/byteranges：状态码206响应报文包含了多个范围的内容时使用 3.5. 获取部分内容的范围请求 指定范围发送的请求叫做范围请求，对于一份10000字节大小的资源，如果使用范围请求，可以只请求5001-10000字节内的资源。 图片 - img 执行范围请求时，会用到首部字段Range来指定资源的byte范围 图片 - img 3.6. 内容协商返回最合适的内容 内容协商机制是指客户端和服务端就响应的资源内容进行交涉，然后提供给客户端最合适的资源。内容协商会以响应资源的语言、编码方式等作为判断的基准。 内容协商类型： 服务器驱动协商 客户端驱动协商 透明协商 参考： 《图解HTTP》 Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2023-03-06 10:57:30 "},"tcpip/http-code.html":{"url":"tcpip/http-code.html","title":"Http状态码","keywords":"","body":"4. HTTP状态码4.1. 2XX成功4.1.1. 200 OK4.1.2. 204 No Content4.1.3. 206 Partial Content4.2. 3XX 重定向4.2.1. 301 Moved Permanently4.2.2. 302 Found4.2.3. 303 See Other4.2.4. 304 Not Modified4.2.5. 307 Temporary Redirect4.3. 4XX 客户端错误4.3.1. 400 Bad Request4.3.2. 401 Unauthorized4.3.3. 403 Forbidden4.3.4. 404 No Found4.4. 5XX 服务器错误4.4.1. 500 Internal Server Error4.4.2. 503 Service Unavailable4. HTTP状态码 状态码即服务器返回的请求结果。 状态码 类型 说明 1xx Informational(信息性状态码) 接收的请求正在处理 2xx Success(成功) 请求正常处理完毕 3xx Redirection(重定向) 需要进行附加操作以完成请求 4xx Client Error(客户端错误) 服务器无法处理请求 5xx Server Error(服务端错误) 服务器处理请求出错 图片 - img 4.1. 2XX成功 4.1.1. 200 OK 图片 - img 4.1.2. 204 No Content 图片 - img 表示请求已成功处理，但在返回的响应报文中不含实体的主体部分。 4.1.3. 206 Partial Content 图片 - img 该状态码表示客户端进行了范围请求，服务器成功执行了这部分的GET请求。响应报文中包含由Content-Range指定范围的实体内容。 4.2. 3XX 重定向 4.2.1. 301 Moved Permanently 永久性重定向，表示资源已被分配了新的URI，以后应使用新的URI。 图片 - img 4.2.2. 302 Found 临时性重定向，表示请求的资源已被分配了新的URI，但是临时性的。 图片 - img 4.2.3. 303 See Other 表示由于请求的资源存在另一个URI，应使用GET方法重定向获取请求的资源。 图片 - img 4.2.4. 304 Not Modified 表示客户端发送附带条件的请求时（GET中的If-Modified-Since等首部），服务器允许访问资源，但未满足附带条件因此直接返回304（服务器的资源未改变，可直接使用客户端未过期的缓存），不包含任何响应的主体部分。 图片 - img 4.2.5. 307 Temporary Redirect 临时重定向，该状态与302有相同的含义。 4.3. 4XX 客户端错误 4.3.1. 400 Bad Request 表示请求报文中存在语法错误，需修改内容重新发送请求。 图片 - img 4.3.2. 401 Unauthorized 表示需要通过HTTP认证。 图片 - img 4.3.3. 403 Forbidden 表示请求被服务器拒绝，未获得访问授权。 图片 - img 4.3.4. 404 No Found 表明服务器上找不到请求的资源，也可以在服务器拒绝请求且不想说明理由时使用。 图片 - img 4.4. 5XX 服务器错误 4.4.1. 500 Internal Server Error 表明服务器在执行请求时发生了错误，也可能是Web应用存在bug或临时故障等。 图片 - img 4.4.2. 503 Service Unavailable 表明服务器暂时处于超负荷或正在进行停机维护，现在不能处理请求。 图片 - img 参考： 《图解HTTP》 Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2023-03-06 10:57:30 "},"network/iptables.html":{"url":"network/iptables.html","title":"iptables","keywords":"","body":"1. 简介2. 基本概念2.1. 链(Chain)2.2. 表2.3. 表和链的关系3. 规则匹配条件4. 数据包经过防火墙的流程1. 简介 iptables是一个设置防火墙（netfilter）规则的命令工具。网络规则包括源地址、目的地址、传输协议（如TCP、UDP、ICMP）和服务类型（如HTTP、FTP和SMTP）等，当数据包与规则匹配时，iptables就根据规则所定义的方法来处理这些数据包，如放行（accept）、拒绝（reject）和丢弃（drop）等。配置防火墙的主要工作就是添加、修改和删除这些规则。 2. 基本概念 2.1. 链(Chain) 网络设置的”关卡“一般有多个网络规则，称为链。 INPUT OUTPUT FORWORD PREROUTING POSTROUTING 2.2. 表 具有相同功能的规则的集合叫做”表”。iptables定义了四类表。 filter表：负责过滤功能，防火墙；内核模块：iptables_filter nat表：network address translation，网络地址转换功能；内核模块：iptable_nat mangle表：拆解报文，做出修改，并重新封装 的功能；iptable_mangle raw表：关闭nat表上启用的连接追踪机制；iptable_raw 2.3. 表和链的关系 PREROUTING的规则可以存在于：raw表，mangle表，nat表。 INPUT的规则可以存在于：mangle表，filter表。 FORWARD的规则可以存在于：mangle表，filter表。 OUTPUT的规则可以存在于：raw表mangle表，nat表，filter表。 POSTROUTING的规则可以存在于：mangle表，nat表。 3. 规则匹配条件 基本匹配条件 源地址Source IP 目标地址 Destination IP 扩展匹配条件 源端口Source Port, 目标端口Destination Port 处理操作 ACCEPT：允许数据包通过。 DROP：直接丢弃数据包，不给任何回应信息，这时候客户端会感觉自己的请求泥牛入海了，过了超时时间才会有反应。 REJECT：拒绝数据包通过，必要时会给数据发送端一个响应的信息，客户端刚请求就会收到拒绝的信息。 SNAT：源地址转换，解决内网用户用同一个公网地址上网的问题。 MASQUERADE：是SNAT的一种特殊形式，适用于动态的、临时会变的ip上。 DNAT：目标地址转换。 REDIRECT：在本机做端口映射。l 4. 数据包经过防火墙的流程 图片来自：https://www.zsythink.net/archives/1199 到本机某进程的报文：PREROUTING –> INPUT 由本机转发的报文：PREROUTING –> FORWARD –> POSTROUTING 由本机的某进程发出报文（通常为响应报文）：OUTPUT –> POSTROUTING 参考： IPtables-朱双印博客 iptables详解（1）：iptables概念-朱双印博客 Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2023-03-06 10:57:30 "},"shell/shell-introduction.html":{"url":"shell/shell-introduction.html","title":"Shell简介","keywords":"","body":"1. shell简介1.1. 编译型语言1.2. 解释型语言2. 常见的Shell类型2.1. 查看shell3. 使用shell场景4. shell脚本5. 运行shell5.1. 作为可执行程序5.2. 作为解释器参数1. shell简介 shell是用户和Linux内核之间的一层代理，解释用户输入的命令，传递给内核。 shell是一种脚本语言（解释性语言）。 Shell既是一种命令语言，又是一种程序设计语言。作为命令语言，它交互式地解释和执行用户输入的命令；作为程序设计语言，它定义了各种变量和参数，并提供了许多在高级语言中才具有的控制结构，包括循环和分支。 Shell有两种执行命令的方式： 交互式（Interactive）：解释执行用户的命令，用户输入一条命令，Shell就解释执行一条。 批处理（Batch）：用户事先写一个Shell脚本(Script)，其中有很多条命令，让Shell一次把这些命令执行完，而不必一条一条地敲命令。 Shell脚本和编程语言很相似，也有变量和流程控制语句，但Shell脚本是解释执行的，不需要编译，Shell程序从脚本中一行一行读取并执行这些命令，相当于一个用户把脚本中的命令一行一行敲到Shell提示符下执行。 Unix/Linux上常见的Shell脚本解释器有bash、sh、csh、ksh等，习惯上把它们称作一种Shell。 程序设计语言可以分为两类：编译型语言和解释型语言。 1.1. 编译型语言 很多传统的程序设计语言，例如Fortran、Ada、Pascal、C、C++和Java，都是编译型语言。这类语言需要预先将我们写好的源代码(source code)转换成目标代码(object code)，这个过程被称作“编译”。运行程序时，直接读取目标代码(object code)。由于编译后的目标代码(object code)非常接近计算机底层，因此执行效率很高，这是编译型语言的优点。 编译型语言多半运作于底层，所处理的是字节、整数、浮点数或是其他机器层级的对象，往往实现一个简单的功能需要大量复杂的代码。 1.2. 解释型语言 有的语言（例如： Shell、JavaScript、Python、PHP等）需要一边执行一边翻译，不会产生任何可执行文件，用户需要拿到源码才能运行程序。程序运行后会即时翻译，翻译一部分执行一部分，并不用等所有代码翻译完。 这个过程叫解释，这类语言叫解释型语言或脚本语言，完成解释过程的软件叫解释器。 解释型语言也被称作“脚本语言”。因为每次执行程序都多了编译的过程，因此效率有所下降。 使用脚本编程语言的好处是，它们多半运行在比编译型语言还高的层级，能够轻易处理文件与目录之类的对象；缺点是它们的效率通常不如编译型语言。 脚本编程语言的例子有awk、Perl、Python、Ruby与Shell。 2. 常见的Shell类型 shell类型 说明 sh sh 是 UNIX 上的标准 shell，很多 UNIX 版本都配有 sh。 bash bash shell 是 Linux 的默认 shell，bash 兼容 sh，但并不完全一致。 csh 语法有点类似C语言。 ... 2.1. 查看shell $ cat /etc/shells /bin/sh /bin/bash /sbin/nologin /usr/bin/sh /usr/bin/bash /usr/sbin/nologin /bin/tcsh /bin/csh 查看默认shell $ echo $SHELL /bin/bash sh 一般被 bash 代替，/bin/sh往往是指向/bin/bash的符号链接。 $ ls -l /bin/sh lrwxrwxrwx. 1 root root 4 Mar 8 2018 /bin/sh -> bash 3. 使用shell场景 之所以要使用Shell脚本是基于： 简单性：Shell是一个高级语言；通过它，你可以简洁地表达复杂的操作。 可移植性：使用POSIX所定义的功能，可以做到脚本无须修改就可在不同的系统上执行。 开发容易：可以在短时间内完成一个功能强大又妤用的脚本。 但是，考虑到Shell脚本的命令限制和效率问题，下列情况一般不使用Shell： 资源密集型的任务，尤其在需要考虑效率时（比如，排序，hash等等）。 需要处理大任务的数学操作，尤其是浮点运算，精确运算，或者复杂的算术运算（这种情况一般使用C++或FORTRAN 来处理）。 有跨平台（操作系统）移植需求（一般使用C 或Java）。 复杂的应用，在必须使用结构化编程的时候（需要变量的类型检查，函数原型，等等）。 对于影响系统全局性的关键任务应用。 对于安全有很高要求的任务，比如你需要一个健壮的系统来防止入侵、破解、恶意破坏等等。 项目由连串的依赖的各个部分组成。 需要大规模的文件操作。 需要多维数组的支持。 需要数据结构的支持，比如链表或数等数据结构。 需要产生或操作图形化界面 GUI。 需要直接操作系统硬件。 需要 I/O 或socket 接口。 需要使用库或者遗留下来的老代码的接口。 私人的、闭源的应用（shell 脚本把代码就放在文本文件中，全世界都能看到）。 4. shell脚本 打开文本编辑器，新建一个文件，扩展名为sh（sh代表shell），扩展名并不影响脚本执行，见名知意就好，如果你用php写shell 脚本，扩展名就用php好了。 输入一些代码： #!/bin/bash echo \"Hello World !\" “#!” 是一个约定的标记，它告诉系统这个脚本需要什么解释器来执行，即使用哪一种Shell。echo命令用于向窗口输出文本。 5. 运行shell 运行Shell脚本有两种方法。 5.1. 作为可执行程序 将上面的代码保存为test.sh，并 cd 到相应目录： chmod +x ./test.sh #使脚本具有执行权限 ./test.sh #执行脚本 注意，一定要写成./test.sh，而不是test.sh。运行其它二进制的程序也一样，直接写test.sh，linux系统会去PATH里寻找有没有叫test.sh的，而只有/bin, /sbin, /usr/bin，/usr/sbin等在PATH里，你的当前目录通常不在PATH里，所以写成test.sh是会找不到命令的，要用./test.sh告诉系统说，就在当前目录找。 5.2. 作为解释器参数 这种运行方式是，直接运行解释器 # 使用 sh 解释器 sh test.sh # 使用 bash 解释器 bash test.sh 这种方式运行的脚本，不需要在第一行指定解释器信息，写了也没用。 参考： http://c.biancheng.net/cpp/shell/ Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2023-03-06 10:57:30 "},"shell/shell-var.html":{"url":"shell/shell-var.html","title":"Shell变量","keywords":"","body":"1. shell变量1.1. 定义变量1.2. 使用变量1.3. 重新定义变量1.4. 只读变量1.5. 删除变量1.6. 变量类型1) 局部变量2) 环境变量3) shell变量2. shell的特殊变量2.1. 命令行参数2.2. $* 和$@ 的区别2.3. 退出状态3. 转义字符4. 变量替换4.1. 命令替换4.2. 变量替换1. shell变量 Shell支持自定义变量。 1.1. 定义变量 定义变量时，变量名不加美元符号（$），如： variableName=\"value\" 注意，变量名和等号之间不能有空格，这可能和你熟悉的所有编程语言都不一样。同时，变量名的命名须遵循如下规则： 首个字符必须为字母（a-z，A-Z）。 中间不能有空格，可以使用下划线（_）。 不能使用标点符号。 不能使用bash里的关键字（可用help命令查看保留关键字）。 1.2. 使用变量 使用一个定义过的变量，只要在变量名前面加美元符号（$）即可，如： your_name=\"mozhiyan\" echo $your_name echo ${your_name} 变量名外面的花括号是可选的，加不加都行，，比如下面这种情况： for skill in Ada Coffe Action Java do echo \"I am good at ${skill}Script\" done 如果不给skill变量加花括号，写成echo \"I am good at $skillScript\"，解释器就会把$skillScript当成一个变量（其值为空），代码执行结果就不是我们期望的样子了。推荐给所有变量加上花括号，这是个好的编程习惯。 1.3. 重新定义变量 已定义的变量，可以被重新定义，如： myUrl=\"http://see.xidian.edu.cn/cpp/linux/\" echo ${myUrl} myUrl=\"http://see.xidian.edu.cn/cpp/shell/\" echo ${myUrl} 这样写是合法的，但注意，第二次赋值的时候不能写 $myUrl=\"http://see.xidian.edu.cn/cpp/shell/\"， 使用变量的时候才加美元符（$）。 1.4. 只读变量 使用 readonly 命令可以将变量定义为只读变量，只读变量的值不能被改变。 下面的例子尝试更改只读变量，结果报错： #!/bin/bash myUrl=\"http://see.xidian.edu.cn/cpp/shell/\" readonly myUrl myUrl=\"http://see.xidian.edu.cn/cpp/danpianji/\" 运行脚本，结果如下： /bin/sh: NAME: This variable is read only. 1.5. 删除变量 使用 unset 命令可以删除变量。语法： unset variable_name 变量被删除后不能再次使用；unset 命令不能删除只读变量。 举个例子： #!/bin/sh myUrl=\"http://see.xidian.edu.cn/cpp/u/xitong/\" unset myUrl echo $myUrl 上面的脚本没有任何输出。 1.6. 变量类型 运行shell时，会同时存在三种变量： 1) 局部变量 局部变量在脚本或命令中定义，仅在当前shell实例中有效，其他shell启动的程序不能访问局部变量。 2) 环境变量 所有的程序，包括shell启动的程序，都能访问环境变量，有些程序需要环境变量来保证其正常运行。必要的时候shell脚本也可以定义环境变量。 3) shell变量 shell变量是由shell程序设置的特殊变量。shell变量中有一部分是环境变量，有一部分是局部变量，这些变量保证了shell的正常运行。 2. shell的特殊变量 特殊变量列表 变量 含义 $0 当前脚本的文件名 $n 传递给脚本或函数的参数。n 是一个数字，表示第几个参数。例如，第一个参数是$1，第二个参数是$2。 $# 传递给脚本或函数的参数个数。 $* 传递给脚本或函数的所有参数。 $@ 传递给脚本或函数的所有参数。被双引号(\" \")包含时，与 $* 稍有不同，下面将会讲到。 $? 上个命令的退出状态，或函数的返回值。 $$ 当前Shell进程ID。对于 Shell 脚本，就是这些脚本所在的进程ID。 2.1. 命令行参数 运行脚本时传递给脚本的参数称为命令行参数。命令行参数用\\ $n 表示，例如，$1 表示第一个参数，$2 表示第二个参数，依次类推。 2.2. $* 和$@ 的区别 $* 和 $@ 都表示传递给函数或脚本的所有参数，不被双引号(\" \")包含时，都以\"$1\" \"$2\" … \"$n\" 的形式输出所有参数。 但是当它们被双引号(\" \")包含时， \"$*\" 会将所有的参数作为一个整体，以\"$1 $2 … $n\"的形式输出所有参数； \"$@\" 会将各个参数分开，以\"$1\" \"$2\" … \"$n\" 的形式输出所有参数。 2.3. 退出状态 $? 可以获取上一个命令的退出状态。所谓退出状态，就是上一个命令执行后的返回结果。 退出状态是一个数字，一般情况下，大部分命令执行成功会返回 0，失败返回 1。 不过，也有一些命令返回其他值，表示不同类型的错误。$? 也可以表示函数的返回值。 3. 转义字符 如果表达式中包含特殊字符，Shell 将会进行替换。例如，在双引号中使用变量就是一种替换，转义字符也是一种替换。 echo -e \"Value of a is $a \\n\" -e 表示对转义字符进行替换 下面的转义字符都可以用在 echo 中： 转义字符 含义 \\ 反斜杠 \\a 警报，响铃 \\b 退格（删除键） \\f 换页(FF)，将当前位置移到下页开头 \\n 换行 \\r 回车 \\t 水平制表符（tab键） \\v 垂直制表符 可以使用 echo 命令的 -E 选项禁止转义，默认也是不转义的；使用 -n 选项可以禁止插入换行符。 4. 变量替换 4.1. 命令替换 命令替换是指Shell可以先执行命令，将输出结果暂时保存，在适当的地方输出。 命令替换的语法： `command` 注意是反引号，不是单引号，这个键位于 Esc 键下方。 DATE=`date` echo \"Date is $DATE\" 4.2. 变量替换 变量替换可以根据变量的状态（是否为空、是否定义等）来改变它的值 可以使用的变量替换形式： 形式 说明 ${var} 变量本来的值 ${var:-word} 如果变量 var 为空或已被删除(unset)，那么返回 word，但不改变 var 的值。 ${var:=word} 如果变量 var 为空或已被删除(unset)，那么返回 word，并将 var 的值设置为 word。 ${var:?message} 如果变量 var 为空或已被删除(unset)，那么将消息 message 送到标准错误输出，可以用来检测变量 var 是否可以被正常赋值。 若此替换出现在Shell脚本中，那么脚本将停止运行。 ${var:+word} 如果变量 var 被定义，那么返回 word，但不改变 var 的值。 作用：用来检测变量是否为空，并提示相关信息。 参考： http://c.biancheng.net/cpp/shell/ Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2023-03-06 10:57:30 "},"shell/shell-char.html":{"url":"shell/shell-char.html","title":"Shell运算符","keywords":"","body":"1. shell运算符1.1. 算术运算符1.2. 关系运算符1.3. 布尔运算符1.4. 字符串运算符1.5. 文件测试运算符2. shell注释1. shell运算符 Bash 支持很多运算符，包括算数运算符、关系运算符、布尔运算符、字符串运算符和文件测试运算符。 awk 和 expr，expr 最常用 例如，两个数相加： #!/bin/bash val=`expr 2 + 2` echo \"Total value : $val\" 运行脚本输出： Total value : 4 两点注意： 表达式和运算符之间要有空格，例如 2+2 是不对的，必须写成 2 + 2，这与我们熟悉的大多数编程语言不一样。 完整的表达式要被``包含，注意这个字符不是常用的单引号，在 Esc 键下边。 1.1. 算术运算符 算术运算符列表 运算符 说明 举例 + 加法 `expr $a + $b` 结果为 30。 - 减法 `expr $a - $b` 结果为 10。 * 乘法 `expr $a * $b` 结果为 200。 / 除法 `expr $b / $a` 结果为 2。 % 取余 `expr $b % $a` 结果为 0。 = 赋值 a=$b 将把变量 b 的值赋给 a。 == 相等。用于比较两个数字，相同则返回 true。 [ $a == $b ] 返回 false。 != 不相等。用于比较两个数字，不相同则返回 true。 [ $a != $b ] 返回 true。 注意： 乘号(*)前边必须加反斜杠()才能实现乘法运算 条件表达式要放在方括号之间，并且要有空格，例如 [$a==$b] 是错误的，必须写成 [$a == $b ]。 1.2. 关系运算符 关系运算符只支持数字，不支持字符串，除非字符串的值是数字。 关系运算符列表 运算符 说明 举例 -eq 检测两个数是否相等，相等返回 true。 [ $a -eq $b ] 返回 true。 -ne 检测两个数是否相等，不相等返回 true。 [ $a -ne $b ] 返回 true。 -gt 检测左边的数是否大于右边的，如果是，则返回 true。 [ $a -gt $b ] 返回 false。 -lt 检测左边的数是否小于右边的，如果是，则返回 true。 [ $a -lt $b ] 返回 true。 -ge 检测左边的数是否大等于右边的，如果是，则返回 true。 [ $a -ge $b ] 返回 false。 -le 检测左边的数是否小于等于右边的，如果是，则返回 true。 [ $a -le $b ] 返回 true。 eq:equal ne:not equal gt:greater than lt:less than ge:greater or equal le:less or equal 1.3. 布尔运算符 布尔运算符列表 运算符 说明 举例 ! 非运算，表达式为 true 则返回 false，否则返回 true。 [ ! false ] 返回 true。 -o 或运算，有一个表达式为 true 则返回 true。 [ $a -lt 20 -o $b -gt 100 ] 返回 true。 -a 与运算，两个表达式都为 true 才返回 true。 [ $a -lt 20 -a $b -gt 100 ] 返回 false。 -o:or 或 -a: and 与 1.4. 字符串运算符 字符串运算符列表 运算符 说明 举例 = 检测两个字符串是否相等，相等返回 true。 [ $a = $b ] 返回 false。 != 检测两个字符串是否相等，不相等返回 true。 [ $a != $b ] 返回 true。 -z 检测字符串长度是否为0，为0返回 true。 [ -z $a ] 返回 false。 -n 检测字符串长度是否为0，不为0返回 true。 [ -z $a ] 返回 true。 str 检测字符串是否为空，不为空返回 true。 [ $a ] 返回 true。 字符串长度： -z:zero --->true -n: not zero--->true 1.5. 文件测试运算符 文件测试运算符用于检测 Unix 文件的各种属性。 文件测试运算符列表 操作符 说明 举例 -b file 检测文件是否是块设备文件，如果是，则返回 true。 [ -b $file ] 返回 false。 -c file 检测文件是否是字符设备文件，如果是，则返回 true。 [ -b $file ] 返回 false。 -d file 检测文件是否是目录，如果是，则返回 true。 [ -d $file ] 返回 false。 -f file 检测文件是否是普通文件（既不是目录，也不是设备文件），如果是，则返回 true。 [ -f $file ] 返回 true。 -g file 检测文件是否设置了 SGID 位，如果是，则返回 true。 [ -g $file ] 返回 false。 -k file 检测文件是否设置了粘着位(Sticky Bit)，如果是，则返回 true。 [ -k $file ] 返回 false。 -p file 检测文件是否是具名管道，如果是，则返回 true。 [ -p $file ] 返回 false。 -u file 检测文件是否设置了 SUID 位，如果是，则返回 true。 [ -u $file ] 返回 false。 -r file 检测文件是否可读，如果是，则返回 true。 [ -r $file ] 返回 true。 -w file 检测文件是否可写，如果是，则返回 true。 [ -w $file ] 返回 true。 -x file 检测文件是否可执行，如果是，则返回 true。 [ -x $file ] 返回 true。 -s file 检测文件是否为空（文件大小是否大于0），不为空返回 true。 [ -s $file ] 返回 true。 -e file 检测文件（包括目录）是否存在，如果是，则返回 true。 [ -e $file ] 返回 true。 2. shell注释 以“#”开头的行就是注释，会被解释器忽略。 sh里没有多行注释，只能每一行加一个#号。 如果在开发过程中，遇到大段的代码需要临时注释起来，过一会儿又取消注释，怎么办呢？每一行加个#符号太费力了，可以把这一段要注释的代码用一对花括号括起来，定义成一个函数，没有地方调用这个函数，这块代码就不会执行，达到了和注释一样的效果。 参考： http://c.biancheng.net/cpp/shell/ Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2023-03-06 10:57:30 "},"shell/shell-array.html":{"url":"shell/shell-array.html","title":"Shell数组","keywords":"","body":"1. 字符串1.1. 单引号1.2. 双引号1.3. 拼接字符串1.4. 获取字符串长度1.5. 提取子字符串1.6. 查找子字符串2. 数组2.1. 定义数组2.2. 读取数组2.3. 获取数组的长度1. 字符串 字符串是shell编程中最常用最有用的数据类型（除了数字和字符串，也没啥其它类型好用了），字符串可以用单引号，也可以用双引号，也可以不用引号。单双引号的区别跟PHP类似。 1.1. 单引号 str='this is a string' 单引号字符串的限制： 单引号里的任何字符都会原样输出，单引号字符串中的变量是无效的； 单引号字串中不能出现单引号（对单引号使用转义符后也不行）。 1.2. 双引号 your_name='qinjx' str=\"Hello, I know your are \\\"$your_name\\\"! \\n\" 双引号的优点： 双引号里可以有变量 双引号里可以出现转义字符 1.3. 拼接字符串 your_name=\"qinjx\" greeting=\"hello, \"$your_name\" !\" greeting_1=\"hello, ${your_name} !\" echo $greeting $greeting_1 1.4. 获取字符串长度 string=\"abcd\" echo ${#string} #输出 4 1.5. 提取子字符串 string=\"alibaba is a great company\" echo ${string:1:4} #输出liba 1.6. 查找子字符串 string=\"alibaba is a great company\" echo `expr index \"$string\" is` 2. 数组 bash支持一维数组（不支持多维数组），并且没有限定数组的大小。类似与C语言，数组元素的下标由0开始编号。获取数组中的元素要利用下标，下标可以是整数或算术表达式，其值应大于或等于0。 2.1. 定义数组 在Shell中，用括号来表示数组，数组元素用“空格”符号分割开。 定义数组的一般形式为： array_name=(value1 ... valuen) 例如： array_name=(value0 value1 value2 value3) 或者 array_name=( value0 value1 value2 value3 ) 还可以单独定义数组的各个分量： array_name[0]=value0 array_name[1]=value1 array_name[2]=value2 可以不使用连续的下标，而且下标的范围没有限制。 2.2. 读取数组 读取数组元素值的一般格式是： ${array_name[index]} 例如： valuen=${array_name[2]} 使用@ 或 * 可以获取数组中的所有元素，例如： ${array_name[*]} ${array_name[@]} 2.3. 获取数组的长度 获取数组长度的方法与获取字符串长度的方法相同，例如： # 取得数组元素的个数 length=${#array_name[@]} # 或者 length=${#array_name[*]} # 取得数组单个元素的长度 lengthn=${#array_name[n]} 参考： http://c.biancheng.net/cpp/shell/ Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2023-03-06 10:57:30 "},"shell/shell-echo.html":{"url":"shell/shell-echo.html","title":"Shell echo命令","keywords":"","body":"1. echo1.1. 显示转义字符1.2. 显示变量1.3. 显示换行1.4. 显示不换行1.5. 显示结果重定向至文件1.6. 原样输出字符串1.7. 显示命令执行结果2. printf1. echo echo是Shell的一个内部指令，用于在屏幕上打印出指定的字符串。命令格式： echo arg 您可以使用echo实现更复杂的输出格式控制。 1.1. 显示转义字符 echo \"\\\"It is a test\\\"\" 结果将是： \"It is a test\" 双引号也可以省略。 1.2. 显示变量 name=\"OK\" echo \"$name It is a test\" 结果将是： OK It is a test 同样双引号也可以省略。 如果变量与其它字符相连的话，需要使用大括号（{ }）： mouth=8 echo \"${mouth}-1-2009\" 结果将是： 8-1-2009 1.3. 显示换行 echo \"OK!\\n\" echo \"It is a test\" 输出： OK! It is a test 1.4. 显示不换行 echo \"OK!\\c\" echo \"It is a test\" 输出： OK!It si a test 1.5. 显示结果重定向至文件 echo \"It is a test\" > myfile 1.6. 原样输出字符串 若需要原样输出字符串（不进行转义），请使用单引号。例如： echo '$name\\\"' 1.7. 显示命令执行结果 echo `date` 结果将显示当前日期 从上面可看出，双引号可有可无，单引号主要用在原样输出中。 2. printf printf 命令用于格式化输出， 是echo命令的增强版。它是C语言printf()库函数的一个有限的变形，并且在语法上有些不同。 printf 不像 echo 那样会自动换行，必须显式添加换行符(\\n)。 注意：printf 由 POSIX 标准所定义，移植性要比 echo 好。 printf 命令的语法： printf format-string [arguments...] format-string 为格式控制字符串，arguments 为参数列表。 printf()功能和用法与 printf 命令类似 这里仅说明与C语言printf()函数的不同： printf 命令不用加括号 format-string 可以没有引号，但最好加上，单引号双引号均可。 参数多于格式控制符(%)时，format-string 可以重用，可以将所有参数都转换。 格式只指定了一个参数，但多出的参数仍然会按照该格式输出，format-string 被重用 arguments 使用空格分隔，不用逗号。 如果没有 arguments，那么 %s 用NULL代替，%d 用 0 代替 如果以 %d 的格式来显示字符串，那么会有警告，提示无效的数字，此时默认置为 0 # format-string为双引号,单引号与双引号效果一样,没有引号也可以输出 $ printf \"%d %s\\n\" 1 \"abc\" 1 abc 注意，根据POSIX标准，浮点格式%e、%E、%f、%g与%G是“不需要被支持”。这是因为awk支持浮点预算，且有它自己的printf语句。这样Shell程序中需要将浮点数值进行格式化的打印时，可使用小型的awk程序实现。然而，内建于bash、ksh93和zsh中的printf命令都支持浮点格式。 参考： http://c.biancheng.net/cpp/shell/ Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2023-03-06 10:57:30 "},"shell/shell-if.html":{"url":"shell/shell-if.html","title":"Shell判断语句","keywords":"","body":"1. if语句1.1. if ... else1.2. if ... else ... fi1.3. if ... elif ... fi 多分枝选择2. case语句1. if语句 if 语句通过关系运算符判断表达式的真假来决定执行哪个分支。Shell 有三种 if ... else 语句： if ... fi 语句； if ... else ... fi 语句； if ... elif ... else ... fi 语句。 1.1. if ... else if ... else 语句的语法： if [ expression ] then Statement(s) to be executed if expression is true fi 如果 expression 返回 true，then 后边的语句将会被执行；如果返回 false，不会执行任何语句。 最后必须以 fi 来结尾闭合 if，fi 就是 if 倒过来拼写。 注意：expression 和方括号([ ])之间必须有空格，否则会有语法错误。 1.2. if ... else ... fi if ... else ... fi 语句的语法： if [ expression ] then Statement(s) to be executed if expression is true else Statement(s) to be executed if expression is not true fi 如果 expression 返回 true，那么 then 后边的语句将会被执行；否则，执行 else 后边的语句。 1.3. if ... elif ... fi 多分枝选择 if ... elif ... fi 语句可以对多个条件进行判断，语法为： if [ expression 1 ] then Statement(s) to be executed if expression 1 is true elif [ expression 2 ] then Statement(s) to be executed if expression 2 is true elif [ expression 3 ] then Statement(s) to be executed if expression 3 is true else Statement(s) to be executed if no expression is true fi 哪一个 expression 的值为 true，就执行哪个 expression 后面的语句；如果都为 false，那么不执行任何语句。 if ... else 语句也可以写成一行，用分号隔开，以命令的方式来运行 if ... else 语句也经常与 test 命令结合使用，test 命令用于检查某个条件是否成立，与方括号([ ])类似。 if test $[num1] -eq $[num2] 2. case语句 case ... esac 与其他语言中的 switch ... case 语句类似，是一种多分枝选择结构。 case 值 in模式1) #------->匹配值 command1 command2 command3 ;; #------>break 模式2） command1 command2 command3 ;; *) # ------->相当于default command1 command2 command3 ;; esac #----->结束标志 case工作方式如上所示。 取值后面必须为关键字 in， 每一模式必须以右括号结束 取值可以为变量或常数。 匹配发现取值符合某一模式后，其间所有命令开始执行直至 ;;。 ;; 与其他语言中的 break 类似，意思是跳到整个 case 语句的最后。 取值将检测匹配的每一个模式。一旦模式匹配，则执行完匹配模式相应命令后不再继续其他模式。 如果无一匹配模式，使用星号 * 捕获该值，再执行后面的命令。 参考： http://c.biancheng.net/cpp/shell/ Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2023-03-06 10:57:30 "},"shell/shell-loop.html":{"url":"shell/shell-loop.html","title":"Shell循环语句","keywords":"","body":"1. for2. while3. until4. break命令5. continue命令1. for for循环一般格式为： for 变量 in 列表 do command1 command2 ... commandN done 列表是一组值（数字、字符串等）组成的序列，每个值通过空格分隔。每循环一次，就将列表中的下一个值赋给变量。 in 列表是可选的，如果不用它，for 循环使用命令行的位置参数。 示例： for loop in 1 2 3 4 5 do echo \"The value is: $loop\" done # 运行结果： The value is: 1 The value is: 2 The value is: 3 The value is: 4 The value is: 5 2. while while循环用于不断执行一系列命令，也用于从输入文件中读取数据 while command do Statement(s) to be executed if command is true done 命令执行完毕，控制返回循环顶部，从头开始直至测试条件为假。 示例： COUNTER=0 while [ $COUNTER -lt 5 ] do COUNTER='expr $COUNTER+1' echo $COUNTER done while循环可用于读取键盘信息。下面的例子中，输入信息被设置为变量FILM，按\\结束循环。 echo 'type to terminate' echo -n 'enter your most liked film: ' while read FILM do echo \"Yeah! great film the $FILM\" done 3. until until 循环执行一系列命令直至条件为 true 时停止。until 循环与 while 循环在处理方式上刚好相反。一般while循环优于until循环，但在某些时候，也只是极少数情况下，until 循环更加有用。 until 循环格式为： until command do Statement(s) to be executed until command is true done command 一般为条件表达式，如果返回值为 false，则继续执行循环体内的语句，否则跳出循环。 示例： #!/bin/bash a=0 until [ ! $a -lt 10 ] do echo $a a=`expr $a + 1` done 4. break命令 break命令允许跳出所有（终止执行后面的所有循环）。 在嵌套循环中，break 命令后面还可以跟一个整数，表示跳出第几层循环 break n 表示跳出第 n 层循环。 示例： #!/bin/bash while : do echo -n \"Input a number between 1 to 5: \" read aNum case $aNum in 1|2|3|4|5) echo \"Your number is $aNum!\" ;; *) echo \"You do not select a number between 1 to 5, game is over!\" break ;; esac done 5. continue命令 continue命令与break命令类似，只有一点差别，它不会跳出所有循环，仅仅跳出循环 同样，continue 后面也可以跟一个数字，表示跳出第几层循环。 示例： #!/bin/bash while : do echo -n \"Input a number between 1 to 5: \" read aNum case $aNum in 1|2|3|4|5) echo \"Your number is $aNum!\" ;; *) echo \"You do not select a number between 1 to 5!\" continue echo \"Game is over!\" ;; esac done 参考： http://c.biancheng.net/cpp/shell/ Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2023-03-06 10:57:30 "},"shell/shell-function.html":{"url":"shell/shell-function.html","title":"Shell函数","keywords":"","body":"1. 函数定义2. 函数返回值3. 函数调用4. 函数参数1. 函数定义 函数可以让我们将一个复杂功能划分成若干模块，让程序结构更加清晰，代码重复利用率更高。Shell 也支持函数。Shell 函数必须先定义后使用。 Shell 函数的定义格式如下： function_name () { list of commands [ return value ] } 也可以在函数名前加上关键字 function： function function_name () { list of commands [ return value ] } 2. 函数返回值 函数返回值，可以显式增加return语句；如果不加，会将最后一条命令运行结果作为返回值。 Shell 函数返回值只能是整数，一般用来表示函数执行成功与否，0表示成功，其他值表示失败。 如果 return 其他数据，比如一个字符串，往往会得到错误提示：“numeric argument required”。 如果一定要让函数返回字符串，那么可以先定义一个变量，用来接收函数的计算结果，脚本在需要的时候访问这个变量来获得函数返回值。函数返回值在调用该函数后通过 $?【$?表示上个命令的退出状态，或函数的返回值。】 来获得。 3. 函数调用 调用函数只需要给出函数名，不需要加括号。 像删除变量一样，删除函数也可以使用 unset 不过要加上 .f 选项 $unset .f function_name 如果你希望直接从终端调用函数，可以将函数定义在主目录下的 .profile 文件，这样每次登录后，在命令提示符后面输入函数名字就可以立即调用。 示例： #!/bin/bash # Define your function here Hello () { echo \"Url is http://see.xidian.edu.cn/cpp/shell/\" } # Invoke your function Hello 运行结果： $./test.sh Hello World $ 4. 函数参数 在Shell中，调用函数时可以向其传递参数。在函数体内部，通过 $n 的形式来获取参数的值，例如，$1表示第一个参数，$2表示第二个参数... 注意，$10 不能获取第十个参数，获取第十个参数需要${10}。当n>=10时，需要使用${n}来获取参数。 特殊变量用来处理参数: 特殊变量 说明 $# 传递给函数的参数个数。 $* 显示所有传递给函数的参数。 $@ 与$*相同，但是略有区别，请查看Shell特殊变量。 $? 函数的返回值。 带参数的函数示例： #!/bin/bash funWithParam(){ echo \"The value of the first parameter is $1 !\" echo \"The value of the second parameter is $2 !\" echo \"The value of the tenth parameter is $10 !\" echo \"The value of the tenth parameter is ${10} !\" echo \"The value of the eleventh parameter is ${11} !\" echo \"The amount of the parameters is $# !\" # 参数个数 echo \"The string of the parameters is $* !\" # 传递给函数的所有参数 } funWithParam 1 2 3 4 5 6 7 8 9 34 73 运行脚本： The value of the first parameter is 1 ! The value of the second parameter is 2 ! The value of the tenth parameter is 10 ! The value of the tenth parameter is 34 ! The value of the eleventh parameter is 73 ! The amount of the parameters is 12 ! The string of the parameters is 1 2 3 4 5 6 7 8 9 34 73 !\" 参考： http://c.biancheng.net/cpp/shell/ Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2023-03-06 10:57:30 "},"shell/shell-stdout.html":{"url":"shell/shell-stdout.html","title":"Shell重定向","keywords":"","body":"1. 输出重定向2. 输入重定向3. 重定向深入讲解4. Here Document5. /dev/null 文件6. 文件包含Unix 命令默认从标准输入设备(stdin)获取输入，将结果输出到标准输出设备(stdout)显示。一般情况下，标准输入设备就是键盘，标准输出设备就是终端，即显示器。 1. 输出重定向 命令的输出不仅可以是显示器，还可以很容易的转移向到文件，这被称为输出重定向。 命令输出重定向的语法为： $ command > file 这样，输出到显示器的内容就可以被重定向到文件。 输出重定向会覆盖文件内容；如果不希望文件内容被覆盖，可以使用 >> 追加到文件末尾 2. 输入重定向 和输出重定向一样，Unix 命令也可以从文件获取输入，语法为： $ command 这样，本来需要从键盘获取输入的命令会转移到文件读取内容。 注意：输出重定向是大于号(>)，输入重定向是小于号( 3. 重定向深入讲解 一般情况下，每个 Unix/Linux 命令运行时都会打开三个文件： 标准输入文件(stdin)：stdin的文件描述符为0，Unix程序默认从stdin读取数据。 标准输出文件(stdout)：stdout 的文件描述符为1，Unix程序默认向stdout输出数据。 标准错误文件(stderr)：stderr的文件描述符为2，Unix程序会向stderr流中写入错误信息。 默认情况下，command > file 将 stdout 重定向到 file，command 如果希望 stderr 重定向到 file，可以这样写： $command 2 > file 如果希望 stderr 追加到 file 文件末尾，可以这样写： $command 2 >> file 2 表示标准错误文件(stderr)。 如果希望将 stdout 和 stderr 合并后重定向到 file，可以这样写： $command > file 2>&1 或 $command >> file 2>&1 如果希望对 stdin 和 stdout 都重定向，可以这样写： $command file2 command 命令将 stdin 重定向到 file1，将 stdout 重定向到 file2。 全部可用的重定向命令列表 命令 说明 command > file 将输出重定向到 file。 command 将输入重定向到 file。 command >> file 将输出以追加的方式重定向到 file。 n > file 将文件描述符为 n 的文件重定向到 file。 n >> file 将文件描述符为 n 的文件以追加的方式重定向到 file。 n >& m 将输出文件 m 和 n 合并。 n 将输入文件 m 和 n 合并。 将开始标记 tag 和结束标记 tag 之间的内容作为输入。 4. Here Document Here Document 目前没有统一的翻译，这里暂译为”嵌入文档“。Here Document 是 Shell 中的一种特殊的重定向方式，它的基本的形式如下： command 它的作用是将两个 delimiter 之间的内容(document) 作为输入传递给 command。 注意： 结尾的delimiter 一定要顶格写，前面不能有任何字符，后面也不能有任何字符，包括空格和 tab 缩进。 开始的delimiter前后的空格会被忽略掉。 5. /dev/null 文件 如果希望执行某个命令，但又不希望在屏幕上显示输出结果，那么可以将输出重定向到 /dev/null： $ command > /dev/null /dev/null 是一个特殊的文件，写入到它的内容都会被丢弃；如果尝试从该文件读取内容，那么什么也读不到。但是 /dev/null 文件非常有用，将命令的输出重定向到它，会起到”禁止输出“的效果。 如果希望屏蔽 stdout 和 stderr，可以这样写： $ command > /dev/null 2>&1 6. 文件包含 像其他语言一样，Shell 也可以包含外部脚本，将外部脚本的内容合并到当前脚本。 Shell 中包含脚本可以使用： . filename 或 source filename 两种方式的效果相同，简单起见，一般使用点号(.)，但是注意点号(.)和文件名中间有一空格。 注意：被包含脚本不需要有执行权限。 参考： http://c.biancheng.net/cpp/shell/ Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2023-03-06 10:57:30 "},"tools/ansible-usage.html":{"url":"tools/ansible-usage.html","title":"Ansible的使用","keywords":"","body":"1. 安装2. 配置2.1. ansible.cfg2.2. hosts3. ansible的命令4. ansible-playbook1. 安装 以centos为例。 yum install -y ansible 2. 配置 默认配置目录在/etc/ansible/，主要有以下两个配置： ansible.cfg：ansible的配置文件 hosts：配置ansible所连接的机器IP信息 2.1. ansible.cfg 2.2. hosts # This is the default ansible 'hosts' file. # # It should live in /etc/ansible/hosts # # - Comments begin with the '#' character # - Blank lines are ignored # - Groups of hosts are delimited by [header] elements # - You can enter hostnames or ip addresses # - A hostname/ip can be a member of multiple groups # Ex 1: Ungrouped hosts, specify before any group headers. # green.example.com # blue.example.com # 192.168.100.1 # 192.168.100.10 # Ex 2: A collection of hosts belonging to the 'webservers' group # [webservers] # alpha.example.org # beta.example.org # 192.168.1.100 # 192.168.1.110 # If you have multiple hosts following a pattern you can specify # them like this: # www[001:006].example.com # Ex 3: A collection of database servers in the 'dbservers' group # [dbservers] # # db01.intranet.mydomain.net # db02.intranet.mydomain.net # 10.25.1.56 # 10.25.1.57 # Here's another example of host ranges, this time there are no # leading 0s: # db-[99:101]-node.example.com [k8s] 192.168.201.52 192.168.201.53 192.168.201.54 192.168.201.55 192.168.201.56 192.168.201.57 # password setting [all:vars] ansible_connection=ssh ansible_ssh_user=root ansible_ssh_pass=xxx 3. ansible的命令 命令格式为：ansible [options] host-pattern：即hosts文件中配置的集群名称 options：命令操作符 例如：ansible k8s -a 'uname -r' [root@k8s-master ansible]# ansible k8s -a 'uname -r' 172.16.201.56 | SUCCESS | rc=0 >> 4.16.11-1.el7.elrepo.x86_64 172.16.201.55 | SUCCESS | rc=0 >> 4.16.11-1.el7.elrepo.x86_64 172.16.201.54 | SUCCESS | rc=0 >> 4.16.11-1.el7.elrepo.x86_64 172.16.201.53 | SUCCESS | rc=0 >> 4.16.11-1.el7.elrepo.x86_64 172.16.201.52 | SUCCESS | rc=0 >> 4.16.11-1.el7.elrepo.x86_64 172.16.201.57 | SUCCESS | rc=0 >> 4.16.11-1.el7.elrepo.x86_64 具体的命令信息： Usage: ansible [options] Define and run a single task 'playbook' against a set of hosts Options: -a MODULE_ARGS, --args=MODULE_ARGS module arguments --ask-vault-pass ask for vault password -B SECONDS, --background=SECONDS run asynchronously, failing after X seconds (default=N/A) -C, --check don't make any changes; instead, try to predict some of the changes that may occur -D, --diff when changing (small) files and templates, show the differences in those files; works great with --check -e EXTRA_VARS, --extra-vars=EXTRA_VARS set additional variables as key=value or YAML/JSON, if filename prepend with @ -f FORKS, --forks=FORKS specify number of parallel processes to use (default=5) -h, --help show this help message and exit -i INVENTORY, --inventory=INVENTORY, --inventory-file=INVENTORY specify inventory host path or comma separated host list. --inventory-file is deprecated -l SUBSET, --limit=SUBSET further limit selected hosts to an additional pattern --list-hosts outputs a list of matching hosts; does not execute anything else -m MODULE_NAME, --module-name=MODULE_NAME module name to execute (default=command) -M MODULE_PATH, --module-path=MODULE_PATH prepend colon-separated path(s) to module library (default=[u'/root/.ansible/plugins/modules', u'/usr/share/ansible/plugins/modules']) -o, --one-line condense output --playbook-dir=BASEDIR Since this tool does not use playbooks, use this as a subsitute playbook directory.This sets the relative path for many features including roles/ group_vars/ etc. -P POLL_INTERVAL, --poll=POLL_INTERVAL set the poll interval if using -B (default=15) --syntax-check perform a syntax check on the playbook, but do not execute it -t TREE, --tree=TREE log output to this directory --vault-id=VAULT_IDS the vault identity to use --vault-password-file=VAULT_PASSWORD_FILES vault password file -v, --verbose verbose mode (-vvv for more, -vvvv to enable connection debugging) --version show program's version number and exit Connection Options: control as whom and how to connect to hosts -k, --ask-pass ask for connection password --private-key=PRIVATE_KEY_FILE, --key-file=PRIVATE_KEY_FILE use this file to authenticate the connection -u REMOTE_USER, --user=REMOTE_USER connect as this user (default=None) -c CONNECTION, --connection=CONNECTION connection type to use (default=smart) -T TIMEOUT, --timeout=TIMEOUT override the connection timeout in seconds (default=10) --ssh-common-args=SSH_COMMON_ARGS specify common arguments to pass to sftp/scp/ssh (e.g. ProxyCommand) --sftp-extra-args=SFTP_EXTRA_ARGS specify extra arguments to pass to sftp only (e.g. -f, -l) --scp-extra-args=SCP_EXTRA_ARGS specify extra arguments to pass to scp only (e.g. -l) --ssh-extra-args=SSH_EXTRA_ARGS specify extra arguments to pass to ssh only (e.g. -R) Privilege Escalation Options: control how and which user you become as on target hosts -s, --sudo run operations with sudo (nopasswd) (deprecated, use become) -U SUDO_USER, --sudo-user=SUDO_USER desired sudo user (default=root) (deprecated, use become) -S, --su run operations with su (deprecated, use become) -R SU_USER, --su-user=SU_USER run operations with su as this user (default=None) (deprecated, use become) -b, --become run operations with become (does not imply password prompting) --become-method=BECOME_METHOD privilege escalation method to use (default=sudo), valid choices: [ sudo | su | pbrun | pfexec | doas | dzdo | ksu | runas | pmrun | enable ] --become-user=BECOME_USER run operations as this user (default=root) --ask-sudo-pass ask for sudo password (deprecated, use become) --ask-su-pass ask for su password (deprecated, use become) -K, --ask-become-pass ask for privilege escalation password Some modules do not make sense in Ad-Hoc (include, meta, etc) 4. ansible-playbook Usage: ansible-playbook [options] playbook.yml [playbook2 ...] Runs Ansible playbooks, executing the defined tasks on the targeted hosts. Options: --ask-vault-pass ask for vault password -C, --check don't make any changes; instead, try to predict some of the changes that may occur -D, --diff when changing (small) files and templates, show the differences in those files; works great with --check -e EXTRA_VARS, --extra-vars=EXTRA_VARS set additional variables as key=value or YAML/JSON, if filename prepend with @ --flush-cache clear the fact cache for every host in inventory --force-handlers run handlers even if a task fails -f FORKS, --forks=FORKS specify number of parallel processes to use (default=5) -h, --help show this help message and exit -i INVENTORY, --inventory=INVENTORY, --inventory-file=INVENTORY specify inventory host path or comma separated host list. --inventory-file is deprecated -l SUBSET, --limit=SUBSET further limit selected hosts to an additional pattern --list-hosts outputs a list of matching hosts; does not execute anything else --list-tags list all available tags --list-tasks list all tasks that would be executed -M MODULE_PATH, --module-path=MODULE_PATH prepend colon-separated path(s) to module library (default=[u'/root/.ansible/plugins/modules', u'/usr/share/ansible/plugins/modules']) --skip-tags=SKIP_TAGS only run plays and tasks whose tags do not match these values --start-at-task=START_AT_TASK start the playbook at the task matching this name --step one-step-at-a-time: confirm each task before running --syntax-check perform a syntax check on the playbook, but do not execute it -t TAGS, --tags=TAGS only run plays and tasks tagged with these values --vault-id=VAULT_IDS the vault identity to use --vault-password-file=VAULT_PASSWORD_FILES vault password file -v, --verbose verbose mode (-vvv for more, -vvvv to enable connection debugging) --version show program's version number and exit Connection Options: control as whom and how to connect to hosts -k, --ask-pass ask for connection password --private-key=PRIVATE_KEY_FILE, --key-file=PRIVATE_KEY_FILE use this file to authenticate the connection -u REMOTE_USER, --user=REMOTE_USER connect as this user (default=None) -c CONNECTION, --connection=CONNECTION connection type to use (default=smart) -T TIMEOUT, --timeout=TIMEOUT override the connection timeout in seconds (default=10) --ssh-common-args=SSH_COMMON_ARGS specify common arguments to pass to sftp/scp/ssh (e.g. ProxyCommand) --sftp-extra-args=SFTP_EXTRA_ARGS specify extra arguments to pass to sftp only (e.g. -f, -l) --scp-extra-args=SCP_EXTRA_ARGS specify extra arguments to pass to scp only (e.g. -l) --ssh-extra-args=SSH_EXTRA_ARGS specify extra arguments to pass to ssh only (e.g. -R) Privilege Escalation Options: control how and which user you become as on target hosts -s, --sudo run operations with sudo (nopasswd) (deprecated, use become) -U SUDO_USER, --sudo-user=SUDO_USER desired sudo user (default=root) (deprecated, use become) -S, --su run operations with su (deprecated, use become) -R SU_USER, --su-user=SU_USER run operations with su as this user (default=None) (deprecated, use become) -b, --become run operations with become (does not imply password prompting) --become-method=BECOME_METHOD privilege escalation method to use (default=sudo), valid choices: [ sudo | su | pbrun | pfexec | doas | dzdo | ksu | runas | pmrun | enable ] --become-user=BECOME_USER run operations as this user (default=root) --ask-sudo-pass ask for sudo password (deprecated, use become) --ask-su-pass ask for su password (deprecated, use become) -K, --ask-become-pass ask for privilege escalation password Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2023-03-06 10:57:30 "},"tools/supervisor-usage.html":{"url":"tools/supervisor-usage.html","title":"Supervisor的使用","keywords":"","body":"1. Supervisor简介2. Supervisor安装3. Supervisor的配置3.1. supervisord.conf的配置3.2. 管理应用的配置4. Surpervisor的启动5. supervisorctl&supervisord5.1. supervisorctl5.2. supervisord6. Supervisor控制台7. supervisor.conf详细配置1. Supervisor简介 Supervisord 是用 Python 实现的一款的进程管理工具，supervisord 要求管理的程序是非 daemon 程序，supervisord 会帮你把它转成 daemon 程序，因此如果用 supervisord 来管理进程，进程需要以非daemon的方式启动。 例如：管理nginx 的话，必须在 nginx 的配置文件里添加一行设置 daemon off 让 nginx 以非 daemon 方式启动。 2. Supervisor安装 以centos系统为例，以下两种方式选择其一。 # yum install 的方式 yum install -y supervisor # easy_install的方式 yum install -y python-setuptools easy_install supervisor echo_supervisord_conf >/etc/supervisord.conf 3. Supervisor的配置 3.1. supervisord.conf的配置 如果使用yum install -y supervisor的命令安装，会生成默认配置/etc/supervisord.conf和目录/etc/supervisord.d，如果没有则自行创建。 在/etc/supervisord.d的目录下创建conf和log两个目录，conf用于存放管理进程的配置，log用于存放管理进程的日志。 cd /etc/supervisord.d mkdir conf log 修改/etc/supervisord.conf的[include]部分，即载入/etc/supervisord.d/conf目录下的所有配置。 vi /etc/supervisord.conf ... [include] files = supervisord.d/conf/*.conf ... 也可以修改supervisor应用日志的目录，默认日志路径为/var/log/supervisor/supervisord.log。 vi /etc/supervisord.conf ... [supervisord] logfile=/var/log/supervisor/supervisord.log ; (main log file;default $CWD/supervisord.log) logfile_maxbytes=50MB ; (max main logfile bytes b4 rotation;default 50MB) logfile_backups=10 ; (num of main logfile rotation backups;default 10) loglevel=info ; (log level;default info; others: debug,warn,trace) pidfile=/var/run/supervisord.pid ; (supervisord pidfile;default supervisord.pid) ... 3.2. 管理应用的配置 进入到/etc/supervisord.d/conf目录，创建管理应用的配置，可以创建多个应用配置。 例如，创建confd.conf配置。 [program:confd] directory = /usr/local/bin ; 程序的启动目录 command = /usr/local/bin/confd -config-file /etc/confd/confd.toml ; 启动命令，与命令行启动的命令是一样的 autostart = true ; 在 supervisord 启动的时候也自动启动 startsecs = 5 ; 启动 5 秒后没有异常退出，就当作已经正常启动了 autorestart = true ; 程序异常退出后自动重启 startretries = 3 ; 启动失败自动重试次数，默认是 3 user = root ; 用哪个用户启动 redirect_stderr = true ; 把 stderr 重定向到 stdout，默认 false stdout_logfile_maxbytes = 20MB ; stdout 日志文件大小，默认 50MB stdout_logfile_backups = 20 ; stdout 日志文件备份数 ; stdout 日志文件，需要注意当指定目录不存在时无法正常启动，所以需要手动创建目录（supervisord 会自动创建日志文件） stdout_logfile = /etc/supervisord.d/log/confd.log ;日志统一放在log目录下 ; 可以通过 environment 来添加需要的环境变量，一种常见的用法是修改 PYTHONPATH ; environment=PYTHONPATH=$PYTHONPATH:/path/to/somewhere 4. Surpervisor的启动 # supervisord二进制启动 supervisord -c /etc/supervisord.conf # 检查进程 ps aux | grep supervisord 或者以systemd的方式管理 vi /etc/rc.d/init.d/supervisord #!/bin/sh # # /etc/rc.d/init.d/supervisord # # Supervisor is a client/server system that # allows its users to monitor and control a # number of processes on UNIX-like operating # systems. # # chkconfig: - 64 36 # description: Supervisor Server # processname: supervisord # Source init functions . /etc/rc.d/init.d/functions prog=\"supervisord\" prefix=\"/usr\" exec_prefix=\"${prefix}\" prog_bin=\"${exec_prefix}/bin/supervisord\" PIDFILE=\"/var/run/$prog.pid\" start() { echo -n $\"Starting $prog: \" daemon $prog_bin --pidfile $PIDFILE -c /etc/supervisord.conf [ -f $PIDFILE ] && success $\"$prog startup\" || failure $\"$prog startup\" echo } stop() { echo -n $\"Shutting down $prog: \" [ -f $PIDFILE ] && killproc $prog || success $\"$prog shutdown\" echo } case \"$1\" in start) start ;; stop) stop ;; status) status $prog ;; restart) stop start ;; *) echo \"Usage: $0 {start|stop|restart|status}\" ;; esac 设置开机启动及systemd方式启动。 sudo chmod +x /etc/rc.d/init.d/supervisord sudo chkconfig --add supervisord sudo chkconfig supervisord on sudo service supervisord start 5. supervisorctl&supervisord Supervisord 安装完成后有两个可用的命令行 supervisord 和 supervisorctl，命令使用解释如下： 5.1. supervisorctl supervisorctl stop programxxx，停止某一个进程(programxxx)，programxxx 为 [program:beepkg] 里配置的值，这个示例就是 beepkg。 supervisorctl start programxxx，启动某个进程。 supervisorctl restart programxxx，重启某个进程。 supervisorctl status，查看进程状态。 supervisorctl stop groupworker ，重启所有属于名为 groupworker 这个分组的进程(start,restart 同理)。 supervisorctl stop all，停止全部进程，注：start、restart、stop 都不会载入最新的配置文件。 supervisorctl reload，载入最新的配置文件，停止原有进程并按新的配置启动、管理所有进程。 supervisorctl update，根据最新的配置文件，启动新配置或有改动的进程，配置没有改动的进程不会受影响而重启。 更多参考： $ supervisorctl --help supervisorctl -- control applications run by supervisord from the cmd line. Usage: /usr/bin/supervisorctl [options] [action [arguments]] Options: -c/--configuration -- configuration file path (default /etc/supervisord.conf) -h/--help -- print usage message and exit -i/--interactive -- start an interactive shell after executing commands -s/--serverurl URL -- URL on which supervisord server is listening (default \"http://localhost:9001\"). -u/--username -- username to use for authentication with server -p/--password -- password to use for authentication with server -r/--history-file -- keep a readline history (if readline is available) action [arguments] -- see below Actions are commands like \"tail\" or \"stop\". If -i is specified or no action is specified on the command line, a \"shell\" interpreting actions typed interactively is started. Use the action \"help\" to find out about available actions. 例如： # supervisorctl status confd RUNNING pid 31256, uptime 0:11:24 twemproxy RUNNING pid 31255, uptime 0:11:24 5.2. supervisord supervisord，初始启动 Supervisord，启动、管理配置中设置的进程。 $ supervisord --help supervisord -- run a set of applications as daemons. Usage: /usr/bin/supervisord [options] Options: -c/--configuration FILENAME -- configuration file -n/--nodaemon -- run in the foreground (same as 'nodaemon true' in config file) -h/--help -- print this usage message and exit -v/--version -- print supervisord version number and exit -u/--user USER -- run supervisord as this user (or numeric uid) -m/--umask UMASK -- use this umask for daemon subprocess (default is 022) -d/--directory DIRECTORY -- directory to chdir to when daemonized -l/--logfile FILENAME -- use FILENAME as logfile path -y/--logfile_maxbytes BYTES -- use BYTES to limit the max size of logfile -z/--logfile_backups NUM -- number of backups to keep when max bytes reached -e/--loglevel LEVEL -- use LEVEL as log level (debug,info,warn,error,critical) -j/--pidfile FILENAME -- write a pid file for the daemon process to FILENAME -i/--identifier STR -- identifier used for this instance of supervisord -q/--childlogdir DIRECTORY -- the log directory for child process logs -k/--nocleanup -- prevent the process from performing cleanup (removal of old automatic child log files) at startup. -a/--minfds NUM -- the minimum number of file descriptors for start success -t/--strip_ansi -- strip ansi escape codes from process output --minprocs NUM -- the minimum number of processes available for start success --profile_options OPTIONS -- run supervisord under profiler and output results based on OPTIONS, which is a comma-sep'd list of 'cumulative', 'calls', and/or 'callers', e.g. 'cumulative,callers') 6. Supervisor控制台 在/etc/supervisord.conf中修改[inet_http_server]的参数，具体如下： [inet_http_server] ; inet (TCP) server disabled by default port=*:9001 ; ip_address:port specifier, *:port for all iface username=root ; default is no username (open server) password=xxxx ; default is no password (open server) 修改后重启supervisor进程，在浏览器访问 http://:9001。 具体如下： 图片 - supervisor 7. supervisor.conf详细配置 cat /etc/supervisord.conf ; Sample supervisor config file. [unix_http_server] file=/var/run/supervisor/supervisor.sock ; (the path to the socket file) ;chmod=0700 ; sockef file mode (default 0700) ;chown=nobody:nogroup ; socket file uid:gid owner ;username=user ; (default is no username (open server)) ;password=123 ; (default is no password (open server)) ;[inet_http_server] ; inet (TCP) server disabled by default ;port=127.0.0.1:9001 ; (ip_address:port specifier, *:port for all iface) ;username=user ; (default is no username (open server)) ;password=123 ; (default is no password (open server)) [supervisord] logfile=/var/log/supervisor/supervisord.log ; (main log file;default $CWD/supervisord.log) logfile_maxbytes=50MB ; (max main logfile bytes b4 rotation;default 50MB) logfile_backups=10 ; (num of main logfile rotation backups;default 10) loglevel=info ; (log level;default info; others: debug,warn,trace) pidfile=/var/run/supervisord.pid ; (supervisord pidfile;default supervisord.pid) nodaemon=false ; (start in foreground if true;default false) minfds=1024 ; (min. avail startup file descriptors;default 1024) minprocs=200 ; (min. avail process descriptors;default 200) ;umask=022 ; (process file creation umask;default 022) ;user=chrism ; (default is current user, required if root) ;identifier=supervisor ; (supervisord identifier, default is 'supervisor') ;directory=/tmp ; (default is not to cd during start) ;nocleanup=true ; (don't clean up tempfiles at start;default false) ;childlogdir=/tmp ; ('AUTO' child log dir, default $TEMP) ;environment=KEY=value ; (key value pairs to add to environment) ;strip_ansi=false ; (strip ansi escape codes in logs; def. false) ; the below section must remain in the config file for RPC ; (supervisorctl/web interface) to work, additional interfaces may be ; added by defining them in separate rpcinterface: sections [rpcinterface:supervisor] supervisor.rpcinterface_factory = supervisor.rpcinterface:make_main_rpcinterface [supervisorctl] serverurl=unix:///var/run/supervisor/supervisor.sock ; use a unix:// URL for a unix socket ;serverurl=http://127.0.0.1:9001 ; use an http:// url to specify an inet socket ;username=chris ; should be same as http_username if set ;password=123 ; should be same as http_password if set ;prompt=mysupervisor ; cmd line prompt (default \"supervisor\") ;history_file=~/.sc_history ; use readline history if available ; The below sample program section shows all possible program subsection values, ; create one or more 'real' program: sections to be able to control them under ; supervisor. ;[program:theprogramname] ;command=/bin/cat ; the program (relative uses PATH, can take args) ;process_name=%(program_name)s ; process_name expr (default %(program_name)s) ;numprocs=1 ; number of processes copies to start (def 1) ;directory=/tmp ; directory to cwd to before exec (def no cwd) ;umask=022 ; umask for process (default None) ;priority=999 ; the relative start priority (default 999) ;autostart=true ; start at supervisord start (default: true) ;autorestart=true ; retstart at unexpected quit (default: true) ;startsecs=10 ; number of secs prog must stay running (def. 1) ;startretries=3 ; max # of serial start failures (default 3) ;exitcodes=0,2 ; 'expected' exit codes for process (default 0,2) ;stopsignal=QUIT ; signal used to kill process (default TERM) ;stopwaitsecs=10 ; max num secs to wait b4 SIGKILL (default 10) ;user=chrism ; setuid to this UNIX account to run the program ;redirect_stderr=true ; redirect proc stderr to stdout (default false) ;stdout_logfile=/a/path ; stdout log path, NONE for none; default AUTO ;stdout_logfile_maxbytes=1MB ; max # logfile bytes b4 rotation (default 50MB) ;stdout_logfile_backups=10 ; # of stdout logfile backups (default 10) ;stdout_capture_maxbytes=1MB ; number of bytes in 'capturemode' (default 0) ;stdout_events_enabled=false ; emit events on stdout writes (default false) ;stderr_logfile=/a/path ; stderr log path, NONE for none; default AUTO ;stderr_logfile_maxbytes=1MB ; max # logfile bytes b4 rotation (default 50MB) ;stderr_logfile_backups=10 ; # of stderr logfile backups (default 10) ;stderr_capture_maxbytes=1MB ; number of bytes in 'capturemode' (default 0) ;stderr_events_enabled=false ; emit events on stderr writes (default false) ;environment=A=1,B=2 ; process environment additions (def no adds) ;serverurl=AUTO ; override serverurl computation (childutils) ; The below sample eventlistener section shows all possible ; eventlistener subsection values, create one or more 'real' ; eventlistener: sections to be able to handle event notifications ; sent by supervisor. ;[eventlistener:theeventlistenername] ;command=/bin/eventlistener ; the program (relative uses PATH, can take args) ;process_name=%(program_name)s ; process_name expr (default %(program_name)s) ;numprocs=1 ; number of processes copies to start (def 1) ;events=EVENT ; event notif. types to subscribe to (req'd) ;buffer_size=10 ; event buffer queue size (default 10) ;directory=/tmp ; directory to cwd to before exec (def no cwd) ;umask=022 ; umask for process (default None) ;priority=-1 ; the relative start priority (default -1) ;autostart=true ; start at supervisord start (default: true) ;autorestart=unexpected ; restart at unexpected quit (default: unexpected) ;startsecs=10 ; number of secs prog must stay running (def. 1) ;startretries=3 ; max # of serial start failures (default 3) ;exitcodes=0,2 ; 'expected' exit codes for process (default 0,2) ;stopsignal=QUIT ; signal used to kill process (default TERM) ;stopwaitsecs=10 ; max num secs to wait b4 SIGKILL (default 10) ;user=chrism ; setuid to this UNIX account to run the program ;redirect_stderr=true ; redirect proc stderr to stdout (default false) ;stdout_logfile=/a/path ; stdout log path, NONE for none; default AUTO ;stdout_logfile_maxbytes=1MB ; max # logfile bytes b4 rotation (default 50MB) ;stdout_logfile_backups=10 ; # of stdout logfile backups (default 10) ;stdout_events_enabled=false ; emit events on stdout writes (default false) ;stderr_logfile=/a/path ; stderr log path, NONE for none; default AUTO ;stderr_logfile_maxbytes=1MB ; max # logfile bytes b4 rotation (default 50MB) ;stderr_logfile_backups ; # of stderr logfile backups (default 10) ;stderr_events_enabled=false ; emit events on stderr writes (default false) ;environment=A=1,B=2 ; process environment additions ;serverurl=AUTO ; override serverurl computation (childutils) ; The below sample group section shows all possible group values, ; create one or more 'real' group: sections to create \"heterogeneous\" ; process groups. ;[group:thegroupname] ;programs=progname1,progname2 ; each refers to 'x' in [program:x] definitions ;priority=999 ; the relative start priority (default 999) ; The [include] section can just contain the \"files\" setting. This ; setting can list multiple files (separated by whitespace or ; newlines). It can also contain wildcards. The filenames are ; interpreted as relative to this file. Included files *cannot* ; include files themselves. [include] files = supervisord.d/conf/*.conf 参考： http://supervisord.org/ Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2023-03-06 10:57:30 "},"tools/confd-usage.html":{"url":"tools/confd-usage.html","title":"Confd的使用","keywords":"","body":"1. confd的部署2. confd的配置2.1. confd.toml2.2. 创建confdir2.2.1. Template Resources2.2.2. Template3. 创建后端存储的配置数据4. 启动confd的服务5. 查看生成的配置文件6. confd动态更新twemproxy6.1. twemproxy.toml6.2. twemproxy.tmpl6.3. etcd中的配置格式6.4. 生成twemproxy配置文件7. confd的命令 confd的源码参考：https://github.com/kelseyhightower/confd 1. confd的部署 以下Linux系统为例。 下载confd的二进制文件，下载地址为：https://github.com/kelseyhightower/confd/releases。例如： # Download the binary wget https://github.com/kelseyhightower/confd/releases/download/v0.16.0/confd-0.16.0-linux-amd64 # 重命名二进制文件，并移动到PATH的目录下 mv confd-0.16.0-linux-amd64 /usr/local/bin/confd chmod +x /usr/local/bin/confd # 验证是否安装成功 confd --help 2. confd的配置 Confd通过读取后端存储的配置信息来动态更新对应的配置文件，对应的后端存储可以是etcd，redis等，其中etcd的v3版本对应的存储后端为etcdv3。 2.1. confd.toml confd.toml为confd服务本身的配置文件，主要记录了使用的存储后端、协议、confdir等参数。 示例： 存储后端etcdv3： backend = \"etcdv3\" confdir = \"/etc/confd\" log-level = \"debug\" interval = 5 nodes = [ \"http://192.168.10.4:12379\", ] scheme = \"http\" watch = true 其中watch参数表示实时监听后端存储的变化，如有变化则更新confd管理的配置。 存储后端为redis backend = \"redis\" confdir = \"/etc/confd\" log-level = \"debug\" interval = 1 # 间隔 1 秒同步一次配置文件 nodes = [ \"127.0.0.1:6379\", ] scheme = \"http\" client_key = \"123456\" # redis的密码，不是 password 参数 #watch = true 如果没有启动watch参数，则会依据interval参数定期去redis存储后端拿取数据，并比较与当前配置数据是否有变化（主要比较md5值），如果有变化则更新配置，没有变化则定期再去拿取数据，以此循环。 如果启动了watch参数，则修改redis存储数据的同时，还要执行publish的操作，促使confd去触发比较配置并更新配置的操作。 publish的命令格式如下: publish __keyspace@0__:{prefix}/{key} set(or del) 2.2. 创建confdir confdir底下包含两个目录: conf.d:confd的配置文件，主要包含配置的生成逻辑，例如模板源，后端存储对应的keys，命令执行等。 templates:配置模板Template，即基于不同组件的配置，修改为符合 Golang text templates的模板文件。 sudo mkdir -p /etc/confd/{conf.d,templates} 2.2.1. Template Resources 模板源配置文件是TOML格式的文件，主要包含配置的生成逻辑，例如模板源，后端存储对应的keys，命令执行等。默认目录在/etc/confd/conf.d。 参数说明： 必要参数 dest (string) - The target file. keys (array of strings) - An array of keys. src (string) - The relative path of a configuration template. 可选参数 gid (int) - The gid that should own the file. Defaults to the effective gid. mode (string) - The permission mode of the file. uid (int) - The uid that should own the file. Defaults to the effective uid. reload_cmd (string) - The command to reload config. check_cmd (string) - The command to check config. Use `` to reference the rendered source template. prefix (string) - The string to prefix to keys. 例子 例如：/etc/confd/conf.d/myapp-nginx.toml [template] prefix = \"/myapp\" src = \"nginx.tmpl\" dest = \"/tmp/myapp.conf\" owner = \"nginx\" mode = \"0644\" keys = [ \"/services/web\" ] check_cmd = \"/usr/sbin/nginx -t -c {{.src}}\" reload_cmd = \"/usr/sbin/service nginx reload\" 2.2.2. Template Template定义了单一应用配置的模板，默认存储在/etc/confd/templates目录下，模板文件符合Go的text/template格式。 模板文件常用函数有base，get，gets，lsdir，json等。具体可参考https://github.com/kelseyhightower/confd/blob/master/docs/templates.md。 例子： /etc/confd/templates/nginx.tmpl {{range $dir := lsdir \"/services/web\"}} upstream {{base $dir}} { {{$custdir := printf \"/services/web/%s/*\" $dir}}{{range gets $custdir}} server {{$data := json .Value}}{{$data.IP}}:80; {{end}} } server { server_name {{base $dir}}.example.com; location / { proxy_pass {{base $dir}}; } } {{end}} 3. 创建后端存储的配置数据 以etcdv3存储为例，在etcd中创建以下数据。 etcdctl --endpoints=$endpoints put /services/web/cust1/2 '{\"IP\": \"10.0.0.2\"}' etcdctl --endpoints=$endpoints put /services/web/cust2/2 '{\"IP\": \"10.0.0.4\"}' etcdctl --endpoints=$endpoints put /services/web/cust2/1 '{\"IP\": \"10.0.0.3\"}' etcdctl --endpoints=$endpoints put /services/web/cust1/1 '{\"IP\": \"10.0.0.1\"}' 4. 启动confd的服务 confd支持以daemon或者onetime两种模式运行，当以daemon模式运行时，confd会监听后端存储的配置变化，并根据配置模板动态生成目标配置文件。 confd可以使用-config-file参数来指定confd的配置文件，而将其他参数写在配置文件中。 /usr/local/bin/confd -config-file /etc/confd/conf/confd.toml 如果以daemon模式运行，在命令后面添加&符号，例如： confd -watch -backend etcdv3 -node http://172.16.5.4:12379 & 以下以onetime模式运行为例。其中对应的后端存储类型是etcdv3。 # 执行命令 confd -onetime -backend etcdv3 -node http://172.16.5.4:12379 # output 2018-05-11T18:04:59+08:00 k8s-dbg-master-1 confd[35808]: INFO Backend set to etcdv3 2018-05-11T18:04:59+08:00 k8s-dbg-master-1 confd[35808]: INFO Starting confd 2018-05-11T18:04:59+08:00 k8s-dbg-master-1 confd[35808]: INFO Backend source(s) set to http://172.16.5.4:12379 2018-05-11T18:04:59+08:00 k8s-dbg-master-1 confd[35808]: INFO /root/myapp/twemproxy/conf/twemproxy.conf has md5sum 6f0f43abede612c75cb840a4840fbea3 should be 32f48664266e3fd6b56ee73a314ee272 2018-05-11T18:04:59+08:00 k8s-dbg-master-1 confd[35808]: INFO Target config /root/myapp/twemproxy/conf/twemproxy.conf out of sync 2018-05-11T18:04:59+08:00 k8s-dbg-master-1 confd[35808]: INFO Target config /root/myapp/twemproxy/conf/twemproxy.conf has been updated 5. 查看生成的配置文件 在/etc/confd/conf.d/myapp-nginx.toml中定义的配置文件的生成路径为/tmp/myapp.conf。 [root@k8s-dbg-master-1 dest]# cat myapp.conf upstream cust1 { server 10.0.0.1:80; server 10.0.0.2:80; } server { server_name cust1.example.com; location / { proxy_pass cust1; } } upstream cust2 { server 10.0.0.3:80; server 10.0.0.4:80; } server { server_name cust2.example.com; location / { proxy_pass cust2; } } 6. confd动态更新twemproxy 6.1. twemproxy.toml confd的模板源文件配置：/etc/confd/conf.d/twemproxy.toml [template] src = \"twemproxy.tmpl\" dest = \"/root/myapp/twemproxy/conf/twemproxy.conf\" keys = [ \"/twemproxy/pool\" ] check_cmd = \"/usr/local/bin/nutcracker -t -c /root/myapp/twemproxy/conf/twemproxy.conf\" reload_cmd = \"bash /root/myapp/twemproxy/reload.sh\" 6.2. twemproxy.tmpl 模板文件：/etc/confd/templates/twemproxy.tmpl global: worker_processes: 4 # 并发进程数, 如果为0, 这 fallback 回原来的单进程模型(不支持 config reload!) user: nobody # worker 进程的用户, 默认 nobody. 只要主进程是 root 用户启动才生效. group: nobody # worker 进程的用户组 worker_shutdown_timeout: 30 # 单位为秒. 用于 reload 过程中在改时间段之后强制退出旧的 worker 进程. pools: {{range gets \"/twemproxy/pool/*\"}} {{base .Key}}: {{$pool := json .Value}} listen: {{$pool.ListenAddr.IP}}:{{$pool.ListenAddr.Port}} hash: fnv1a_64 # 选择实例的 hash 规则 distribution: ketama auto_eject_hosts: true # server 有问题是否剔除 redis: true # 是否为 Redis 协议 {{if $pool.Password}}redis_auth: {{$pool.Password}}{{end}} server_retry_timeout: 5000 # 被剔除多长时间后会重试 server_connections: 25 # NOTE: server 连接池的大小, 默认为 1, 建议调整 server_failure_limit: 5 # 失败多少次后暂时剔除 timeout: 1000 # Server 超时时间, 1 sec backlog: 1024 # 连接队列大小 preconnect: true # 预连接大小 servers:{{range $server := $pool.Servers}} - {{$server.IP}}:{{$server.Port}}:1 {{if $server.Master}}master{{end}} {{end}} {{end}} 6.3. etcd中的配置格式 etcd中的配置通过一个map来定义为完整的配置内容。其中key是twemproxy中pool的名称，value是pool的所有内容。 配置对应go结构体如下： type Pool struct{ ListenAddr ListenAddr `json:\"ListenAddr,omitempty\"` Servers []Server `json:\"Servers,omitempty\"` Password string `json:\"Password,omitempty\"` } type ListenAddr struct { IP string `json:\"IP,omitempty\"` Port string `json:\"Port,omitempty\"` } type Server struct { IP string `json:\"IP,omitempty\"` Port string `json:\"Port,omitempty\"` Master bool `json:\"Master,omitempty\"` } 配置对应JSON格式如下： { \"ListenAddr\": { \"IP\": \"192.168.5.7\", \"Port\": \"22225\" }, \"Servers\": [ { \"IP\": \"10.233.116.168\", \"Port\": \"6379\", \"Master\": true }, { \"IP\": \"10.233.110.207\", \"Port\": \"6379\", \"Master\": false } ], \"Password\": \"987654\" } 6.4. 生成twemproxy配置文件 global: worker_processes: 4 # 并发进程数, 如果为0, 这 fallback 回原来的单进程模型(不支持 config reload!) user: nobody # worker 进程的用户, 默认 nobody. 只要主进程是 root 用户启动才生效. group: nobody # worker 进程的用户组 worker_shutdown_timeout: 30 # 单位为秒. 用于 reload 过程中在改时间段之后强制退出旧的 worker 进程. pools: redis1: listen: 192.168.5.7:22223 hash: fnv1a_64 # 选择实例的 hash 规则 distribution: ketama auto_eject_hosts: true # server 有问题是否剔除 redis: true # 是否为 Redis 协议 redis_auth: 987654 server_retry_timeout: 5000 # 被剔除多长时间后会重试 server_connections: 25 # NOTE: server 连接池的大小, 默认为 1, 建议调整 server_failure_limit: 5 # 失败多少次后暂时剔除 timeout: 1000 # Server 超时时间, 1 sec backlog: 1024 # 连接队列大小 preconnect: true # 预连接大小 servers: - 10.233.116.169:6379:1 redis2: listen: 192.168.5.7:22224 hash: fnv1a_64 # 选择实例的 hash 规则 distribution: ketama auto_eject_hosts: true # server 有问题是否剔除 redis: true # 是否为 Redis 协议 redis_auth: 987654 server_retry_timeout: 5000 # 被剔除多长时间后会重试 server_connections: 25 # NOTE: server 连接池的大小, 默认为 1, 建议调整 server_failure_limit: 5 # 失败多少次后暂时剔除 timeout: 1000 # Server 超时时间, 1 sec backlog: 1024 # 连接队列大小 preconnect: true # 预连接大小 servers: - 10.233.110.223:6379:1 master - 10.233.111.21:6379:1 7. confd的命令 $ confd --help Usage of confd: -app-id string Vault app-id to use with the app-id backend (only used with -backend=vault and auth-type=app-id) -auth-token string Auth bearer token to use -auth-type string Vault auth backend type to use (only used with -backend=vault) -backend string backend to use (default \"etcd\") -basic-auth Use Basic Auth to authenticate (only used with -backend=consul and -backend=etcd) -client-ca-keys string client ca keys -client-cert string the client cert -client-key string the client key -confdir string confd conf directory (default \"/etc/confd\") -config-file string the confd config file (default \"/etc/confd/confd.toml\") -file value the YAML file to watch for changes (only used with -backend=file) -filter string files filter (only used with -backend=file) (default \"*\") -interval int backend polling interval (default 600) -keep-stage-file keep staged files -log-level string level which confd should log messages -node value list of backend nodes -noop only show pending changes -onetime run once and exit -password string the password to authenticate with (only used with vault and etcd backends) -path string Vault mount path of the auth method (only used with -backend=vault) -prefix string key path prefix -role-id string Vault role-id to use with the AppRole, Kubernetes backends (only used with -backend=vault and either auth-type=app-role or auth-type=kubernetes) -scheme string the backend URI scheme for nodes retrieved from DNS SRV records (http or https) (default \"http\") -secret-id string Vault secret-id to use with the AppRole backend (only used with -backend=vault and auth-type=app-role) -secret-keyring string path to armored PGP secret keyring (for use with crypt functions) -separator string the separator to replace '/' with when looking up keys in the backend, prefixed '/' will also be removed (only used with -backend=redis) -srv-domain string the name of the resource record -srv-record string the SRV record to search for backends nodes. Example: _etcd-client._tcp.example.com -sync-only sync without check_cmd and reload_cmd -table string the name of the DynamoDB table (only used with -backend=dynamodb) -user-id string Vault user-id to use with the app-id backend (only used with -backend=value and auth-type=app-id) -username string the username to authenticate as (only used with vault and etcd backends) -version print version and exit -watch enable watch support 参考文章： https://github.com/kelseyhightower/confd/blob/master/docs/installation.md https://github.com/kelseyhightower/confd/blob/master/docs/quick-start-guide.md https://github.com/kelseyhightower/confd/blob/master/docs/template-resources.md https://github.com/kelseyhightower/confd/blob/master/docs/templates.md Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2023-03-06 10:57:30 "},"tools/nfs-usage.html":{"url":"tools/nfs-usage.html","title":"NFS的使用","keywords":"","body":"1. NFS简介2. NFS的安装与配置2.1 服务端2.1.1. 安装nfs-utils、rpcbind两个包2.1.2. 创建共享目录2.1.3. NFS共享目录文件配置2.1.4. 启动NFS服务2.1.5. 服务端验证2.2 客户端2.2.1. 安装nfs-utils的包2.2.2. 创建挂载点2.2.3. 查看NFS服务器的共享2.2.4. 挂载2.2.5. 验证挂载信息1. NFS简介 NFS，是Network File System的简写，即网络文件系统。网络文件系统是FreeBSD支持的文件系统中的一种，也被称为NFS. NFS允许一个系统在网络上与他人共享目录和文件。 通过使用NFS，用户和程序可以像访问本地文件一样访问远端系统上的文件。 2. NFS的安装与配置 2.1 服务端 NFS需要安装nfs-utils、rpcbind两个包。 #可以先检查下本地是否已经安装，如果安装则无需重复安装包 [root@k8s-dbg-master-1 build]# rpm -qa|grep rpcbind rpcbind-0.2.0-42.el7.x86_64 [root@k8s-dbg-master-1 build]# rpm -qa|grep nfs libnfsidmap-0.25-17.el7.x86_64 nfs-utils-1.3.0-0.48.el7_4.x86_64 2.1.1. 安装nfs-utils、rpcbind两个包 #centos系统 yum -y install nfs-utils rpcbind #Ubuntu系统 #服务端 apt-get install nfs-kernel-server #客户端 apt-get install nfs-common 2.1.2. 创建共享目录 服务端共享目录：/data/nfs-storage/ mkdir /data/nfs-storage/ 2.1.3. NFS共享目录文件配置 vi /etc/exports #添加以下信息 /data/nfs-storage *(rw,insecure,sync,no_subtree_check,no_root_squash) 以上配置分为三个部分： 第一部分就是本地要共享出去的目录。 第二部分为允许访问的主机（可以是一个IP也可以是一个IP段），*代表允许所有的网段访问。 第三部分小括号里面的，为一些权限选项。 权限说明 rw ：读写； ro ：只读； sync ：同步模式，内存中数据时时写入磁盘； async ：不同步，把内存中数据定期写入磁盘中； secure ：nfs通过1024以下的安全TCP/IP端口发送 insecure ：nfs通过1024以上的端口发送 no_root_squash ：加上这个选项后，root用户就会对共享的目录拥有至高的权限控制，就像是对本机的目录操作一样。不安全，不建议使用； root_squash ：和上面的选项对应，root用户对共享目录的权限不高，只有普通用户的权限，即限制了root； subtree_check ：如果共享/usr/bin之类的子目录时，强制nfs检查父目录的权限（默认） no_subtree_check ：和上面相对，不检查父目录权限 all_squash ：不管使用NFS的用户是谁，他的身份都会被限定成为一个指定的普通用户身份； anonuid/anongid ：要和root_squash 以及 all_squash一同使用，用于指定使用NFS的用户限定后的uid和gid，前提是本机的/etc/passwd中存在这个uid和gid。 2.1.4. 启动NFS服务 #先启动rpcbind service rpcbind start #后启动nfs service nfs start #可以设置开机启动 chkconfig rpcbind on chkconfig nfs on 2.1.5. 服务端验证 通过showmount -e命令如果正常显示共享目录，表示安装正常。 [root@k8s-dbg-master-1 build]# showmount -e Export list for k8s-dbg-master-1: /data/nfs-storage * 2.2 客户端 2.2.1. 安装nfs-utils的包 yum install nfs-utils.x86_64 -y 2.2.2. 创建挂载点 客户端挂载目录：/mnt/store mkdir /mnt/store 2.2.3. 查看NFS服务器的共享 root@k8s-dbg-node-5:~# showmount -e 172.16.5.4 Export list for 172.16.5.4: /data/nfs-storage * 2.2.4. 挂载 mount -t nfs : #例如： mount -t nfs 172.16.5.4:/data/nfs-storage /mnt/store 2.2.5. 验证挂载信息 使用mount命令 root@k8s-dbg-node-5:~# mount |grep /mnt/store 172.16.5.4:/data/nfs-storage/k8s-storage/ssd on /mnt/store type nfs4 (rw,relatime,vers=4.0,rsize=1048576,wsize=1048576,namlen=255,hard,proto=tcp,port=0,timeo=600,retrans=2,sec=sys,clientaddr=172.16.200.24,local_lock=none,addr=172.16.5.4) 使用df -h命令 root@k8s-dbg-node-5:~# df -h|grep nfs 172.16.5.4:/data/nfs-storage 40G 25G 13G 67% /mnt/store 创建文件测试 #进入客户端的挂载目录，创建文件 cd /mnt/store touch test.txt #进入服务端的共享目录，查看客户端创建的文件是否同步 cd /data/nfs-storage ls Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2023-03-06 10:57:30 "},"tools/ceph-fuse.html":{"url":"tools/ceph-fuse.html","title":"ceph-fuse的使用","keywords":"","body":"1. 安装ceph-fuse2. 配置客户端访问的key3. ceph-fuse 挂载4. 查看是否挂载成功5. ceph-fuse命令说明1. 安装ceph-fuse yum install -y ceph-fuse 如果安装失败，先执行以下命令，再执行上述安装命令 yum -y install epel-release rpm -Uhv http://download.ceph.com/rpm-jewel/el7/noarch/ceph-release-1-1.el7.noarch.rpm 2. 配置客户端访问的key mkdir /etc/ceph/ vi /etc/ceph/ceph.client.admin.keyring [client.admin] key = xxxxxxxxxxxxxxxxxxxxxxxxxxxxxx== 3. ceph-fuse 挂载 ceph-fuse -m :6789,:6789,:6789 -r -o nonempty 例如： # ceph-fuse -m 192.168.18.3:6789,192.168.18.4:6789,192.168.18.5:6789 -r /pvc-volumes /root/cephfsdir -o nonempty 2019-03-27 17:58:04.435985 7fc61b67cec0 -1 did not load config file, using default settings. ceph-fuse[18051]: starting ceph client 2019-03-27 17:58:04.469144 7fc61b67cec0 -1 init, newargv = 0x55cecaba81c0 newargc=13 ceph-fuse[18051]: starting fuse 4. 查看是否挂载成功 # df -h Filesystem Size Used Avail Use% Mounted on ... ceph-fuse 1.6T 8.8G 1.6T 1% /root/cephfsdir 5. ceph-fuse命令说明 # ceph-fuse --help 2019-03-27 18:01:16.421376 7fae11998ec0 -1 did not load config file, using default settings. usage: ceph-fuse [-m mon-ip-addr:mon-port] [OPTIONS] --client_mountpoint/-r use root_directory as the mounted root, rather than the full Ceph tree. usage: ceph-fuse mountpoint [options] general options: -o opt,[opt...] mount options -h --help print help -V --version print version FUSE options: -d -o debug enable debug output (implies -f) -f foreground operation -s disable multi-threaded operation --conf/-c FILE read configuration from the given configuration file --id/-i ID set ID portion of my name --name/-n TYPE.ID set name --cluster NAME set cluster name (default: ceph) --setuser USER set uid to user or uid (and gid to user's gid) --setgroup GROUP set gid to group or gid --version show version and quit Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2023-03-06 10:57:30 "},"tools/ssh-tips.html":{"url":"tools/ssh-tips.html","title":"ssh tips","keywords":"","body":"1. ssh/scp免密码1.1. 在A生成密钥对1.2. 拷贝A的公钥（id_rsa.pub）到B1.3. 登录B1.4. 登录或拷贝2. 配置跳板机快速登录2.1. 配置ssh config文件2.2. 记录机器文件2.3. 安装fzf2.4. 设置命令别名2.5. 使用3. ssh配置项说明1. ssh/scp免密码 A服务器地址：10.8.216.25，下面简称A B服务器地址：10.8.216.26，下面简称B 实现A登录B免密码。 1.1. 在A生成密钥对 无密码方式： ssh-keygen -t rsa -P 自定义密码参数： ssh-keygen -C -f -t rsa -P \"\" 执行上述命令，一路回车，会在当前登录用户的home目录下的.ssh目录下生成id_rsa和id_rsa.pub两个文件，分别代表密钥对的私钥和公钥，如下图所示： 1.2. 拷贝A的公钥（id_rsa.pub）到B 这里拷贝到B的root用户home目录下为例： scp /root/.ssh/id_rsa.pub root@10.8.216.26:/root 1.3. 登录B 拷贝A的id_rsa.pub内容到.ssh目录下的authorized_keys文件中 cd /root cat id_rsa.pub >> .ssh/authorized_keys 如图： 1.4. 登录或拷贝 此时在A中用SSH登录B或向B拷贝文件，将不需要密码 ssh root@10.8.216.26 scp abc.txt root@10.8.216.26:/root 2. 配置跳板机快速登录 2.1. 配置ssh config文件 ssh config 路径：~/.ssh/config AddKeysToAgent yes ServerAliveInterval 3 Host jump HostName {jump_ip} Port {port} User {username} forwardagent yes identityfile ~/.ssh/id_rsa Host *.gw user {username} port {port} proxycommand ssh -W $(echo %h | sed -e \"s/.gw$//\"):%p jump Host bj* User {username} Port {port} proxycommand ssh -W 192.168.123.$(echo %h | awk -F 'bj' '{print $2}'):%p jump 多层跳板机 Host jump1 Hostname {jump1_ip} Port {port} User {username} forwardagent yes identityfile ~/.ssh/id_rsa Host jump2 Hostname {jump2_ip} Port {port} User {username} ProxyCommand ssh -q -x -W %h:%p jump1 Host * Hostname %h Port {port} User {username} ProxyCommand ssh -q -x -W %h:%p jump2 2.2. 记录机器文件 将关键字和IP写入文件记录，例如 ~/.my_hosts。 示例：可以是IP + 环境等关键字，中间用空格隔开。 # release 192.168.123.11 rel-node-11 192.168.123.12 rel-node-12 # pre 192.168.321.13 pre-node-13 192.168.321.14 pre-node-14 192.168.321.15 pre-node-15 # dev 192.168.111.16 dev-node-16 192.168.111.17 dev-node-17 2.3. 安装fzf # for mac brew install fzf 2.4. 设置命令别名 设置 alias 到shell rc 文件(.bashrc / .zshrc) alias goto=\"ssh \\$(cat ~/.my_hosts | fzf | awk '{ printf(\\\"%s.gw\\\", \\$1)}')\" 2.5. 使用 使用别名命令，输入关键字搜索，点击回车进入指定机器。 也可以使用ssh命令登录机器别名。 ssh bj11 3. ssh配置项说明 可以通过man查看ssh配置说明 man ssh_config 配置文件示例： Host jump port 22 Host * !jump StrictHostKeyChecking no HostName %h UserKnownHostsFile /dev/null LogLevel ERROR IdentityFile ~/.ssh/id_rsa ProxyCommand ssh -p 22 -F /dev/null jump -W %h:%p SendEnv LANG LC_* 配置项说明： Host: 标识设备，*表示通配所有字符，!表示例外通配。 StrictHostKeyChecking no：连接时不进行公钥交互确认操作。 UserKnownHostsFile /dev/null：不提示确认known_hosts文件。 ProxyCommand：代理命令 如果使用命令加参数的方式： ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o ProxyCommand=\"ssh -p 22 jump -W %h:%p\" Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2023-03-06 10:57:30 "},"git/git.html":{"url":"git/git.html","title":"Git 介绍","keywords":"","body":"1. Git是什么1.1. 概述1.2. 特性2. 为什么要用Git3. Git 命令思维导图1. Git是什么 1.1. 概述 Git是分布式版本控制系统，与SVN类似的集中化版本控制系统相比，集中化版本控制系统如果中央服务器宕机则会影响数据和协同开发。 Git是分布式的版本控制系统，客户端不只是提取最新版本的快照，而且将整个代码仓库镜像复制下来。如果任何协同工作用的服务器发生故障了，也可以用任何一个代码仓库来恢复。而且在协作服务器宕机期间，你也可以提交代码到本地仓库，当协作服务器正常工作后，你再将本地仓库同步到远程仓库。 1.2. 特性 能够对文件版本控制和多人协作开发 拥有强大的分支特性，所以能够灵活地以不同的工作流协同开发 分布式版本控制系统，即使协作服务器宕机，也能继续提交代码或文件到本地仓库，当协作服务器恢复正常工作时，再将本地仓库同步到远程仓库。 当团队中某个成员完成某个功能时，通过pull request操作来通知其他团队成员，其他团队成员能够review code后再合并代码。 2. 为什么要用Git 能够对文件版本控制和多人协作开发 拥有强大的分支特性，所以能够灵活地以不同的工作流协同开发 分布式版本控制系统，即使协作服务器宕机，也能继续提交代码或文件到本地仓库，当协作服务器恢复正常工作时，再将本地仓库同步到远程仓库。 当团队中某个成员完成某个功能时，通过pull request操作来通知其他团队成员，其他团队成员能够review code后再合并代码。 3. Git 命令思维导图 图片 - git-map Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2023-03-06 10:57:30 "},"git/git-common-cmd.html":{"url":"git/git-common-cmd.html","title":"Git 常用命令","keywords":"","body":"1. Git常用命令2. git rebase2.1. 合并多余提交记录2.2. 修改提交记录3. git设置忽略特殊文件3.1. 忽略文件的原则3.2. 设置的方法3.3. gitignore 不生效解决方法4. Git分支重命名5. 代码冲突6. 修改历史提交的用户信息7. 撤销已经push的提交1. Git常用命令 分类 子类 git command zsh alias 分支 查看当前分支 git branch gb 创建新分支,仍停留在当前分支 git branch 创建并切换到新分支 git checkout -b gcb 切换分支 git checkout 合并分支 git checkout #切换到要合并的分支git merge –no-ff #合并指定分支到当前分支 提交 查看状态 git status gst 查看修改部分 git diff --color gd 添加文件到暂存区 git add --all 提交本地仓库 git commit -m \"\" 推送到指定分支 git push -u origin 查看提交日志 git log - 2. git rebase 如果信息修改无法生效，设置永久环境变量：export EDITOR=vim 帮助信息： # Rebase 67da308..6ef692b onto 67da308 (1 command) # # Commands: # p, pick = use commit # r, reword = use commit, but edit the commit message # e, edit = use commit, but stop for amending # s, squash = use commit, but meld into previous commit # f, fixup = like \"squash\", but discard this commit's log message # x, exec = run command (the rest of the line) using shell # d, drop = remove commit # # These lines can be re-ordered; they are executed from top to bottom. # # If you remove a line here THAT COMMIT WILL BE LOST. # # However, if you remove everything, the rebase will be aborted. # # Note that empty commits are commented out 2.1. 合并多余提交记录 #以交互的方式进行rebase git rebase -i master #合并多余提交记录：s, squash = use commit, but meld into previous commit pick 6ef692b FIX: Fix parsing docker image version error s 3df667y FIX: the second push s 3fds95t FIX: the third push 保存退出 # 进入修改交互界面 删除需要删除的提交记录，保存退出 #查看提交记录是否已被修改 git log #最后强制提交到分支 git commit --force -u origin fix/add-unit-test-for-global-role-revoking 2.2. 修改提交记录 #以交互的方式进行rebase git rebase -i master #修改提交记录：e, edit = use commit, but stop for amending e 6ef692b FIX: Fix parsing docker image version error e 5ty697u FIX: Fix parsing docker image version error #保存退出 git commit --amend #修改提交记录内容，保存退出 git rebase --continue git commit --amend #修改下一条提交记录，保存退出 git rebase --continue git status # 查看状态提示 #最后强制提交到分支 git commit --force -u origin fix/add-unit-test-for-global-role-revoking #查看提交记录是否已被修改 git log 3. git设置忽略特殊文件 3.1. 忽略文件的原则 忽略操作系统自动生成的文件，比如缩略图等； 忽略编译生成的中间文件、可执行文件等，也就是如果一个文件是通过另一个文件自动生成的，那自动生成的文件就没必要放进版本库，比如Java编译产生的.class文件； 忽略你自己的带有敏感信息的配置文件，比如存放口令的配置文件。 3.2. 设置的方法 在项目的workdir 下编辑 .gitignore 文件，文件的路径填写为workdir的相对路径。 .idea/ #IDE的配置文件 _build/ server/server #二进制文件 3.3. gitignore 不生效解决方法 原因是.gitignore只能忽略那些原来没有被track的文件，如果某些文件已经被纳入了版本管理中，则修改.gitignore是无效的。那么解决方法就是先把本地缓存删除（改变成未track状态），然后再提交： git rm -r --cached . git add . git commit -m 'update .gitignore' 4. Git分支重命名 假设分支名称为oldName 想要修改为 newName 1. 本地分支重命名(还没有推送到远程) git branch -m oldName newName 2. 远程分支重命名 (已经推送远程-假设本地分支和远程对应分支名称相同) a. 重命名远程分支对应的本地分支 git branch -m oldName newName b. 删除远程分支 git push --delete origin oldName c. 上传新命名的本地分支 git push origin newName d.把修改后的本地分支与远程分支关联 git branch --set-upstream-to origin/newName 5. 代码冲突 git checkout master git pull git checkout git rebase -i master fix conflict git rebase --continue git push --force -u origin 6. 修改历史提交的用户信息 1、克隆并进入你的仓库 git clone --bare https://github.com/user/repo.git cd repo.git 2、创建以下脚本，例如命名为rename.sh #!/bin/sh git filter-branch --env-filter ' OLD_EMAIL=\"your-old-email@example.com\" #修改参数为你的旧提交邮箱 CORRECT_NAME=\"Your Correct Name\" #修改参数为你新的用户名 CORRECT_EMAIL=\"your-correct-email@example.com\" #修改参数为你新的邮箱名 if [ \"$GIT_COMMITTER_EMAIL\" = \"$OLD_EMAIL\" ] then export GIT_COMMITTER_NAME=\"$CORRECT_NAME\" export GIT_COMMITTER_EMAIL=\"$CORRECT_EMAIL\" fi if [ \"$GIT_AUTHOR_EMAIL\" = \"$OLD_EMAIL\" ] then export GIT_AUTHOR_NAME=\"$CORRECT_NAME\" export GIT_AUTHOR_EMAIL=\"$CORRECT_EMAIL\" fi ' --tag-name-filter cat -- --branches --tags 3、执行脚本 chmod +x rename.sh sh rename.sh 4、查看新 Git 历史有没有错误。 #可以看到提交记录的用户信息已经修改为新的用户信息 git log 5、确认提交内容，重新提交（可以先把rename.sh移除掉） git push --force --tags origin 'refs/heads/*' 7. 撤销已经push的提交 # 本地仓库回退到某一版本 git reset -hard # 强制 PUSH，此时远程分支已经恢复成指定的 commit 了 git push origin master --force Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2023-03-06 10:57:30 "},"git/git-commands.html":{"url":"git/git-commands.html","title":"Git 命令分类","keywords":"","body":"Git 命令详解1. 示意图2. Git 命令分类2.1. 新建代码库2.2. 配置2.3. 增加/删除文件2.4. 代码提交2.5. 分支2.6. 标签2.7. 查看信息2.8. 远程同步2.9. 撤销2.10. 其他Git 命令详解 1. 示意图 图片 - 这里写图片描述 Workspace：工作区 Index / Stage：暂存区 Repository：仓库区（或本地仓库） Remote：远程仓库 2. Git 命令分类 2.1. 新建代码库 # 在当前目录新建一个Git代码库 $ git init # 新建一个目录，将其初始化为Git代码库 $ git init [project-name] # 下载一个项目和它的整个代码历史 $ git clone [url] 2.2. 配置 Git的设置文件为.gitconfig，它可以在用户主目录下（全局配置），也可以在项目目录下（项目配置）。 # 显示当前的Git配置 $ git config --list # 编辑Git配置文件 $ git config -e [--global] # 设置提交代码时的用户信息 $ git config [--global] user.name \"[name]\" $ git config [--global] user.email \"[email address]\" 2.3. 增加/删除文件 # 添加指定文件到暂存区 $ git add [file1] [file2] ... # 添加指定目录到暂存区，包括子目录 $ git add [dir] # 添加当前目录的所有文件到暂存区 $ git add . # 添加每个变化前，都会要求确认 # 对于同一个文件的多处变化，可以实现分次提交 $ git add -p # 删除工作区文件，并且将这次删除放入暂存区 $ git rm [file1] [file2] ... # 停止追踪指定文件，但该文件会保留在工作区 $ git rm --cached [file] # 改名文件，并且将这个改名放入暂存区 $ git mv [file-original] [file-renamed] 2.4. 代码提交 # 提交暂存区到仓库区 $ git commit -m [message] # 提交暂存区的指定文件到仓库区 $ git commit [file1] [file2] ... -m [message] # 提交工作区自上次commit之后的变化，直接到仓库区 $ git commit -a # 提交时显示所有diff信息 $ git commit -v # 使用一次新的commit，替代上一次提交 # 如果代码没有任何新变化，则用来改写上一次commit的提交信息 $ git commit --amend -m [message] # 重做上一次commit，并包括指定文件的新变化 $ git commit --amend [file1] [file2] ... 2.5. 分支 # 列出所有本地分支 $ git branch # 列出所有远程分支 $ git branch -r # 列出所有本地分支和远程分支 $ git branch -a # 新建一个分支，但依然停留在当前分支 $ git branch [branch-name] # 新建一个分支，并切换到该分支 $ git checkout -b [branch] # 新建一个分支，指向指定commit $ git branch [branch] [commit] # 新建一个分支，与指定的远程分支建立追踪关系 $ git branch --track [branch] [remote-branch] # 切换到指定分支，并更新工作区 $ git checkout [branch-name] # 切换到上一个分支 $ git checkout - # 建立追踪关系，在现有分支与指定的远程分支之间 $ git branch --set-upstream [branch] [remote-branch] # 合并指定分支到当前分支 $ git merge [branch] # 选择一个commit，合并进当前分支 $ git cherry-pick [commit] # 删除分支 $ git branch -d [branch-name] # 删除远程分支 $ git push origin --delete [branch-name] $ git branch -dr [remote/branch] 2.6. 标签 # 列出所有tag $ git tag # 新建一个tag在当前commit $ git tag [tag] # 新建一个tag在指定commit $ git tag [tag] [commit] # 删除本地tag $ git tag -d [tag] # 删除远程tag $ git push origin :refs/tags/[tagName] # 查看tag信息 $ git show [tag] # 提交指定tag $ git push [remote] [tag] # 提交所有tag $ git push [remote] --tags # 新建一个分支，指向某个tag $ git checkout -b [branch] [tag] 2.7. 查看信息 # 显示有变更的文件 $ git status # 显示当前分支的版本历史 $ git log # 显示commit历史，以及每次commit发生变更的文件 $ git log --stat # 搜索提交历史，根据关键词 $ git log -S [keyword] # 显示某个commit之后的所有变动，每个commit占据一行 $ git log [tag] HEAD --pretty=format:%s # 显示某个commit之后的所有变动，其\"提交说明\"必须符合搜索条件 $ git log [tag] HEAD --grep feature # 显示某个文件的版本历史，包括文件改名 $ git log --follow [file] $ git whatchanged [file] # 显示指定文件相关的每一次diff $ git log -p [file] # 显示过去5次提交 $ git log -5 --pretty --oneline # 显示所有提交过的用户，按提交次数排序 $ git shortlog -sn # 显示指定文件是什么人在什么时间修改过 $ git blame [file] # 显示暂存区和工作区的差异 $ git diff # 显示暂存区和上一个commit的差异 $ git diff --cached [file] # 显示工作区与当前分支最新commit之间的差异 $ git diff HEAD # 显示两次提交之间的差异 $ git diff [first-branch]...[second-branch] # 显示今天你写了多少行代码 $ git diff --shortstat \"@{0 day ago}\" # 显示某次提交的元数据和内容变化 $ git show [commit] # 显示某次提交发生变化的文件 $ git show --name-only [commit] # 显示某次提交时，某个文件的内容 $ git show [commit]:[filename] # 显示当前分支的最近几次提交 $ git reflog 2.8. 远程同步 # 下载远程仓库的所有变动 $ git fetch [remote] # 显示所有远程仓库 $ git remote -v # 显示某个远程仓库的信息 $ git remote show [remote] # 增加一个新的远程仓库，并命名 $ git remote add [shortname] [url] # 取回远程仓库的变化，并与本地分支合并 $ git pull [remote] [branch] # 上传本地指定分支到远程仓库 $ git push [remote] [branch] # 强行推送当前分支到远程仓库，即使有冲突 $ git push [remote] --force # 推送所有分支到远程仓库 $ git push [remote] --all 2.9. 撤销 # 恢复暂存区的指定文件到工作区 $ git checkout [file] # 恢复某个commit的指定文件到暂存区和工作区 $ git checkout [commit] [file] # 恢复暂存区的所有文件到工作区 $ git checkout . # 重置暂存区的指定文件，与上一次commit保持一致，但工作区不变 $ git reset [file] # 重置暂存区与工作区，与上一次commit保持一致 $ git reset --hard # 重置当前分支的指针为指定commit，同时重置暂存区，但工作区不变 $ git reset [commit] # 重置当前分支的HEAD为指定commit，同时重置暂存区和工作区，与指定commit一致 $ git reset --hard [commit] # 重置当前HEAD为指定commit，但保持暂存区和工作区不变 $ git reset --keep [commit] # 新建一个commit，用来撤销指定commit # 后者的所有变化都将被前者抵消，并且应用到当前分支 $ git revert [commit] # 暂时将未提交的变化移除，稍后再移入 $ git stash $ git stash pop 2.10. 其他 # 生成一个可供发布的压缩包 $ git archive # 设置换行符为LF git config --global core.autocrlf false #拒绝提交包含混合换行符的文件 git config --global core.safecrlf true 参考文章： http://www.ruanyifeng.com/blog/2015/12/git-cheat-sheet.html Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2023-03-06 10:57:30 "},"git/git-commit-msg.html":{"url":"git/git-commit-msg.html","title":"Git commit规范","keywords":"","body":"1. Git commit规范1.1. 格式1.2. type1.3. scope1.4. subject2. Git commit工具1. Git commit规范 1.1. 格式 (): 示例： fix(ngRepeat): fix trackBy function being invoked with incorrect scope 1.2. type 主要的提交类型如下： Type 说明 备注 feat 提交新功能 常用 fix 修复bug 常用 docs 修改文档 style 修改格式，例如格式化代码，空格，拼写错误等 refactor 重构代码，没有添加新功能也没有修复bug test 添加或修改测试用例 perf 代码性能调优 chore 修改构建工具、构建流程、更新依赖库、文档生成逻辑 例如vendor包 1.3. scope 表示此次commit涉及的文件范围，可以使用*来表示涉及多个范围。 1.4. subject 描述此次commit涉及的修改内容。 使用祈使句（动词开头）、动宾短语。 第一个字母不要大写。 不要以.句号结尾。 2. Git commit工具 安装commitizen和cz-conventional-changelog。 npm install -g commitizen cz-conventional-changelog echo '{ \"path\": \"cz-conventional-changelog\" }' > ~/.czrc 使用cz-cli $ git cz cz-cli@4.0.3, cz-conventional-changelog@3.0.1 ? Select the type of change that you're committing: (Use arrow keys) ❯ feat: A new feature fix: A bug fix docs: Documentation only changes style: Changes that do not affect the meaning of the code (white-space, formatting, missing semi-colons, etc) refactor: A code change that neither fixes a bug nor adds a feature perf: A code change that improves performance test: Adding missing tests or correcting existing tests (Move up and down to reveal more choices) 参考： https://github.com/angular/angular.js/blob/master/DEVELOPERS.md#-git-commit-guidelines https://juejin.im/post/5afc5242f265da0b7f44bee4 commitizen/cz-cli commitizen/cz-conventional-changelog Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2023-03-06 10:57:30 "},"git/git-alias-zsh.html":{"url":"git/git-alias-zsh.html","title":"Git 命令别名","keywords":"","body":"git pluginAliases常用完整列表DeprecatedFunctionsCurrentWork in Progress (WIP)Deprecated 以下由zsh-plugin-git内容转载过来，以备查询使用。 git plugin The git plugin provides many aliases and a few useful functions. To use it, add git to the plugins array in your zshrc file: plugins=(... git) Aliases 常用 Alias Command g git ga git add gaa git add --all gcmsg git commit -m -------------------- ------------------------------------------------------------ ggp git push origin $(current_branch) ggf git push --force origin $(current_branch) gp git push gl git pull ggl git pull origin $(current_branch) -------------------- ------------------------------------------------------------ gco git checkout gcb git checkout -b gcm git checkout master gb git branch gba git branch -a gcf git config --list gd git diff 完整列表 Alias Command g git ga git add gaa git add --all gapa git add --patch gau git add --update gav git add --verbose gap git apply gb git branch gba git branch -a gbd git branch -d gbda - gbD git branch -D gbl git blame -b -w gbnm git branch --no-merged gbr git branch --remote gbs git bisect gbsb git bisect bad gbsg git bisect good gbsr git bisect reset gbss git bisect start gc git commit -v gc! git commit -v --amend gcn! git commit -v --no-edit --amend gca git commit -v -a gca! git commit -v -a --amend gcan! git commit -v -a --no-edit --amend gcans! git commit -v -a -s --no-edit --amend gcam git commit -a -m gcsm git commit -s -m gcb git checkout -b gcf git config --list gcl git clone --recurse-submodules gclean git clean -id gpristine git reset --hard && git clean -dfx gcm git checkout master gcd git checkout develop gcmsg git commit -m gco git checkout gcount git shortlog -sn gcp git cherry-pick gcpa git cherry-pick --abort gcpc git cherry-pick --continue gcs git commit -S gd git diff gdca git diff --cached gdcw git diff --cached --word-diff gdct git describe --tags $(git rev-list --tags --max-count=1) gds git diff --staged gdt git diff-tree --no-commit-id --name-only -r gdv - gdw git diff --word-diff gf git fetch gfa git fetch --all --prune gfg - gfo git fetch origin gg git gui citool gga git gui citool --amend ggf git push --force origin $(current_branch) ggfl git push --force-with-lease origin $(current_branch) ggl git pull origin $(current_branch) ggp git push origin $(current_branch) ggpnp ggl && ggp ggpull git pull origin \"$(git_current_branch)\" ggpur ggu ggpush git push origin \"$(git_current_branch)\" ggsup git branch --set-upstream-to=origin/$(git_current_branch) ggu git pull --rebase origin $(current_branch) gpsup git push --set-upstream origin $(git_current_branch) ghh git help gignore git update-index --assume-unchanged gignored - git-svn-dcommit-push git svn dcommit && git push github master:svntrunk gk gitk --all --branches gke gitk --all $(git log -g --pretty=%h) gl git pull glg git log --stat glgp git log --stat -p glgg git log --graph glgga git log --graph --decorate --all glgm git log --graph --max-count=10 glo git log --oneline --decorate glol git log --graph --pretty='%Cred%h%Creset -%C(auto)%d%Creset %s %Cgreen(%cr) %C(bold blue)%Creset' glols git log --graph --pretty='%Cred%h%Creset -%C(auto)%d%Creset %s %Cgreen(%cr) %C(bold blue)%Creset' --stat glod git log --graph --pretty='%Cred%h%Creset -%C(auto)%d%Creset %s %Cgreen(%ad) %C(bold blue)%Creset' glods git log --graph --pretty='%Cred%h%Creset -%C(auto)%d%Creset %s %Cgreen(%ad) %C(bold blue)%Creset' --date=short glola git log --graph --pretty='%Cred%h%Creset -%C(auto)%d%Creset %s %Cgreen(%cr) %C(bold blue)%Creset' --all glog git log --oneline --decorate --graph gloga git log --oneline --decorate --graph --all glp _git_log_prettily gm git merge gmom git merge origin/master gmt git mergetool --no-prompt gmtvim git mergetool --no-prompt --tool=vimdiff gmum git merge upstream/master gma git merge --abort gp git push gpd git push --dry-run gpf git push --force-with-lease gpf! git push --force gpoat git push origin --all && git push origin --tags gpu git push upstream gpv git push -v gr git remote gra git remote add grb git rebase grba git rebase --abort grbc git rebase --continue grbd git rebase develop grbi git rebase -i grbm git rebase master grbs git rebase --skip grh git reset grhh git reset --hard groh git reset origin/$(git_current_branch) --hard grm git rm grmc git rm --cached grmv git remote rename grrm git remote remove grset git remote set-url grt - gru git reset -- grup git remote update grv git remote -v gsb git status -sb gsd git svn dcommit gsh git show gsi git submodule init gsps git show --pretty=short --show-signature gsr git svn rebase gss git status -s gst git status gsta git stash push gsta git stash save gstaa git stash apply gstc git stash clear gstd git stash drop gstl git stash list gstp git stash pop gsts git stash show --text gstall git stash --all gsu git submodule update gts git tag -s gtv - gtl gtl(){ git tag --sort=-v:refname -n -l ${1}* }; noglob gtl gunignore git update-index --no-assume-unchanged gunwip - gup git pull --rebase gupv git pull --rebase -v gupa git pull --rebase --autostash gupav git pull --rebase --autostash -v glum git pull upstream master gwch git whatchanged -p --abbrev-commit --pretty=medium gwip git add -A; git rm $(git ls-files --deleted) 2> /dev/null; git commit --no-verify --no-gpg-sign -m \"--wip-- [skip ci]\" Deprecated These are aliases that have been removed, renamed, or otherwise modified in a way that may, or may not, receive further support. Alias Command Modification gap git add --patch new alias gapa gcl git config --list new alias gcf gdc git diff --cached new alias gdca gdt git difftool no replacement ggpull git pull origin $(current_branch) new alias ggl (ggpull still exists for now though) ggpur git pull --rebase origin $(current_branch) new alias ggu (ggpur still exists for now though) ggpush git push origin $(current_branch) new alias ggp (ggpush still exists for now though) gk gitk --all --branches now aliased to gitk --all --branches glg git log --stat --max-count = 10 now aliased to git log --stat --color glgg git log --graph --max-count = 10 now aliased to git log --graph --color gwc git whatchanged -p --abbrev-commit --pretty = medium new alias gwch Functions Current Command Description current_branch Return the name of the current branch git_current_user_name Returns the user.name config value git_current_user_email Returns the user.email config value Work in Progress (WIP) These features allow to pause a branch development and switch to another one (\"Work in Progress\", or wip). When you want to go back to work, just unwip it. Command Description work_in_progress Echoes a warning if the current branch is a wip gwip Commit wip branch gunwip Uncommit wip branch Deprecated Command Description Reason current_repository Return the names of the current remotes Didn't work properly. Use git remote -v instead (grv alias) 参考 https://github.com/robbyrussell/oh-my-zsh/tree/master/plugins/git Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2023-03-06 10:57:30 "},"mysql/system-manage.html":{"url":"mysql/system-manage.html","title":"系统管理","keywords":"","body":"1. 系统管理1.1. 连接mysql1.2. 备份数据库1.3. 用户管理1.4. 权限管理1.4.1. grant1.4.2. 普通数据库用户1.4.3. DBA 用户1.4.4. 查看用户权限1.4.5. 权限列表1.4.6.查看主从关系1. 系统管理 1.1. 连接mysql 快速部署docker mysql docker pull mysql:5.7 启动MySQL mkdir -p ~/data/mysql docker run --name my-mysql -v ~/data/mysql:/var/lib/mysql -p 3306:3306 -e MYSQL_ROOT_PASSWORD=123456 -d mysql:5.7 格式： mysql -h主机地址 -u用户名 －p用户密码 #连接本地 mysql -h -u用户名 －p用户密码 #连接远程 mysql -h -u用户名 －p用户密码 #退出连接 exit 1.2. 备份数据库 1.导出整个数据库 导出文件默认是存在mysql\\bin目录下 #1）备份单个数据库 mysqldump -u 用户名 -p 数据库名 > 导出的文件名 mysqldump -u user_name -p123456 database_name > outfile_name.sql #2）同时备份多个数据库，例如database1_name，database2_name mysqldump -u user_name -p123456 --databases database1_name database2_name > outfile_name.sql #3）备份全部数据库 mysqldump -u user_name -p123456 --all-databases > outfile_name.sql 2.导出一个表 mysqldump -u 用户名 -p 数据库名 表名> 导出的文件名 mysqldump -u user_name -p database_name table_name > outfile_name.sql 3.导出一个数据库结构 mysqldump -u user_name -p -d –add-drop-table database_name > outfile_name.sql -d 没有数据 –add-drop-table 在每个create语句之前增加一个drop table 4.带语言参数导出 mysqldump -uroot -p –default-character-set=latin1 –set-charset=gbk –skip-opt database_name > outfile_name.sql 5、导入数据库 #1）多个个数据库 mysql -u root –p 1.3. 用户管理 #创建用户 create user '用户名'@'IP地址' identified by '密码'; #删除用户 drop user '用户名'@'IP地址'; delete from user where user='用户名' and host='localhost'; #修改用户 rename user '用户名'@'IP地址'; to '新用户名'@'IP地址';; #修改密码 set password for '用户名'@'IP地址' = Password('新密码') mysqladmin -u用户名 -p旧密码 password 新密码 1.4. 权限管理 1.4.1. grant 1、grant 权限 on 数据库对象 to 用户 数据库对象的格式为.。.*：表示授权数据库对象该数据库的所有表；*.*：表示授权数据库对象为所有数据库的所有表。 grant all privileges on . to @'' identified by '';如果为'%'表示不限制IP。 2、撤销权限： revoke all on . from @; 1.4.2. 普通数据库用户 查询、插入、更新、删除 数据库中所有表数据的权利 grant select, insert, update, delete on testdb.* to @''; 1.4.3. DBA 用户 #1、授权 grant all privileges on . to @'' identified by ''; #2、刷新系统权限 flush privileges; 1.4.4. 查看用户权限 #查看当前用户（自己）权限 show grants; #查看指定MySQL 用户权限 show grants for @; #查看user和host select user,host from mysql.user order by user; 1.4.5. 权限列表 权限 说明 网站使用账户是否给予 Select 可对其下所有表进行查询 建议给予 Insert 可对其下所有表进行插入 建议给予 Update 可对其下所有表进行更新 建议给予 Delete 可对其下所有表进行删除 建议给予 Create 可在此数据库下创建表或索引 建议给予 Drop 可删除此数据库及数据库下所有表 不建议给予 Grant 赋予权限选项 不建议给予 References 未来MySQL特性的占位符 不建议给予 Index 可对其下所有表进行索引 建议给予 Alter 可对其下所有表进行更改 建议给予 Create_tmp_table 创建临时表 不建议给予 Lock_tables 可对其下所有表进行锁定 不建议给予 Create_view 可在此数据下创建视图 建议给予 Show_view 可在此数据下查看视图 建议给予 Create_routine 可在此数据下创建存储过程 不建议给予 Alter_routine 可在此数据下更改存储过程 不建议给予 Execute 可在此数据下执行存储过程 不建议给予 Event 可在此数据下创建事件调度器 不建议给予 Trigger 可在此数据下创建触发器 不建议给予 1.4.6.查看主从关系 #登录主机 show slave hosts; #登录从机 show slave status; Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2023-03-06 10:57:30 "},"mysql/table-operation.html":{"url":"mysql/table-operation.html","title":"数据表操作","keywords":"","body":"2. 数据库操作3. 数据表操作3.1. 创建表3.2. 查看表3.3. 删除表3.4. 清空表内容3.5. 查看表结构3.6. 修改表2. 数据库操作 #创建数据库 create database #显示数据库 show databases #删除数据 drop database 3. 数据表操作 3.1. 创建表 create table 表名( 列名 类型 是否可以为空， 列名 类型 是否可以为空 )ENGINE=InnoDB DEFAULT CHARSET=utf8 默认值，创建列时可以指定默认值，当插入数据时如果未主动设置，则自动添加默认值 自增，如果为某列设置自增列，插入数据时无需设置此列，默认将自增（表中只能有一个自增列）注意：1、对于自增列，必须是索引（含主键）2、对于自增可以设置步长和起始值 主键，一种特殊的唯一索引，不允许有空值，如果主键使用单个列，则它的值必须唯一，如果是多列，则其组合必须唯一。 3.2. 查看表 show tables; # 查看数据库全部表 select * from 表名; # 查看表所有内容 3.3. 删除表 drop table 表名 3.4. 清空表内容 delete from 表名 truncate table 表名 3.5. 查看表结构 desc 表名 3.6. 修改表 列操作 #添加列 alter table 表名 add 列名 类型 alter table 表名 add 列名 类型 after `列名` #删除列 alter table 表名 drop column 列名 #修改列 alter table 表名 modify column 列名 类型; -- 类型 alter table 表名 change 原列名 新列名 类型; -- 列名，类型 主键操作 #添加主键 alter table 表名 add primary key(列名); #删除主键 alter table 表名 drop primary key; alter table 表名 modify 列名 int, drop primary key; #修改主键：先删除后添加 alter table 表名 drop primary key; alter table 表名 add primary key(列名); #添加外键 alter table 从表 add constraint 外键名称（形如：FK从表主表） foreign key 从表(外键字段) references 主表(主键字段); #删除外键 alter table 表名 drop foreign key 外键名称 默认值操作 #修改默认值： ALTER TABLE testalter_tbl ALTER i SET DEFAULT 1000; #删除默认值： ALTER TABLE testalter_tbl ALTER i DROP DEFAULT; 调整表结构字段顺序 alter table modify varchar(10) after ; alter table modify id int(10) unsigned auto_increment first; Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2023-03-06 10:57:30 "},"mysql/curd-commands.html":{"url":"mysql/curd-commands.html","title":"表内容操作","keywords":"","body":"4. 表内容操作4.1. 增4.2. 删4.3. 改4.4. 查4.5. 条件判断4.5.1. where4.5.2. 通配符like4.5.3. 限制limit4.5.4. 排序asc，desc4.5.5. 分组group by4. 表内容操作 4.1. 增 insert into 表 (列名,列名...) values (值,值,...) insert into 表 (列名,列名...) values (值,值,...),(值,值,值...) insert into 表 (列名,列名...) select (列名,列名...) from 表 例： insert into tab1(name,email) values('zhangyanlin','zhangyanlin8851@163.com') 4.2. 删 delete from 表 # 删除表里全部数据 delete from 表 where id＝1 and name＝'zhangyanlin' # 删除ID =1 和name='zhangyanlin' 那一行数据 4.3. 改 update 表 set name ＝ 'zhangyanlin' where id>1 4.4. 查 select * from 表 select * from 表 where id > 1 select nid,name,gender as gg from 表 where id > 1 4.5. 条件判断 4.5.1. where select * from where id >1 and name!='huwh' and num =12; select * from where id between 5 and 6; select * from where id in (11,22,33); select * from where id not in (11,22,33); select * from where id in (select nid from ) 4.5.2. 通配符like select * from where name like 'hu%'; #hu开头 select * from where name like 'hu_' #hu开头后接一个字符 4.5.3. 限制limit select * from limit 5; #前5行 select * from limit 4,5 #从第四行开始的5行 select * from limit 5 offset 4;#从第四行开始的5行 4.5.4. 排序asc，desc select * from order by 列 asc; #跟据“列”从小到大排序（不指定默认为从小到大排序） select * from order by 列 desc; #根据“列”从大到小排序 select * from order by 列1 desc,列2 asc; #根据“列1”从大到小排序，如果相同则按“列2”从小到大排序 4.5.5. 分组group by group by 必须在where之后，order by之前。 select num,from group by num; select num,nid from group by num,nid; select num from where nid > 10 group by num,nid order nid desc; select num,nid,count(*),sum(score),max(score) from group by num; select num from group by num having max(id) > 10; select num from group by num; Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2023-03-06 10:57:30 "},"redis/redis-introduction.html":{"url":"redis/redis-introduction.html","title":"Redis介绍","keywords":"","body":"1. redis是什么？（what）2. 为什么使用redis？（why）2.1. redis的特点2.2. redis的优势2.3. redis与其他key-value存储有什么不同3. 如何使用redis？（how）3.1. redis的数据类型3.2. redis常用命令1. redis是什么？（what） Redis是一个开源（BSD许可），内存存储的数据结构服务器，可用作数据库，高速缓存和消息队列代理。它支持字符串、哈希表、列表、集合、有序集合，位图，hyperloglogs等数据类型。内置复制、Lua脚本、LRU收回、事务以及不同级别磁盘持久化功能，同时通过Redis Sentinel提供高可用，通过Redis Cluster提供自动分区。 Redis是一个开源的使用ANSI C语言编写、遵守BSD协议、支持网络、可基于内存亦可持久化的日志型、Key-Value数据库，并提供多种语言的API。 2. 为什么使用redis？（why） 2.1. redis的特点 Redis支持数据的持久化，可以将内存中的数据保持在磁盘中，重启的时候可以再次加载进行使用。 Redis不仅仅支持简单的key-value类型的数据，同时还提供list，set，zset，hash等数据结构的存储。 Redis支持数据的备份，即master-slave模式的数据备份。 2.2. redis的优势 性能极高 – Redis能读的速度是110000次/s,写的速度是81000次/s 。 丰富的数据类型 – Redis支持二进制案例的 Strings, Lists, Hashes, Sets 及 Ordered Sets 数据类型操作。 原子 – Redis的所有操作都是原子性的，同时Redis还支持对几个操作全并后的原子性执行。 丰富的特性 – Redis还支持 publish/subscribe, 通知, key 过期等等特性。 2.3. redis与其他key-value存储有什么不同 Redis有着更为复杂的数据结构并且提供对他们的原子性操作，Redis的数据类型都是基于基本数据结构的同时对程序员透明，无需进行额外的抽象。 Redis运行在内存中但是可以持久化到磁盘，所以在对不同数据集进行高速读写时需要权衡内存，应为数据量不能大于硬件内存。 相比在磁盘上相同的复杂的数据结构，在内存中操作起来非常简单，这样Redis可以做很多内部复杂性很强的事情。 在磁盘格式方面他们是紧凑的以追加的方式产生的，因为他们并不需要进行随机访问。 3. 如何使用redis？（how） 3.1. redis的数据类型 数据类型 概念 常用命令 String(字符串) key-value型 SET ，GET Hash(哈希) field-value,适用于存储对象类型（对象名-对象属性值） HMSET，HEGTALL List(列表) string类型的有序列表，按照插入顺序排序 lpush，lrange Set(集合) string类型的无序集合 sadd，smembers zset(sorted set：有序集合) string类型元素的集合,且不允许重复的成员。每个元素关联一个double值来进行排序，double值可以重复但元素不能重复。 zadd，ZRANGEBYSCORE 3.2. redis常用命令 Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2023-03-06 10:57:30 "},"redis/redis-cluster.html":{"url":"redis/redis-cluster.html","title":"Redis集群模式部署","keywords":"","body":"1. Redis部署1.1 下载和编译1.2 启动服务1.3 客户端测试2. Redis集群部署2.1 手动部署集群2.1.1 设置配置文件及启动实例2.1.2 redis-trib创建集群2.1.3 部署结果验证1. Redis部署 以下以Linux系统为例 1.1 下载和编译 $ wget http://download.redis.io/releases/redis-4.0.7.tar.gz $ tar xzf redis-4.0.7.tar.gz $ cd redis-4.0.7 $ make 编译完成后会在src目录下生成Redis服务端程序redis-server和客户端程序redis-cli。 1.2 启动服务 1、前台运行 src/redis-server 该方式启动默认为前台方式运行，使用默认配置。 2、后台运行 可以修改redis.conf文件的daemonize参数为yes，指定配置文件启动，例如： vi redis.conf # By default Redis does not run as a daemon. Use 'yes' if you need it. # Note that Redis will write a pid file in /var/run/redis.pid when daemonized. daemonize yes 指定配置文件启动。 src/redis-server redis.conf 例如： #指定配置文件后台启动 [root@kube-node-1 redis-4.0.7]# src/redis-server redis.conf 95778:C 30 Jan 00:44:37.633 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo 95778:C 30 Jan 00:44:37.634 # Redis version=4.0.7, bits=64, commit=00000000, modified=0, pid=95778, just started 95778:C 30 Jan 00:44:37.634 # Configuration loaded #查看Redis进程 [root@kube-node-1 redis-4.0.7]# ps aux|grep redis root 95779 0.0 0.0 145268 468 ? Ssl 00:44 0:00 src/redis-server 127.0.0.1:6379 更多启动参数如下： [root@kube-node-1 src]# ./redis-server --help Usage: ./redis-server [/path/to/redis.conf] [options] ./redis-server - (read config from stdin) ./redis-server -v or --version ./redis-server -h or --help ./redis-server --test-memory Examples: ./redis-server (run the server with default conf) ./redis-server /etc/redis/6379.conf ./redis-server --port 7777 ./redis-server --port 7777 --slaveof 127.0.0.1 8888 ./redis-server /etc/myredis.conf --loglevel verbose Sentinel mode: ./redis-server /etc/sentinel.conf --sentinel 1.3 客户端测试 $ src/redis-cli redis> set foo bar OK redis> get foo \"bar\" 2. Redis集群部署 Redis的集群部署需要在每台集群部署的机器上安装Redis（可参考上述的[Redis安装] ），然后修改配置以集群的方式启动。 2.1 手动部署集群 2.1.1 设置配置文件及启动实例 修改配置文件redis.conf，集群模式的最小化配置文件如下： #可选操作，该项设置后台方式运行， daemonize yes port 7000 cluster-enabled yes cluster-config-file nodes.conf cluster-node-timeout 5000 appendonly yes 更多集群配置参数可参考默认配置文件redis.conf中Cluster模块的说明 最小集群模式需要三个master实例，一般建议起六个实例，即三主三从。因此我们创建6个以端口号命名的目录存放实例的配置文件和其他信息。 mkdir cluster-test cd cluster-test mkdir 7000 7001 7002 7003 7004 7005 在对应端口号的目录中创建redis.conf的文件，配置文件的内容可参考上述的集群模式配置。每个配置文件中的端口号port参数改为对应目录的端口号。 复制redis-server的二进制文件到cluster-test目录中，通过指定配置文件的方式启动redis服务，例如： cd 7000 ../redis-server ./redis.conf 如果是以前台方式运行，则会在控制台输出以下信息： [82462] 26 Nov 11:56:55.329 * No cluster configuration found, I'm 97a3a64667477371c4479320d683e4c8db5858b1 每个实例都会生成一个Node ID，类似97a3a64667477371c4479320d683e4c8db5858b1，用来作为Redis实例在集群中的唯一标识，而不是通过IP和Port，IP和Port可能会改变，该Node ID不会改变。 目录结构可参考： cluster-test/ ├── 7000 │ ├── appendonly.aof │ ├── dump.rdb │ ├── nodes.conf │ └── redis.conf ├── 7001 │ ├── appendonly.aof │ ├── dump.rdb │ ├── nodes.conf │ └── redis.conf ├── 7002 │ ├── appendonly.aof │ ├── dump.rdb │ ├── nodes.conf │ └── redis.conf ├── 7003 │ ├── appendonly.aof │ ├── dump.rdb │ ├── nodes.conf │ └── redis.conf ├── 7004 │ ├── appendonly.aof │ ├── dump.rdb │ ├── nodes.conf │ └── redis.conf ├── 7005 │ ├── appendonly.aof │ ├── dump.rdb │ ├── nodes.conf │ └── redis.conf ├── redis-cli └── redis-server 2.1.2 redis-trib创建集群 Redis的实例全部运行之后，还需要redis-trib.rb工具来完成集群的创建，redis-trib.rb二进制文件在Redis包主目录下的src目录中，运行该工具依赖Ruby环境和gem，因此需要提前安装。 1、安装Ruby yum -y install ruby rubygems 查看Ruby版本信息。 [root@kube-node-1 src]# ruby --version ruby 2.0.0p648 (2015-12-16) [x86_64-linux] 由于centos系统默认支持Ruby版本为2.0.0，因此执行gem install redis命令时会报以下错误。 [root@kube-node-1 src]# gem install redis Fetching: redis-4.0.1.gem (100%) ERROR: Error installing redis: redis requires Ruby version >= 2.2.2. 解决方法是先安装rvm，再升级ruby版本。 2、安装rvm curl -L get.rvm.io | bash -s stable 如果遇到以下报错，则执行报错中的gpg2 --recv-keys的命令。 [root@kube-node-1 ~]# curl -L get.rvm.io | bash -s stable % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 194 100 194 0 0 335 0 --:--:-- --:--:-- --:--:-- 335 100 24090 100 24090 0 0 17421 0 0:00:01 0:00:01 --:--:-- 44446 Downloading https://github.com/rvm/rvm/archive/1.29.3.tar.gz Downloading https://github.com/rvm/rvm/releases/download/1.29.3/1.29.3.tar.gz.asc gpg: 于 2017年09月11日 星期一 04时59分21秒 CST 创建的签名，使用 RSA，钥匙号 BF04FF17 gpg: 无法检查签名：没有公钥 Warning, RVM 1.26.0 introduces signed releases and automated check of signatures when GPG software found. Assuming you trust Michal Papis import the mpapis public key (downloading the signatures). GPG signature verification failed for '/usr/local/rvm/archives/rvm-1.29.3.tgz' - 'https://github.com/rvm/rvm/releases/download/1.29.3/1.29.3.tar.gz.asc'! Try to install GPG v2 and then fetch the public key: gpg2 --recv-keys 409B6B1796C275462A1703113804BB82D39DC0E3 or if it fails: command curl -sSL https://rvm.io/mpapis.asc | gpg2 --import - the key can be compared with: https://rvm.io/mpapis.asc https://keybase.io/mpapis NOTE: GPG version 2.1.17 have a bug which cause failures during fetching keys from remote server. Please downgrade or upgrade to newer version (if available) or use the second method described above. 执行报错中的gpg2 --recv-keys的命令。 例如： [root@kube-node-1 ~]# gpg2 --recv-keys 409B6B1796C275462A1703113804BB82D39DC0E3 gpg: 钥匙环‘/root/.gnupg/secring.gpg’已建立 gpg: 下载密钥‘D39DC0E3’，从 hkp 服务器 keys.gnupg.net gpg: /root/.gnupg/trustdb.gpg：建立了信任度数据库 gpg: 密钥 D39DC0E3：公钥“Michal Papis (RVM signing) ”已导入 gpg: 没有找到任何绝对信任的密钥 gpg: 合计被处理的数量：1 gpg: 已导入：1 (RSA: 1) 再次执行命令curl -L get.rvm.io | bash -s stable。例如： [root@kube-node-1 ~]# curl -L get.rvm.io | bash -s stable % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 194 100 194 0 0 310 0 --:--:-- --:--:-- --:--:-- 309 100 24090 100 24090 0 0 18230 0 0:00:01 0:00:01 --:--:-- 103k Downloading https://github.com/rvm/rvm/archive/1.29.3.tar.gz Downloading https://github.com/rvm/rvm/releases/download/1.29.3/1.29.3.tar.gz.asc gpg: 于 2017年09月11日 星期一 04时59分21秒 CST 创建的签名，使用 RSA，钥匙号 BF04FF17 gpg: 完好的签名，来自于“Michal Papis (RVM signing) ” gpg: 亦即“Michal Papis ” gpg: 亦即“[jpeg image of size 5015]” gpg: 警告：这把密钥未经受信任的签名认证！ gpg: 没有证据表明这个签名属于它所声称的持有者。 主钥指纹： 409B 6B17 96C2 7546 2A17 0311 3804 BB82 D39D C0E3 子钥指纹： 62C9 E5F4 DA30 0D94 AC36 166B E206 C29F BF04 FF17 GPG verified '/usr/local/rvm/archives/rvm-1.29.3.tgz' Creating group 'rvm' Installing RVM to /usr/local/rvm/ Installation of RVM in /usr/local/rvm/ is almost complete: * First you need to add all users that will be using rvm to 'rvm' group, and logout - login again, anyone using rvm will be operating with `umask u=rwx,g=rwx,o=rx`. * To start using RVM you need to run `source /etc/profile.d/rvm.sh` in all your open bash windows, in rare cases you need to reopen all bash windows. 以上表示执行成功， source /usr/local/rvm/scripts/rvm 查看rvm库中已知的ruby版本 rvm list known 例如： [root@kube-node-1 ~]# rvm list known # MRI Rubies [ruby-]1.8.6[-p420] [ruby-]1.8.7[-head] # security released on head [ruby-]1.9.1[-p431] [ruby-]1.9.2[-p330] [ruby-]1.9.3[-p551] [ruby-]2.0.0[-p648] [ruby-]2.1[.10] [ruby-]2.2[.7] [ruby-]2.3[.4] [ruby-]2.4[.1] ruby-head ... 3、升级Ruby #安装ruby rvm install 2.4.0 #使用新版本 rvm use 2.4.0 #移除旧版本 rvm remove 2.0.0 #查看当前版本 ruby --version 例如： [root@kube-node-1 ~]# rvm install 2.4.0 Searching for binary rubies, this might take some time. Found remote file https://rvm_io.global.ssl.fastly.net/binaries/centos/7/x86_64/ruby-2.4.0.tar.bz2 Checking requirements for centos. Installing requirements for centos. Installing required packages: autoconf, automake, bison, bzip2, gcc-c++, libffi-devel, libtool, readline-devel, sqlite-devel, zlib-devel, libyaml-devel, openssl-devel................................ Requirements installation successful. ruby-2.4.0 - #configure ruby-2.4.0 - #download % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 14.0M 100 14.0M 0 0 852k 0 0:00:16 0:00:16 --:--:-- 980k No checksum for downloaded archive, recording checksum in user configuration. ruby-2.4.0 - #validate archive ruby-2.4.0 - #extract ruby-2.4.0 - #validate binary ruby-2.4.0 - #setup ruby-2.4.0 - #gemset created /usr/local/rvm/gems/ruby-2.4.0@global ruby-2.4.0 - #importing gemset /usr/local/rvm/gemsets/global.gems.............................. ruby-2.4.0 - #generating global wrappers........ ruby-2.4.0 - #gemset created /usr/local/rvm/gems/ruby-2.4.0 ruby-2.4.0 - #importing gemsetfile /usr/local/rvm/gemsets/default.gems evaluated to empty gem list ruby-2.4.0 - #generating default wrappers........ [root@kube-node-1 ~]# rvm use 2.4.0 Using /usr/local/rvm/gems/ruby-2.4.0 [root@kube-node-1 ~]# rvm remove 2.0.0 ruby-2.0.0-p648 - #already gone Using /usr/local/rvm/gems/ruby-2.4.0 [root@kube-node-1 ~]# ruby --version ruby 2.4.0p0 (2016-12-24 revision 57164) [x86_64-linux] 4、安装gem gem install redis 例如： [root@kube-node-1 ~]# gem install redis Fetching: redis-4.0.1.gem (100%) Successfully installed redis-4.0.1 Parsing documentation for redis-4.0.1 Installing ri documentation for redis-4.0.1 Done installing documentation for redis after 2 seconds 1 gem installed 5、执行redis-trib.rb命令 以上表示安装成功，可以执行redis-trib.rb命令。 cd src #执行redis-trib.rb命令 ./redis-trib.rb create --replicas 1 127.0.0.1:7000 127.0.0.1:7001 \\ > 127.0.0.1:7002 127.0.0.1:7003 127.0.0.1:7004 127.0.0.1:7005 参数create表示创建一个新的集群，--replicas 1表示为每个master创建一个slave。 如果创建成功会显示以下信息 [OK] All 16384 slots covered 例如： [root@kube-node-1 src]# ./redis-trib.rb create --replicas 1 127.0.0.1:7000 127.0.0.1:7001 \\ > 127.0.0.1:7002 127.0.0.1:7003 127.0.0.1:7004 127.0.0.1:7005 >>> Creating cluster >>> Performing hash slots allocation on 6 nodes... Using 3 masters: 127.0.0.1:7000 127.0.0.1:7001 127.0.0.1:7002 Adding replica 127.0.0.1:7004 to 127.0.0.1:7000 Adding replica 127.0.0.1:7005 to 127.0.0.1:7001 Adding replica 127.0.0.1:7003 to 127.0.0.1:7002 >>> Trying to optimize slaves allocation for anti-affinity [WARNING] Some slaves are in the same host as their master M: d5a834d075fd93eefab877c6ebb86efff680650f 127.0.0.1:7000 slots:0-5460 (5461 slots) master M: 13d0c397604a0b2644244c37b666fce83f29faa8 127.0.0.1:7001 slots:5461-10922 (5462 slots) master M: be2718476eba4e56f696e56b75e67df720b7fc24 127.0.0.1:7002 slots:10923-16383 (5461 slots) master S: 3d02f59b34047486faecc023685379de7b38076c 127.0.0.1:7003 replicates 13d0c397604a0b2644244c37b666fce83f29faa8 S: dedf672f0a75faf37407ac4edd5da23bc4651e25 127.0.0.1:7004 replicates be2718476eba4e56f696e56b75e67df720b7fc24 S: 99c07119a449a703583019f7699e15afa0e41952 127.0.0.1:7005 replicates d5a834d075fd93eefab877c6ebb86efff680650f Can I set the above configuration? (type 'yes' to accept): yes >>> Nodes configuration updated >>> Assign a different config epoch to each node >>> Sending CLUSTER MEET messages to join the cluster Waiting for the cluster to join.... >>> Performing Cluster Check (using node 127.0.0.1:7000) M: d5a834d075fd93eefab877c6ebb86efff680650f 127.0.0.1:7000 slots:0-5460 (5461 slots) master 1 additional replica(s) M: be2718476eba4e56f696e56b75e67df720b7fc24 127.0.0.1:7002 slots:10923-16383 (5461 slots) master 1 additional replica(s) M: 13d0c397604a0b2644244c37b666fce83f29faa8 127.0.0.1:7001 slots:5461-10922 (5462 slots) master 1 additional replica(s) S: 3d02f59b34047486faecc023685379de7b38076c 127.0.0.1:7003 slots: (0 slots) slave replicates 13d0c397604a0b2644244c37b666fce83f29faa8 S: 99c07119a449a703583019f7699e15afa0e41952 127.0.0.1:7005 slots: (0 slots) slave replicates d5a834d075fd93eefab877c6ebb86efff680650f S: dedf672f0a75faf37407ac4edd5da23bc4651e25 127.0.0.1:7004 slots: (0 slots) slave replicates be2718476eba4e56f696e56b75e67df720b7fc24 [OK] All nodes agree about slots configuration. >>> Check for open slots... >>> Check slots coverage... [OK] All 16384 slots covered. 2.1.3 部署结果验证 1、客户端访问 使用客户端redis-cli二进制访问某个实例，执行set和get的测试。 $ redis-cli -c -p 7000 redis 127.0.0.1:7000> set foo bar -> Redirected to slot [12182] located at 127.0.0.1:7002 OK redis 127.0.0.1:7002> set hello world -> Redirected to slot [866] located at 127.0.0.1:7000 OK redis 127.0.0.1:7000> get foo -> Redirected to slot [12182] located at 127.0.0.1:7002 \"bar\" redis 127.0.0.1:7000> get hello -> Redirected to slot [866] located at 127.0.0.1:7000 \"world\" 2、查看集群状态 使用cluster info命令查看集群状态。 127.0.0.1:7000> cluster info cluster_state:ok #集群状态 cluster_slots_assigned:16384 #被分配的槽位数 cluster_slots_ok:16384 #正确分配的槽位 cluster_slots_pfail:0 cluster_slots_fail:0 cluster_known_nodes:6 #当前节点 cluster_size:3 cluster_current_epoch:6 cluster_my_epoch:1 cluster_stats_messages_ping_sent:48273 cluster_stats_messages_pong_sent:49884 cluster_stats_messages_sent:98157 cluster_stats_messages_ping_received:49879 cluster_stats_messages_pong_received:48273 cluster_stats_messages_meet_received:5 cluster_stats_messages_received:98157 3、查看节点状态 使用cluster nodes命令查看节点状态。 127.0.0.1:7000> cluster nodes be2718476eba4e56f696e56b75e67df720b7fc24 127.0.0.1:7002@17002 master - 0 1517303607000 3 connected 10923-16383 13d0c397604a0b2644244c37b666fce83f29faa8 127.0.0.1:7001@17001 master - 0 1517303606000 2 connected 5461-10922 3d02f59b34047486faecc023685379de7b38076c 127.0.0.1:7003@17003 slave 13d0c397604a0b2644244c37b666fce83f29faa8 0 1517303606030 4 connected d5a834d075fd93eefab877c6ebb86efff680650f 127.0.0.1:7000@17000 myself,master - 0 1517303604000 1 connected 0-5460 99c07119a449a703583019f7699e15afa0e41952 127.0.0.1:7005@17005 slave d5a834d075fd93eefab877c6ebb86efff680650f 0 1517303607060 6 connected dedf672f0a75faf37407ac4edd5da23bc4651e25 127.0.0.1:7004@17004 slave be2718476eba4e56f696e56b75e67df720b7fc24 0 1517303608082 5 connected 参考文章： https://redis.io/download https://redis.io/topics/cluster-tutorial Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2023-03-06 10:57:30 "},"redis/redis-sentinel.html":{"url":"redis/redis-sentinel.html","title":"Redis主从及哨兵模式部署","keywords":"","body":"1. 部署Redis集群1.1 部署与配置1.2 配置主从关系2. 部署sentinel集群2.1 部署与配置2.2 启动sentinel实例2.3 查看状态1. 部署Redis集群 redis的安装及配置参考[redis部署] 本文以创建一主二从的集群为例。 1.1 部署与配置 先创建sentinel目录，在该目录下创建8000，8001，8002三个以端口号命名的目录。 mkdir sentinel cd sentinel mkdir 8000 8001 8002 在对应端口号目录中创建redis.conf的文件，配置文件中的端口号port参数改为对应目录的端口号。配置如下： # 守护进程模式 daemonize yes # pid file pidfile /var/run/redis.pid # 监听端口 port 8000 # TCP接收队列长度，受/proc/sys/net/core/somaxconn和tcp_max_syn_backlog这两个内核参数的影响 tcp-backlog 511 # 一个客户端空闲多少秒后关闭连接(0代表禁用，永不关闭) timeout 0 # 如果非零，则设置SO_KEEPALIVE选项来向空闲连接的客户端发送ACK tcp-keepalive 60 # 指定服务器调试等级 # 可能值： # debug （大量信息，对开发/测试有用） # verbose （很多精简的有用信息，但是不像debug等级那么多） # notice （适量的信息，基本上是你生产环境中需要的） # warning （只有很重要/严重的信息会记录下来） loglevel notice # 指明日志文件名 logfile \"./redis8000.log\" # 设置数据库个数 databases 16 # 会在指定秒数和数据变化次数之后把数据库写到磁盘上 # 900秒（15分钟）之后，且至少1次变更 # 300秒（5分钟）之后，且至少10次变更 # 60秒之后，且至少10000次变更 save 900 1 save 300 10 save 60 10000 # 默认如果开启RDB快照(至少一条save指令)并且最新的后台保存失败，Redis将会停止接受写操作 # 这将使用户知道数据没有正确的持久化到硬盘，否则可能没人注意到并且造成一些灾难 stop-writes-on-bgsave-error yes # 当导出到 .rdb 数据库时是否用LZF压缩字符串对象 rdbcompression yes # 版本5的RDB有一个CRC64算法的校验和放在了文件的最后。这将使文件格式更加可靠。 rdbchecksum yes # 持久化数据库的文件名 dbfilename dump.rdb # 工作目录 dir ./ # 当master服务设置了密码保护时，slave服务连接master的密码 masterauth 0234kz9*l # 当一个slave失去和master的连接，或者同步正在进行中，slave的行为可以有两种： # # 1) 如果 slave-serve-stale-data 设置为 \"yes\" (默认值)，slave会继续响应客户端请求， # 可能是正常数据，或者是过时了的数据，也可能是还没获得值的空数据。 # 2) 如果 slave-serve-stale-data 设置为 \"no\"，slave会回复\"正在从master同步 # （SYNC with master in progress）\"来处理各种请求，除了 INFO 和 SLAVEOF 命令。 slave-serve-stale-data yes # 你可以配置salve实例是否接受写操作。可写的slave实例可能对存储临时数据比较有用(因为写入salve # 的数据在同master同步之后将很容易被删除 slave-read-only yes # 是否在slave套接字发送SYNC之后禁用 TCP_NODELAY？ # 如果你选择“yes”Redis将使用更少的TCP包和带宽来向slaves发送数据。但是这将使数据传输到slave # 上有延迟，Linux内核的默认配置会达到40毫秒 # 如果你选择了 \"no\" 数据传输到salve的延迟将会减少但要使用更多的带宽 repl-disable-tcp-nodelay no # slave的优先级是一个整数展示在Redis的Info输出中。如果master不再正常工作了，哨兵将用它来 # 选择一个slave提升=升为master。 # 优先级数字小的salve会优先考虑提升为master，所以例如有三个slave优先级分别为10，100，25， # 哨兵将挑选优先级最小数字为10的slave。 # 0作为一个特殊的优先级，标识这个slave不能作为master，所以一个优先级为0的slave永远不会被 # 哨兵挑选提升为master slave-priority 100 # 密码验证 # 警告：因为Redis太快了，所以外面的人可以尝试每秒150k的密码来试图破解密码。这意味着你需要 # 一个高强度的密码，否则破解太容易了 requirepass 0234kz9*l # redis实例最大占用内存，不要用比设置的上限更多的内存。一旦内存使用达到上限，Redis会根据选定的回收策略（参见： # maxmemmory-policy）删除key maxmemory 3gb # 最大内存策略：如果达到内存限制了，Redis如何选择删除key。你可以在下面五个行为里选： # volatile-lru -> 根据LRU算法删除带有过期时间的key。 # allkeys-lru -> 根据LRU算法删除任何key。 # volatile-random -> 根据过期设置来随机删除key, 具备过期时间的key。 # allkeys->random -> 无差别随机删, 任何一个key。 # volatile-ttl -> 根据最近过期时间来删除（辅以TTL）, 这是对于有过期时间的key # noeviction -> 谁也不删，直接在写操作时返回错误。 maxmemory-policy volatile-lru # 默认情况下，Redis是异步的把数据导出到磁盘上。这种模式在很多应用里已经足够好，但Redis进程 # 出问题或断电时可能造成一段时间的写操作丢失(这取决于配置的save指令)。 # # AOF是一种提供了更可靠的替代持久化模式，例如使用默认的数据写入文件策略（参见后面的配置） # 在遇到像服务器断电或单写情况下Redis自身进程出问题但操作系统仍正常运行等突发事件时，Redis # 能只丢失1秒的写操作。 # # AOF和RDB持久化能同时启动并且不会有问题。 # 如果AOF开启，那么在启动时Redis将加载AOF文件，它更能保证数据的可靠性。 appendonly no # aof文件名 appendfilename \"appendonly.aof\" # fsync() 系统调用告诉操作系统把数据写到磁盘上，而不是等更多的数据进入输出缓冲区。 # 有些操作系统会真的把数据马上刷到磁盘上；有些则会尽快去尝试这么做。 # # Redis支持三种不同的模式： # # no：不要立刻刷，只有在操作系统需要刷的时候再刷。比较快。 # always：每次写操作都立刻写入到aof文件。慢，但是最安全。 # everysec：每秒写一次。折中方案。 appendfsync everysec # 如果AOF的同步策略设置成 \"always\" 或者 \"everysec\"，并且后台的存储进程（后台存储或写入AOF # 日志）会产生很多磁盘I/O开销。某些Linux的配置下会使Redis因为 fsync()系统调用而阻塞很久。 # 注意，目前对这个情况还没有完美修正，甚至不同线程的 fsync() 会阻塞我们同步的write(2)调用。 # # 为了缓解这个问题，可以用下面这个选项。它可以在 BGSAVE 或 BGREWRITEAOF 处理时阻止主进程进行fsync()。 # # 这就意味着如果有子进程在进行保存操作，那么Redis就处于\"不可同步\"的状态。 # 这实际上是说，在最差的情况下可能会丢掉30秒钟的日志数据。（默认Linux设定） # # 如果你有延时问题把这个设置成\"yes\"，否则就保持\"no\"，这是保存持久数据的最安全的方式。 no-appendfsync-on-rewrite yes # 自动重写AOF文件 auto-aof-rewrite-percentage 100 auto-aof-rewrite-min-size 64mb # AOF文件可能在尾部是不完整的（这跟system关闭有问题，尤其是mount ext4文件系统时 # 没有加上data=ordered选项。只会发生在os死时，redis自己死不会不完整）。 # 那redis重启时load进内存的时候就有问题了。 # 发生的时候，可以选择redis启动报错，并且通知用户和写日志，或者load尽量多正常的数据。 # 如果aof-load-truncated是yes，会自动发布一个log给客户端然后load（默认）。 # 如果是no，用户必须手动redis-check-aof修复AOF文件才可以。 # 注意，如果在读取的过程中，发现这个aof是损坏的，服务器也是会退出的， # 这个选项仅仅用于当服务器尝试读取更多的数据但又找不到相应的数据时。 aof-load-truncated yes # Lua 脚本的最大执行时间，毫秒为单位 lua-time-limit 5000 # Redis慢查询日志可以记录超过指定时间的查询 slowlog-log-slower-than 10000 # 这个长度没有限制。只是要主要会消耗内存。你可以通过 SLOWLOG RESET 来回收内存。 slowlog-max-len 128 # redis延时监控系统在运行时会采样一些操作，以便收集可能导致延时的数据根源。 # 通过 LATENCY命令 可以打印一些图样和获取一些报告，方便监控 # 这个系统仅仅记录那个执行时间大于或等于预定时间（毫秒）的操作, # 这个预定时间是通过latency-monitor-threshold配置来指定的， # 当设置为0时，这个监控系统处于停止状态 latency-monitor-threshold 0 # Redis能通知 Pub/Sub 客户端关于键空间发生的事件，默认关闭 notify-keyspace-events \"\" # 当hash只有少量的entry时，并且最大的entry所占空间没有超过指定的限制时，会用一种节省内存的 # 数据结构来编码。可以通过下面的指令来设定限制 hash-max-ziplist-entries 512 hash-max-ziplist-value 64 # 与hash似，数据元素较少的list，可以用另一种方式来编码从而节省大量空间。 # 这种特殊的方式只有在符合下面限制时才可以用 list-max-ziplist-entries 512 list-max-ziplist-value 64 # set有一种特殊编码的情况：当set数据全是十进制64位有符号整型数字构成的字符串时。 # 下面这个配置项就是用来设置set使用这种编码来节省内存的最大长度。 set-max-intset-entries 512 # 与hash和list相似，有序集合也可以用一种特别的编码方式来节省大量空间。 # 这种编码只适合长度和元素都小于下面限制的有序集合 zset-max-ziplist-entries 128 zset-max-ziplist-value 64 # HyperLogLog稀疏结构表示字节的限制。该限制包括 # 16个字节的头。当HyperLogLog使用稀疏结构表示 # 这些限制，它会被转换成密度表示。 # 值大于16000是完全没用的，因为在该点 # 密集的表示是更多的内存效率。 # 建议值是3000左右，以便具有的内存好处, 减少内存的消耗 hll-sparse-max-bytes 3000 # 启用哈希刷新，每100个CPU毫秒会拿出1个毫秒来刷新Redis的主哈希表（顶级键值映射表） activerehashing yes # 客户端的输出缓冲区的限制，可用于强制断开那些因为某种原因从服务器读取数据的速度不够快的客户端 client-output-buffer-limit normal 0 0 0 client-output-buffer-limit slave 256mb 64mb 60 client-output-buffer-limit pubsub 32mb 8mb 60 # 默认情况下，“hz”的被设定为10。提高该值将在Redis空闲时使用更多的CPU时，但同时当有多个key # 同时到期会使Redis的反应更灵敏，以及超时可以更精确地处理 hz 10 # 当一个子进程重写AOF文件时，如果启用下面的选项，则文件每生成32M数据会被同步 aof-rewrite-incremental-fsync yes 1.2 配置主从关系 1、启动实例 三个Redis实例配置相同，分别启动三个Redis实例。建议将redis-server、redis-cli、redis-sentinel的二进制复制到/usr/local/bin的目录下。 cd 8000 redis-server redis.conf 2、配置主从关系 例如，将8000端口实例设为主，8001和8002端口的实例设为从。 则分别登录8001和8002的实例，执行slaveof 命令。 例如： [root@kube-node-1 8000]# redis-cli -c -p 8001 -a 0234kz9*l 127.0.0.1:8001> slaveof 127.0.0.1 8000 OK 3、检查集群状态 登录master和slave实例，执行info replication查看集群状态。 Master [root@kube-node-1 8000]# redis-cli -c -p 8000 -a 0234kz9*l 127.0.0.1:8000> info replication # Replication role:master connected_slaves:2 slave0:ip=127.0.0.1,port=8001,state=online,offset=2853,lag=0 slave1:ip=127.0.0.1,port=8002,state=online,offset=2853,lag=0 master_replid:4f8331d5f180a4669241ab0dd97e43508abd6d8f master_replid2:0000000000000000000000000000000000000000 master_repl_offset:2853 second_repl_offset:-1 repl_backlog_active:1 repl_backlog_size:1048576 repl_backlog_first_byte_offset:1 repl_backlog_histlen:2853 Slave [root@kube-node-1 8000]# redis-cli -c -p 8001 -a 0234kz9*l 127.0.0.1:8001> info replication # Replication role:slave master_host:127.0.0.1 master_port:8000 master_link_status:up master_last_io_seconds_ago:3 master_sync_in_progress:0 slave_repl_offset:2909 slave_priority:100 slave_read_only:1 connected_slaves:0 master_replid:4f8331d5f180a4669241ab0dd97e43508abd6d8f master_replid2:0000000000000000000000000000000000000000 master_repl_offset:2909 second_repl_offset:-1 repl_backlog_active:1 repl_backlog_size:1048576 repl_backlog_first_byte_offset:1 repl_backlog_histlen:2909 也可以往master写数据，从slave读取数据来验证。 2. 部署sentinel集群 2.1 部署与配置 在之前创建的sentinel目录中场景sentinel端口号命名的目录28000，28001，28002。 cd sentinel mkdir 28000 28001 28002 在对应端口号目录中创建redis.conf的文件，配置文件中的端口号port参数改为对应目录的端口号。配置如下： port 28000 sentinel monitor mymaster 127.0.0.1 8000 2 sentinel down-after-milliseconds mymaster 60000 sentinel failover-timeout mymaster 180000 sentinel parallel-syncs mymaster 1 2.2 启动sentinel实例 #& 表示后台运行的方式 redis-sentinel sentinel.conf & 2.3 查看状态 使用sentinel masters命令查看监控的master节点。 [root@kube-node-1 28000]# redis-cli -c -p 28000 -a 0234kz9*l 127.0.0.1:28000> 127.0.0.1:28000> ping PONG 127.0.0.1:28000> 127.0.0.1:28000> sentinel masters 1) 1) \"name\" 1) \"mymaster\" 2) \"ip\" 3) \"127.0.0.1\" 4) \"port\" 5) \"8000\" 6) \"runid\" 7) \"\" 8) \"flags\" 1) \"s_down,master,disconnected\" 2) \"link-pending-commands\" 3) \"0\" 4) \"link-refcount\" 5) \"1\" 6) \"last-ping-sent\" 7) \"187539\" 8) \"last-ok-ping-reply\" 9) \"187539\" 10) \"last-ping-reply\" 11) \"3943\" 12) \"s-down-time\" 13) \"127491\" 14) \"down-after-milliseconds\" 15) \"60000\" 16) \"info-refresh\" 17) \"1517346914642\" 18) \"role-reported\" 19) \"master\" 20) \"role-reported-time\" 21) \"187539\" 22) \"config-epoch\" 23) \"0\" 24) \"num-slaves\" 25) \"0\" 26) \"num-other-sentinels\" 27) \"0\" 28) \"quorum\" 29) \"2\" 30) \"failover-timeout\" 31) \"180000\" 32) \"parallel-syncs\" 33) \"1\" 参考文章： https://redis.io/topics/sentinel Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2023-03-06 10:57:30 "},"redis/redis-conf-cn.html":{"url":"redis/redis-conf-cn.html","title":"Redis配置详解(中文版)","keywords":"","body":"INCLUDESGENERALSNAPSHOTTINGREPLICATIONSECURITYLIMITSAPPEND ONLY MODESLOW LOGEvent notificationADVANCED CONFIG 以下为redis.conf的文件的中文描述，整理于网络 # Redis 配置文件示例 # 注意单位: 当需要配置内存大小时, 可能需要指定像1k,5GB,4M等常见格式 # # 1k => 1000 bytes # 1kb => 1024 bytes # 1m => 1000000 bytes # 1mb => 1024*1024 bytes # 1g => 1000000000 bytes # 1gb => 1024*1024*1024 bytes # # 单位是对大小写不敏感的 1GB 1Gb 1gB 是相同的。 INCLUDES ################################## INCLUDES ################################### # 可以在这里包含一个或多个其他的配置文件。如果你有一个适用于所有Redis服务器的标准配置模板 # 但也需要一些每个服务器自定义的设置，这个功能将很有用。被包含的配置文件也可以包含其他配置文件， # 所以需要谨慎的使用这个功能。 # # 注意“inclue”选项不能被admin或Redis哨兵的\"CONFIG REWRITE\"命令重写。 # 因为Redis总是使用最后解析的配置行最为配置指令的值, 你最好在这个文件的开头配置includes来 # 避免它在运行时重写配置。 # 如果相反你想用includes的配置覆盖原来的配置，你最好在该文件的最后使用include # # include /path/to/local.conf # include /path/to/other.conf GENERAL ################################ GENERAL ##################################### # 默认Rdis不会作为守护进程运行。如果需要的话配置成'yes' # 注意配置成守护进程后Redis会将进程号写入文件/var/run/redis.pid daemonize no # 当以守护进程方式运行时，默认Redis会把进程ID写到 /var/run/redis.pid。你可以在这里修改路径。 pidfile /var/run/redis.pid # 接受连接的特定端口，默认是6379 # 如果端口设置为0，Redis就不会监听TCP套接字。 port 6379 # TCP listen() backlog. # # 在高并发环境下你需要一个高backlog值来避免慢客户端连接问题。注意Linux内核默默地将这个值减小 # 到/proc/sys/net/core/somaxconn的值，所以需要确认增大somaxconn和tcp_max_syn_backlog # 两个值来达到想要的效果。 tcp-backlog 511 # 默认Redis监听服务器上所有可用网络接口的连接。可以用\"bind\"配置指令跟一个或多个ip地址来实现 # 监听一个或多个网络接口 # # 示例: # # bind 192.168.1.100 10.0.0.1 # bind 127.0.0.1 # 指定用来监听Unix套套接字的路径。没有默认值， 所以在没有指定的情况下Redis不会监听Unix套接字 # # unixsocket /tmp/redis.sock # unixsocketperm 755 # 一个客户端空闲多少秒后关闭连接。(0代表禁用，永不关闭) timeout 0 # TCP keepalive. # # 如果非零，则设置SO_KEEPALIVE选项来向空闲连接的客户端发送ACK，由于以下两个原因这是很有用的： # # 1）能够检测无响应的对端 # 2）让该连接中间的网络设备知道这个连接还存活 # # 在Linux上，这个指定的值(单位：秒)就是发送ACK的时间间隔。 # 注意：要关闭这个连接需要两倍的这个时间值。 # 在其他内核上这个时间间隔由内核配置决定 # # 这个选项的一个合理值是60秒 tcp-keepalive 0 # 指定服务器调试等级 # 可能值： # debug （大量信息，对开发/测试有用） # verbose （很多精简的有用信息，但是不像debug等级那么多） # notice （适量的信息，基本上是你生产环境中需要的） # warning （只有很重要/严重的信息会记录下来） loglevel notice # 指明日志文件名。也可以使用\"stdout\"来强制让Redis把日志信息写到标准输出上。 # 注意:如果Redis以守护进程方式运行，而设置日志显示到标准输出的话，日志会发送到/dev/null logfile \"\" # 要使用系统日志记录器，只要设置 \"syslog-enabled\" 为 \"yes\" 就可以了。 # 然后根据需要设置其他一些syslog参数就可以了。 # syslog-enabled no # 指明syslog身份 # syslog-ident redis # 指明syslog的设备。必须是user或LOCAL0 ~ LOCAL7之一。 # syslog-facility local0 # 设置数据库个数。默认数据库是 DB 0， # 可以通过select (0 SNAPSHOTTING ################################ SNAPSHOTTING ################################ # # 把数据库存到磁盘上: # # save # # 会在指定秒数和数据变化次数之后把数据库写到磁盘上。 # # 下面的例子将会进行把数据写入磁盘的操作: # 900秒（15分钟）之后，且至少1次变更 # 300秒（5分钟）之后，且至少10次变更 # 60秒之后，且至少10000次变更 # # 注意：你要想不写磁盘的话就把所有 \"save\" 设置注释掉就行了。 # # 通过添加一条带空字符串参数的save指令也能移除之前所有配置的save指令 # 像下面的例子： # save \"\" save 900 1 save 300 10 save 60 10000 # 默认如果开启RDB快照(至少一条save指令)并且最新的后台保存失败，Redis将会停止接受写操作 # 这将使用户知道数据没有正确的持久化到硬盘，否则可能没人注意到并且造成一些灾难。 # # 如果后台保存进程能重新开始工作，Redis将自动允许写操作 # # 然而如果你已经部署了适当的Redis服务器和持久化的监控，你可能想关掉这个功能以便于即使是 # 硬盘，权限等出问题了Redis也能够像平时一样正常工作， stop-writes-on-bgsave-error yes # 当导出到 .rdb 数据库时是否用LZF压缩字符串对象？ # 默认设置为 \"yes\"，因为几乎在任何情况下它都是不错的。 # 如果你想节省CPU的话你可以把这个设置为 \"no\"，但是如果你有可压缩的key和value的话， # 那数据文件就会更大了。 rdbcompression yes # 因为版本5的RDB有一个CRC64算法的校验和放在了文件的最后。这将使文件格式更加可靠但在 # 生产和加载RDB文件时，这有一个性能消耗(大约10%)，所以你可以关掉它来获取最好的性能。 # # 生成的关闭校验的RDB文件有一个0的校验和，它将告诉加载代码跳过检查 rdbchecksum yes # 持久化数据库的文件名 dbfilename dump.rdb # 工作目录 # # 数据库会写到这个目录下，文件名就是上面的 \"dbfilename\" 的值。 # # 累加文件也放这里。 # # 注意你这里指定的必须是目录，不是文件名。 dir ./ REPLICATION ################################# REPLICATION ################################# # 主从同步。通过 slaveof 指令来实现Redis实例的备份。 # 注意，这里是本地从远端复制数据。也就是说，本地可以有不同的数据库文件、绑定不同的IP、监听 # 不同的端口。 # # slaveof # 如果master设置了密码保护（通过 \"requirepass\" 选项来配置），那么slave在开始同步之前必须 # 进行身份验证，否则它的同步请求会被拒绝。 # # masterauth # 当一个slave失去和master的连接，或者同步正在进行中，slave的行为有两种可能： # # 1) 如果 slave-serve-stale-data 设置为 \"yes\" (默认值)，slave会继续响应客户端请求， # 可能是正常数据，也可能是还没获得值的空数据。 # 2) 如果 slave-serve-stale-data 设置为 \"no\"，slave会回复\"正在从master同步 # （SYNC with master in progress）\"来处理各种请求，除了 INFO 和 SLAVEOF 命令。 # slave-serve-stale-data yes # 你可以配置salve实例是否接受写操作。可写的slave实例可能对存储临时数据比较有用(因为写入salve # 的数据在同master同步之后将很容被删除)，但是如果客户端由于配置错误在写入时也可能产生一些问题。 # # 从Redis2.6默认所有的slave为只读 # # 注意:只读的slave不是为了暴露给互联网上不可信的客户端而设计的。它只是一个防止实例误用的保护层。 # 一个只读的slave支持所有的管理命令比如config,debug等。为了限制你可以用'rename-command'来 # 隐藏所有的管理和危险命令来增强只读slave的安全性 slave-read-only yes # slave根据指定的时间间隔向master发送ping请求。 # 时间间隔可以通过 repl_ping_slave_period 来设置。 # 默认10秒。 # # repl-ping-slave-period 10 # 以下选项设置同步的超时时间 # # 1）slave在与master SYNC期间有大量数据传输，造成超时 # 2）在slave角度，master超时，包括数据、ping等 # 3）在master角度，slave超时，当master发送REPLCONF ACK pings # # 确保这个值大于指定的repl-ping-slave-period，否则在主从间流量不高时每次都会检测到超时 # # repl-timeout 60 # 是否在slave套接字发送SYNC之后禁用 TCP_NODELAY ？ # # 如果你选择“yes”Redis将使用更少的TCP包和带宽来向slaves发送数据。但是这将使数据传输到slave # 上有延迟，Linux内核的默认配置会达到40毫秒 # # 如果你选择了 \"no\" 数据传输到salve的延迟将会减少但要使用更多的带宽 # # 默认我们会为低延迟做优化，但高流量情况或主从之间的跳数过多时，把这个选项设置为“yes” # 是个不错的选择。 repl-disable-tcp-nodelay no # 设置数据备份的backlog大小。backlog是一个slave在一段时间内断开连接时记录salve数据的缓冲， # 所以一个slave在重新连接时，不必要全量的同步，而是一个增量同步就足够了，将在断开连接的这段 # 时间内slave丢失的部分数据传送给它。 # # 同步的backlog越大，slave能够进行增量同步并且允许断开连接的时间就越长。 # # backlog只分配一次并且至少需要一个slave连接 # # repl-backlog-size 1mb # 当master在一段时间内不再与任何slave连接，backlog将会释放。以下选项配置了从最后一个 # slave断开开始计时多少秒后，backlog缓冲将会释放。 # # 0表示永不释放backlog # # repl-backlog-ttl 3600 # slave的优先级是一个整数展示在Redis的Info输出中。如果master不再正常工作了，哨兵将用它来 # 选择一个slave提升=升为master。 # # 优先级数字小的salve会优先考虑提升为master，所以例如有三个slave优先级分别为10，100，25， # 哨兵将挑选优先级最小数字为10的slave。 # # 0作为一个特殊的优先级，标识这个slave不能作为master，所以一个优先级为0的slave永远不会被 # 哨兵挑选提升为master # # 默认优先级为100 slave-priority 100 # 如果master少于N个延时小于等于M秒的已连接slave，就可以停止接收写操作。 # # N个slave需要是“oneline”状态 # # 延时是以秒为单位，并且必须小于等于指定值，是从最后一个从slave接收到的ping（通常每秒发送） # 开始计数。 # # This option does not GUARANTEES that N replicas will accept the write, but # will limit the window of exposure for lost writes in case not enough slaves # are available, to the specified number of seconds. # # 例如至少需要3个延时小于等于10秒的slave用下面的指令： # # min-slaves-to-write 3 # min-slaves-max-lag 10 # # 两者之一设置为0将禁用这个功能。 # # 默认 min-slaves-to-write 值是0（该功能禁用）并且 min-slaves-max-lag 值是10。 SECURITY ################################## SECURITY ################################### # 要求客户端在处理任何命令时都要验证身份和密码。 # 这个功能在有你不信任的其它客户端能够访问redis服务器的环境里非常有用。 # # 为了向后兼容的话这段应该注释掉。而且大多数人不需要身份验证(例如:它们运行在自己的服务器上) # # 警告：因为Redis太快了，所以外面的人可以尝试每秒150k的密码来试图破解密码。这意味着你需要 # 一个高强度的密码，否则破解太容易了。 # # requirepass foobared # 命令重命名 # # 在共享环境下，可以为危险命令改变名字。比如，你可以为 CONFIG 改个其他不太容易猜到的名字， # 这样内部的工具仍然可以使用，而普通的客户端将不行。 # # 例如： # # rename-command CONFIG b840fc02d524045429941cc15f59e41cb7be6c52 # # 也可以通过改名为空字符串来完全禁用一个命令 # # rename-command CONFIG \"\" # # 请注意：改变命令名字被记录到AOF文件或被传送到从服务器可能产生问题。 LIMITS ################################### LIMITS #################################### # 设置最多同时连接的客户端数量。默认这个限制是10000个客户端，然而如果Redis服务器不能配置 # 处理文件的限制数来满足指定的值，那么最大的客户端连接数就被设置成当前文件限制数减32（因 # 为Redis服务器保留了一些文件描述符作为内部使用） # # 一旦达到这个限制，Redis会关闭所有新连接并发送错误'max number of clients reached' # # maxclients 10000 # 不要用比设置的上限更多的内存。一旦内存使用达到上限，Redis会根据选定的回收策略（参见： # maxmemmory-policy）删除key # # 如果因为删除策略Redis无法删除key，或者策略设置为 \"noeviction\"，Redis会回复需要更 # 多内存的错误信息给命令。例如，SET,LPUSH等等，但是会继续响应像Get这样的只读命令。 # # 在使用Redis作为LRU缓存，或者为实例设置了硬性内存限制的时候（使用 \"noeviction\" 策略） # 的时候，这个选项通常事很有用的。 # # 警告：当有多个slave连上达到内存上限的实例时，master为同步slave的输出缓冲区所需 # 内存不计算在使用内存中。这样当驱逐key时，就不会因网络问题 / 重新同步事件触发驱逐key # 的循环，反过来slaves的输出缓冲区充满了key被驱逐的DEL命令，这将触发删除更多的key， # 直到这个数据库完全被清空为止 # # 总之...如果你需要附加多个slave，建议你设置一个稍小maxmemory限制，这样系统就会有空闲 # 的内存作为slave的输出缓存区(但是如果最大内存策略设置为\"noeviction\"的话就没必要了) # # maxmemory # 最大内存策略：如果达到内存限制了，Redis如何选择删除key。你可以在下面五个行为里选： # # volatile-lru -> 根据LRU算法生成的过期时间来删除。 # allkeys-lru -> 根据LRU算法删除任何key。 # volatile-random -> 根据过期设置来随机删除key。 # allkeys->random -> 无差别随机删。 # volatile-ttl -> 根据最近过期时间来删除（辅以TTL） # noeviction -> 谁也不删，直接在写操作时返回错误。 # # 注意：对所有策略来说，如果Redis找不到合适的可以删除的key都会在写操作时返回一个错误。 # # 目前为止涉及的命令：set setnx setex append # incr decr rpush lpush rpushx lpushx linsert lset rpoplpush sadd # sinter sinterstore sunion sunionstore sdiff sdiffstore zadd zincrby # zunionstore zinterstore hset hsetnx hmset hincrby incrby decrby # getset mset msetnx exec sort # # 默认值如下： # # maxmemory-policy volatile-lru # LRU和最小TTL算法的实现都不是很精确，但是很接近（为了省内存），所以你可以用样本量做检测。 # 例如：默认Redis会检查3个key然后取最旧的那个，你可以通过下面的配置指令来设置样本的个数。 # # maxmemory-samples 3 APPEND ONLY MODE ############################## APPEND ONLY MODE ############################### # 默认情况下，Redis是异步的把数据导出到磁盘上。这种模式在很多应用里已经足够好，但Redis进程 # 出问题或断电时可能造成一段时间的写操作丢失(这取决于配置的save指令)。 # # AOF是一种提供了更可靠的替代持久化模式，例如使用默认的数据写入文件策略（参见后面的配置） # 在遇到像服务器断电或单写情况下Redis自身进程出问题但操作系统仍正常运行等突发事件时，Redis # 能只丢失1秒的写操作。 # # AOF和RDB持久化能同时启动并且不会有问题。 # 如果AOF开启，那么在启动时Redis将加载AOF文件，它更能保证数据的可靠性。 # # 请查看 http://redis.io/topics/persistence 来获取更多信息. appendonly no # 纯累加文件名字（默认：\"appendonly.aof\"） appendfilename \"appendonly.aof\" # fsync() 系统调用告诉操作系统把数据写到磁盘上，而不是等更多的数据进入输出缓冲区。 # 有些操作系统会真的把数据马上刷到磁盘上；有些则会尽快去尝试这么做。 # # Redis支持三种不同的模式： # # no：不要立刻刷，只有在操作系统需要刷的时候再刷。比较快。 # always：每次写操作都立刻写入到aof文件。慢，但是最安全。 # everysec：每秒写一次。折中方案。 # # 默认的 \"everysec\" 通常来说能在速度和数据安全性之间取得比较好的平衡。根据你的理解来 # 决定，如果你能放宽该配置为\"no\" 来获取更好的性能(但如果你能忍受一些数据丢失，可以考虑使用 # 默认的快照持久化模式)，或者相反，用“always”会比较慢但比everysec要更安全。 # # 请查看下面的文章来获取更多的细节 # http://antirez.com/post/redis-persistence-demystified.html # # 如果不能确定，就用 \"everysec\" # appendfsync always appendfsync everysec # appendfsync no # 如果AOF的同步策略设置成 \"always\" 或者 \"everysec\"，并且后台的存储进程（后台存储或写入AOF # 日志）会产生很多磁盘I/O开销。某些Linux的配置下会使Redis因为 fsync()系统调用而阻塞很久。 # 注意，目前对这个情况还没有完美修正，甚至不同线程的 fsync() 会阻塞我们同步的write(2)调用。 # # 为了缓解这个问题，可以用下面这个选项。它可以在 BGSAVE 或 BGREWRITEAOF 处理时阻止fsync()。 # # 这就意味着如果有子进程在进行保存操作，那么Redis就处于\"不可同步\"的状态。 # 这实际上是说，在最差的情况下可能会丢掉30秒钟的日志数据。（默认Linux设定） # # 如果把这个设置成\"yes\"带来了延迟问题，就保持\"no\"，这是保存持久数据的最安全的方式。 no-appendfsync-on-rewrite no # 自动重写AOF文件 # 如果AOF日志文件增大到指定百分比，Redis能够通过 BGREWRITEAOF 自动重写AOF日志文件。 # # 工作原理：Redis记住上次重写时AOF文件的大小（如果重启后还没有写操作，就直接用启动时的AOF大小） # # 这个基准大小和当前大小做比较。如果当前大小超过指定比例，就会触发重写操作。你还需要指定被重写 # 日志的最小尺寸，这样避免了达到指定百分比但尺寸仍然很小的情况还要重写。 # # 指定百分比为0会禁用AOF自动重写特性。 auto-aof-rewrite-percentage 100 auto-aof-rewrite-min-size 64mb ################################ LUA SCRIPTING ############################### # Lua 脚本的最大执行时间，毫秒为单位 # # 如果达到了最大的执行时间，Redis将要记录在达到最大允许时间之后一个脚本仍然在执行，并且将 # 开始对查询进行错误响应。 # # 当一个长时间运行的脚本超过了最大执行时间，只有 SCRIPT KILL 和 SHUTDOWN NOSAVE 两个 # 命令可用。第一个可以用于停止一个还没有调用写命名的脚本。第二个是关闭服务器唯一方式，当 # 写命令已经通过脚本开始执行，并且用户不想等到脚本的自然终止。 # # 设置成0或者负值表示不限制执行时间并且没有任何警告 lua-time-limit 5000 SLOW LOG ################################## SLOW LOG ################################### # Redis慢查询日志可以记录超过指定时间的查询。运行时间不包括各种I/O时间，例如：连接客户端， # 发送响应数据等，而只计算命令执行的实际时间（这只是线程阻塞而无法同时为其他请求服务的命令执 # 行阶段） # # 你可以为慢查询日志配置两个参数:一个指明Redis的超时时间(单位为微秒)来记录超过这个时间的命令 # 另一个是慢查询日志长度。当一个新的命令被写进日志的时候，最老的那个记录从队列中移除。 # # 下面的时间单位是微秒，所以1000000就是1秒。注意，负数时间会禁用慢查询日志，而0则会强制记录 # 所有命令。 slowlog-log-slower-than 10000 # 这个长度没有限制。只是要主要会消耗内存。你可以通过 SLOWLOG RESET 来回收内存。 slowlog-max-len 128 Event notification ############################# Event notification ############################## # Redis 能通知 Pub/Sub 客户端关于键空间发生的事件 # 这个功能文档位于http://redis.io/topics/keyspace-events # # 例如：如果键空间事件通知被开启，并且客户端对 0 号数据库的键 foo 执行 DEL 命令时，将通过 # Pub/Sub发布两条消息： # PUBLISH __keyspace@0__:foo del # PUBLISH __keyevent@0__:del foo # # 可以在下表中选择Redis要通知的事件类型。事件类型由单个字符来标识： # # K 键空间通知，以__keyspace@__为前缀 # E 键事件通知，以__keysevent@__为前缀 # g DEL , EXPIRE , RENAME 等类型无关的通用命令的通知, ... # $ String命令 # l List命令 # s Set命令 # h Hash命令 # z 有序集合命令 # x 过期事件（每次key过期时生成） # e 驱逐事件（当key在内存满了被清除时生成） # A g$lshzxe的别名，因此”AKE”意味着所有的事件 # # notify-keyspace-events 带一个由0到多个字符组成的字符串参数。空字符串意思是通知被禁用。 # # 例子：启用List和通用事件通知： # notify-keyspace-events Elg # # 例子2：为了获取过期key的通知订阅名字为 __keyevent@__:expired 的频道，用以下配置 # notify-keyspace-events Ex # # 默认所用的通知被禁用，因为用户通常不需要该特性，并且该特性会有性能损耗。 # 注意如果你不指定至少K或E之一，不会发送任何事件。 notify-keyspace-events \"\" ADVANCED CONFIG # 当hash只有少量的entry时，并且最大的entry所占空间没有超过指定的限制时，会用一种节省内存的 # 数据结构来编码。可以通过下面的指令来设定限制 hash-max-ziplist-entries 512 hash-max-ziplist-value 64 # 与hash似，数据元素较少的list，可以用另一种方式来编码从而节省大量空间。 # 这种特殊的方式只有在符合下面限制时才可以用： list-max-ziplist-entries 512 list-max-ziplist-value 64 # set有一种特殊编码的情况：当set数据全是十进制64位有符号整型数字构成的字符串时。 # 下面这个配置项就是用来设置set使用这种编码来节省内存的最大长度。 set-max-intset-entries 512 # 与hash和list相似，有序集合也可以用一种特别的编码方式来节省大量空间。 # 这种编码只适合长度和元素都小于下面限制的有序集合： zset-max-ziplist-entries 128 zset-max-ziplist-value 64 # HyperLogLog sparse representation bytes limit. The limit includes the # 16 bytes header. When an HyperLogLog using the sparse representation crosses # this limit, it is converted into the dense representation. # # A value greater than 16000 is totally useless, since at that point the # dense representation is more memory efficient. # # The suggested value is ~ 3000 in order to have the benefits of # the space efficient encoding without slowing down too much PFADD, # which is O(N) with the sparse encoding. The value can be raised to # ~ 10000 when CPU is not a concern, but space is, and the data set is # composed of many HyperLogLogs with cardinality in the 0 - 15000 range. hll-sparse-max-bytes 3000 # 启用哈希刷新，每100个CPU毫秒会拿出1个毫秒来刷新Redis的主哈希表（顶级键值映射表）。 # redis所用的哈希表实现（见dict.c）采用延迟哈希刷新机制：你对一个哈希表操作越多，哈希刷新 # 操作就越频繁；反之，如果服务器是空闲的，那么哈希刷新就不会完成，哈希表就会占用更多的一些 # 内存而已。 # # 默认是每秒钟进行10次哈希表刷新，用来刷新字典，然后尽快释放内存。 # # 建议： # 如果你对延迟比较在意，不能够接受Redis时不时的对请求有2毫秒的延迟的话，就用 # \"activerehashing no\"，如果不太在意延迟而希望尽快释放内存就设置\"activerehashing yes\" activerehashing yes # 客户端的输出缓冲区的限制，可用于强制断开那些因为某种原因从服务器读取数据的速度不够快的客户端， # （一个常见的原因是一个发布/订阅客户端消费消息的速度无法赶上生产它们的速度） # # 可以对三种不同的客户端设置不同的限制： # normal -> 正常客户端 # slave -> slave和 MONITOR 客户端 # pubsub -> 至少订阅了一个pubsub channel或pattern的客户端 # # 下面是每个client-output-buffer-limit语法: # client-output-buffer-limit # 一旦达到硬限制客户端会立即被断开，或者达到软限制并持续达到指定的秒数（连续的）。 # 例如，如果硬限制为32兆字节和软限制为16兆字节/10秒，客户端将会立即断开 # 如果输出缓冲区的大小达到32兆字节，或客户端达到16兆字节并连续超过了限制10秒，就将断开连接。 # # 默认normal客户端不做限制，因为他们在不主动请求时不接收数据（以推的方式），只有异步客户端 # 可能会出现请求数据的速度比它可以读取的速度快的场景。 # # pubsub和slave客户端会有一个默认值，因为订阅者和slaves以推的方式来接收数据 # # 把硬限制和软限制都设置为0来禁用该功能 client-output-buffer-limit normal 0 0 0 client-output-buffer-limit slave 256mb 64mb 60 client-output-buffer-limit pubsub 32mb 8mb 60 # Redis调用内部函数来执行许多后台任务，如关闭客户端超时的连接，清除未被请求过的过期Key等等。 # # 不是所有的任务都以相同的频率执行，但Redis依照指定的“hz”值来执行检查任务。 # # 默认情况下，“hz”的被设定为10。提高该值将在Redis空闲时使用更多的CPU时，但同时当有多个key # 同时到期会使Redis的反应更灵敏，以及超时可以更精确地处理。 # # 范围是1到500之间，但是值超过100通常不是一个好主意。 # 大多数用户应该使用10这个默认值，只有在非常低的延迟要求时有必要提高到100。 hz 10 # 当一个子进程重写AOF文件时，如果启用下面的选项，则文件每生成32M数据会被同步。为了增量式的 # 写入硬盘并且避免大的延迟高峰这个指令是非常有用的 aof-rewrite-incremental-fsync yes Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2023-03-06 10:57:30 "},"redis/redis-conf-en.html":{"url":"redis/redis-conf-en.html","title":"Redis配置详解(英文版)","keywords":"","body":"INCLUDESMODULESNETWORKGENERALSNAPSHOTTINGREPLICATIONSECURITYCLIENTSMEMORY MANAGEMENTLAZY FREEINGAPPEND ONLY MODELUA SCRIPTINGREDIS CLUSTERCLUSTER DOCKER/NAT supportSLOW LOGLATENCY MONITOREVENT NOTIFICATIONADVANCED CONFIGACTIVE DEFRAGMENTATION 本文来自redis 官方配置文件 # Redis configuration file example. # # Note that in order to read the configuration file, Redis must be # started with the file path as first argument: # # ./redis-server /path/to/redis.conf # Note on units: when memory size is needed, it is possible to specify # it in the usual form of 1k 5GB 4M and so forth: # # 1k => 1000 bytes # 1kb => 1024 bytes # 1m => 1000000 bytes # 1mb => 1024*1024 bytes # 1g => 1000000000 bytes # 1gb => 1024*1024*1024 bytes # # units are case insensitive so 1GB 1Gb 1gB are all the same. INCLUDES ################################## INCLUDES ################################### # Include one or more other config files here. This is useful if you # have a standard template that goes to all Redis servers but also need # to customize a few per-server settings. Include files can include # other files, so use this wisely. # # Notice option \"include\" won't be rewritten by command \"CONFIG REWRITE\" # from admin or Redis Sentinel. Since Redis always uses the last processed # line as value of a configuration directive, you'd better put includes # at the beginning of this file to avoid overwriting config change at runtime. # # If instead you are interested in using includes to override configuration # options, it is better to use include as the last line. # # include /path/to/local.conf # include /path/to/other.conf MODULES ################################## MODULES ##################################### # Load modules at startup. If the server is not able to load modules # it will abort. It is possible to use multiple loadmodule directives. # # loadmodule /path/to/my_module.so # loadmodule /path/to/other_module.so NETWORK ################################## NETWORK ##################################### # By default, if no \"bind\" configuration directive is specified, Redis listens # for connections from all the network interfaces available on the server. # It is possible to listen to just one or multiple selected interfaces using # the \"bind\" configuration directive, followed by one or more IP addresses. # # Examples: # # bind 192.168.1.100 10.0.0.1 # bind 127.0.0.1 ::1 # # ~~~ WARNING ~~~ If the computer running Redis is directly exposed to the # internet, binding to all the interfaces is dangerous and will expose the # instance to everybody on the internet. So by default we uncomment the # following bind directive, that will force Redis to listen only into # the IPv4 lookback interface address (this means Redis will be able to # accept connections only from clients running into the same computer it # is running). # # IF YOU ARE SURE YOU WANT YOUR INSTANCE TO LISTEN TO ALL THE INTERFACES # JUST COMMENT THE FOLLOWING LINE. # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ bind 127.0.0.1 # Protected mode is a layer of security protection, in order to avoid that # Redis instances left open on the internet are accessed and exploited. # # When protected mode is on and if: # # 1) The server is not binding explicitly to a set of addresses using the # \"bind\" directive. # 2) No password is configured. # # The server only accepts connections from clients connecting from the # IPv4 and IPv6 loopback addresses 127.0.0.1 and ::1, and from Unix domain # sockets. # # By default protected mode is enabled. You should disable it only if # you are sure you want clients from other hosts to connect to Redis # even if no authentication is configured, nor a specific set of interfaces # are explicitly listed using the \"bind\" directive. protected-mode yes # Accept connections on the specified port, default is 6379 (IANA #815344). # If port 0 is specified Redis will not listen on a TCP socket. port 6379 # TCP listen() backlog. # # In high requests-per-second environments you need an high backlog in order # to avoid slow clients connections issues. Note that the Linux kernel # will silently truncate it to the value of /proc/sys/net/core/somaxconn so # make sure to raise both the value of somaxconn and tcp_max_syn_backlog # in order to get the desired effect. tcp-backlog 511 # Unix socket. # # Specify the path for the Unix socket that will be used to listen for # incoming connections. There is no default, so Redis will not listen # on a unix socket when not specified. # # unixsocket /tmp/redis.sock # unixsocketperm 700 # Close the connection after a client is idle for N seconds (0 to disable) timeout 0 # TCP keepalive. # # If non-zero, use SO_KEEPALIVE to send TCP ACKs to clients in absence # of communication. This is useful for two reasons: # # 1) Detect dead peers. # 2) Take the connection alive from the point of view of network # equipment in the middle. # # On Linux, the specified value (in seconds) is the period used to send ACKs. # Note that to close the connection the double of the time is needed. # On other kernels the period depends on the kernel configuration. # # A reasonable value for this option is 300 seconds, which is the new # Redis default starting with Redis 3.2.1. tcp-keepalive 300 GENERAL ################################# GENERAL ##################################### # By default Redis does not run as a daemon. Use 'yes' if you need it. # Note that Redis will write a pid file in /var/run/redis.pid when daemonized. daemonize yes # If you run Redis from upstart or systemd, Redis can interact with your # supervision tree. Options: # supervised no - no supervision interaction # supervised upstart - signal upstart by putting Redis into SIGSTOP mode # supervised systemd - signal systemd by writing READY=1 to $NOTIFY_SOCKET # supervised auto - detect upstart or systemd method based on # UPSTART_JOB or NOTIFY_SOCKET environment variables # Note: these supervision methods only signal \"process is ready.\" # They do not enable continuous liveness pings back to your supervisor. supervised no # If a pid file is specified, Redis writes it where specified at startup # and removes it at exit. # # When the server runs non daemonized, no pid file is created if none is # specified in the configuration. When the server is daemonized, the pid file # is used even if not specified, defaulting to \"/var/run/redis.pid\". # # Creating a pid file is best effort: if Redis is not able to create it # nothing bad happens, the server will start and run normally. pidfile /var/run/redis_6379.pid # Specify the server verbosity level. # This can be one of: # debug (a lot of information, useful for development/testing) # verbose (many rarely useful info, but not a mess like the debug level) # notice (moderately verbose, what you want in production probably) # warning (only very important / critical messages are logged) loglevel notice # Specify the log file name. Also the empty string can be used to force # Redis to log on the standard output. Note that if you use standard # output for logging but daemonize, logs will be sent to /dev/null logfile \"\" # To enable logging to the system logger, just set 'syslog-enabled' to yes, # and optionally update the other syslog parameters to suit your needs. # syslog-enabled no # Specify the syslog identity. # syslog-ident redis # Specify the syslog facility. Must be USER or between LOCAL0-LOCAL7. # syslog-facility local0 # Set the number of databases. The default database is DB 0, you can select # a different one on a per-connection basis using SELECT where # dbid is a number between 0 and 'databases'-1 databases 16 # By default Redis shows an ASCII art logo only when started to log to the # standard output and if the standard output is a TTY. Basically this means # that normally a logo is displayed only in interactive sessions. # # However it is possible to force the pre-4.0 behavior and always show a # ASCII art logo in startup logs by setting the following option to yes. always-show-logo yes SNAPSHOTTING ################################ SNAPSHOTTING ################################ # # Save the DB on disk: # # save # # Will save the DB if both the given number of seconds and the given # number of write operations against the DB occurred. # # In the example below the behaviour will be to save: # after 900 sec (15 min) if at least 1 key changed # after 300 sec (5 min) if at least 10 keys changed # after 60 sec if at least 10000 keys changed # # Note: you can disable saving completely by commenting out all \"save\" lines. # # It is also possible to remove all the previously configured save # points by adding a save directive with a single empty string argument # like in the following example: # # save \"\" save 900 1 save 300 10 save 60 10000 # By default Redis will stop accepting writes if RDB snapshots are enabled # (at least one save point) and the latest background save failed. # This will make the user aware (in a hard way) that data is not persisting # on disk properly, otherwise chances are that no one will notice and some # disaster will happen. # # If the background saving process will start working again Redis will # automatically allow writes again. # # However if you have setup your proper monitoring of the Redis server # and persistence, you may want to disable this feature so that Redis will # continue to work as usual even if there are problems with disk, # permissions, and so forth. stop-writes-on-bgsave-error yes # Compress string objects using LZF when dump .rdb databases? # For default that's set to 'yes' as it's almost always a win. # If you want to save some CPU in the saving child set it to 'no' but # the dataset will likely be bigger if you have compressible values or keys. rdbcompression yes # Since version 5 of RDB a CRC64 checksum is placed at the end of the file. # This makes the format more resistant to corruption but there is a performance # hit to pay (around 10%) when saving and loading RDB files, so you can disable it # for maximum performances. # # RDB files created with checksum disabled have a checksum of zero that will # tell the loading code to skip the check. rdbchecksum yes # The filename where to dump the DB dbfilename dump.rdb # The working directory. # # The DB will be written inside this directory, with the filename specified # above using the 'dbfilename' configuration directive. # # The Append Only File will also be created inside this directory. # # Note that you must specify a directory here, not a file name. dir ./ REPLICATION ################################# REPLICATION ################################# # Master-Slave replication. Use slaveof to make a Redis instance a copy of # another Redis server. A few things to understand ASAP about Redis replication. # # 1) Redis replication is asynchronous, but you can configure a master to # stop accepting writes if it appears to be not connected with at least # a given number of slaves. # 2) Redis slaves are able to perform a partial resynchronization with the # master if the replication link is lost for a relatively small amount of # time. You may want to configure the replication backlog size (see the next # sections of this file) with a sensible value depending on your needs. # 3) Replication is automatic and does not need user intervention. After a # network partition slaves automatically try to reconnect to masters # and resynchronize with them. # # slaveof # If the master is password protected (using the \"requirepass\" configuration # directive below) it is possible to tell the slave to authenticate before # starting the replication synchronization process, otherwise the master will # refuse the slave request. # # masterauth # When a slave loses its connection with the master, or when the replication # is still in progress, the slave can act in two different ways: # # 1) if slave-serve-stale-data is set to 'yes' (the default) the slave will # still reply to client requests, possibly with out of date data, or the # data set may just be empty if this is the first synchronization. # # 2) if slave-serve-stale-data is set to 'no' the slave will reply with # an error \"SYNC with master in progress\" to all the kind of commands # but to INFO and SLAVEOF. # slave-serve-stale-data yes # You can configure a slave instance to accept writes or not. Writing against # a slave instance may be useful to store some ephemeral data (because data # written on a slave will be easily deleted after resync with the master) but # may also cause problems if clients are writing to it because of a # misconfiguration. # # Since Redis 2.6 by default slaves are read-only. # # Note: read only slaves are not designed to be exposed to untrusted clients # on the internet. It's just a protection layer against misuse of the instance. # Still a read only slave exports by default all the administrative commands # such as CONFIG, DEBUG, and so forth. To a limited extent you can improve # security of read only slaves using 'rename-command' to shadow all the # administrative / dangerous commands. slave-read-only yes # Replication SYNC strategy: disk or socket. # # ------------------------------------------------------- # WARNING: DISKLESS REPLICATION IS EXPERIMENTAL CURRENTLY # ------------------------------------------------------- # # New slaves and reconnecting slaves that are not able to continue the replication # process just receiving differences, need to do what is called a \"full # synchronization\". An RDB file is transmitted from the master to the slaves. # The transmission can happen in two different ways: # # 1) Disk-backed: The Redis master creates a new process that writes the RDB # file on disk. Later the file is transferred by the parent # process to the slaves incrementally. # 2) Diskless: The Redis master creates a new process that directly writes the # RDB file to slave sockets, without touching the disk at all. # # With disk-backed replication, while the RDB file is generated, more slaves # can be queued and served with the RDB file as soon as the current child producing # the RDB file finishes its work. With diskless replication instead once # the transfer starts, new slaves arriving will be queued and a new transfer # will start when the current one terminates. # # When diskless replication is used, the master waits a configurable amount of # time (in seconds) before starting the transfer in the hope that multiple slaves # will arrive and the transfer can be parallelized. # # With slow disks and fast (large bandwidth) networks, diskless replication # works better. repl-diskless-sync no # When diskless replication is enabled, it is possible to configure the delay # the server waits in order to spawn the child that transfers the RDB via socket # to the slaves. # # This is important since once the transfer starts, it is not possible to serve # new slaves arriving, that will be queued for the next RDB transfer, so the server # waits a delay in order to let more slaves arrive. # # The delay is specified in seconds, and by default is 5 seconds. To disable # it entirely just set it to 0 seconds and the transfer will start ASAP. repl-diskless-sync-delay 5 # Slaves send PINGs to server in a predefined interval. It's possible to change # this interval with the repl_ping_slave_period option. The default value is 10 # seconds. # # repl-ping-slave-period 10 # The following option sets the replication timeout for: # # 1) Bulk transfer I/O during SYNC, from the point of view of slave. # 2) Master timeout from the point of view of slaves (data, pings). # 3) Slave timeout from the point of view of masters (REPLCONF ACK pings). # # It is important to make sure that this value is greater than the value # specified for repl-ping-slave-period otherwise a timeout will be detected # every time there is low traffic between the master and the slave. # # repl-timeout 60 # Disable TCP_NODELAY on the slave socket after SYNC? # # If you select \"yes\" Redis will use a smaller number of TCP packets and # less bandwidth to send data to slaves. But this can add a delay for # the data to appear on the slave side, up to 40 milliseconds with # Linux kernels using a default configuration. # # If you select \"no\" the delay for data to appear on the slave side will # be reduced but more bandwidth will be used for replication. # # By default we optimize for low latency, but in very high traffic conditions # or when the master and slaves are many hops away, turning this to \"yes\" may # be a good idea. repl-disable-tcp-nodelay no # Set the replication backlog size. The backlog is a buffer that accumulates # slave data when slaves are disconnected for some time, so that when a slave # wants to reconnect again, often a full resync is not needed, but a partial # resync is enough, just passing the portion of data the slave missed while # disconnected. # # The bigger the replication backlog, the longer the time the slave can be # disconnected and later be able to perform a partial resynchronization. # # The backlog is only allocated once there is at least a slave connected. # # repl-backlog-size 1mb # After a master has no longer connected slaves for some time, the backlog # will be freed. The following option configures the amount of seconds that # need to elapse, starting from the time the last slave disconnected, for # the backlog buffer to be freed. # # Note that slaves never free the backlog for timeout, since they may be # promoted to masters later, and should be able to correctly \"partially # resynchronize\" with the slaves: hence they should always accumulate backlog. # # A value of 0 means to never release the backlog. # # repl-backlog-ttl 3600 # The slave priority is an integer number published by Redis in the INFO output. # It is used by Redis Sentinel in order to select a slave to promote into a # master if the master is no longer working correctly. # # A slave with a low priority number is considered better for promotion, so # for instance if there are three slaves with priority 10, 100, 25 Sentinel will # pick the one with priority 10, that is the lowest. # # However a special priority of 0 marks the slave as not able to perform the # role of master, so a slave with priority of 0 will never be selected by # Redis Sentinel for promotion. # # By default the priority is 100. slave-priority 100 # It is possible for a master to stop accepting writes if there are less than # N slaves connected, having a lag less or equal than M seconds. # # The N slaves need to be in \"online\" state. # # The lag in seconds, that must be SECURITY ################################## SECURITY ################################### # Require clients to issue AUTH before processing any other # commands. This might be useful in environments in which you do not trust # others with access to the host running redis-server. # # This should stay commented out for backward compatibility and because most # people do not need auth (e.g. they run their own servers). # # Warning: since Redis is pretty fast an outside user can try up to # 150k passwords per second against a good box. This means that you should # use a very strong password otherwise it will be very easy to break. # # requirepass foobared # Command renaming. # # It is possible to change the name of dangerous commands in a shared # environment. For instance the CONFIG command may be renamed into something # hard to guess so that it will still be available for internal-use tools # but not available for general clients. # # Example: # # rename-command CONFIG b840fc02d524045429941cc15f59e41cb7be6c52 # # It is also possible to completely kill a command by renaming it into # an empty string: # # rename-command CONFIG \"\" # # Please note that changing the name of commands that are logged into the # AOF file or transmitted to slaves may cause problems. CLIENTS ################################### CLIENTS #################################### # Set the max number of connected clients at the same time. By default # this limit is set to 10000 clients, however if the Redis server is not # able to configure the process file limit to allow for the specified limit # the max number of allowed clients is set to the current file limit # minus 32 (as Redis reserves a few file descriptors for internal uses). # # Once the limit is reached Redis will close all the new connections sending # an error 'max number of clients reached'. # # maxclients 10000 MEMORY MANAGEMENT ############################## MEMORY MANAGEMENT ################################ # Set a memory usage limit to the specified amount of bytes. # When the memory limit is reached Redis will try to remove keys # according to the eviction policy selected (see maxmemory-policy). # # If Redis can't remove keys according to the policy, or if the policy is # set to 'noeviction', Redis will start to reply with errors to commands # that would use more memory, like SET, LPUSH, and so on, and will continue # to reply to read-only commands like GET. # # This option is usually useful when using Redis as an LRU or LFU cache, or to # set a hard memory limit for an instance (using the 'noeviction' policy). # # WARNING: If you have slaves attached to an instance with maxmemory on, # the size of the output buffers needed to feed the slaves are subtracted # from the used memory count, so that network problems / resyncs will # not trigger a loop where keys are evicted, and in turn the output # buffer of slaves is full with DELs of keys evicted triggering the deletion # of more keys, and so forth until the database is completely emptied. # # In short... if you have slaves attached it is suggested that you set a lower # limit for maxmemory so that there is some free RAM on the system for slave # output buffers (but this is not needed if the policy is 'noeviction'). # # maxmemory # MAXMEMORY POLICY: how Redis will select what to remove when maxmemory # is reached. You can select among five behaviors: # # volatile-lru -> Evict using approximated LRU among the keys with an expire set. # allkeys-lru -> Evict any key using approximated LRU. # volatile-lfu -> Evict using approximated LFU among the keys with an expire set. # allkeys-lfu -> Evict any key using approximated LFU. # volatile-random -> Remove a random key among the ones with an expire set. # allkeys-random -> Remove a random key, any key. # volatile-ttl -> Remove the key with the nearest expire time (minor TTL) # noeviction -> Don't evict anything, just return an error on write operations. # # LRU means Least Recently Used # LFU means Least Frequently Used # # Both LRU, LFU and volatile-ttl are implemented using approximated # randomized algorithms. # # Note: with any of the above policies, Redis will return an error on write # operations, when there are no suitable keys for eviction. # # At the date of writing these commands are: set setnx setex append # incr decr rpush lpush rpushx lpushx linsert lset rpoplpush sadd # sinter sinterstore sunion sunionstore sdiff sdiffstore zadd zincrby # zunionstore zinterstore hset hsetnx hmset hincrby incrby decrby # getset mset msetnx exec sort # # The default is: # # maxmemory-policy noeviction # LRU, LFU and minimal TTL algorithms are not precise algorithms but approximated # algorithms (in order to save memory), so you can tune it for speed or # accuracy. For default Redis will check five keys and pick the one that was # used less recently, you can change the sample size using the following # configuration directive. # # The default of 5 produces good enough results. 10 Approximates very closely # true LRU but costs more CPU. 3 is faster but not very accurate. # # maxmemory-samples 5 LAZY FREEING ############################# LAZY FREEING #################################### # Redis has two primitives to delete keys. One is called DEL and is a blocking # deletion of the object. It means that the server stops processing new commands # in order to reclaim all the memory associated with an object in a synchronous # way. If the key deleted is associated with a small object, the time needed # in order to execute the DEL command is very small and comparable to most other # O(1) or O(log_N) commands in Redis. However if the key is associated with an # aggregated value containing millions of elements, the server can block for # a long time (even seconds) in order to complete the operation. # # For the above reasons Redis also offers non blocking deletion primitives # such as UNLINK (non blocking DEL) and the ASYNC option of FLUSHALL and # FLUSHDB commands, in order to reclaim memory in background. Those commands # are executed in constant time. Another thread will incrementally free the # object in the background as fast as possible. # # DEL, UNLINK and ASYNC option of FLUSHALL and FLUSHDB are user-controlled. # It's up to the design of the application to understand when it is a good # idea to use one or the other. However the Redis server sometimes has to # delete keys or flush the whole database as a side effect of other operations. # Specifically Redis deletes objects independently of a user call in the # following scenarios: # # 1) On eviction, because of the maxmemory and maxmemory policy configurations, # in order to make room for new data, without going over the specified # memory limit. # 2) Because of expire: when a key with an associated time to live (see the # EXPIRE command) must be deleted from memory. # 3) Because of a side effect of a command that stores data on a key that may # already exist. For example the RENAME command may delete the old key # content when it is replaced with another one. Similarly SUNIONSTORE # or SORT with STORE option may delete existing keys. The SET command # itself removes any old content of the specified key in order to replace # it with the specified string. # 4) During replication, when a slave performs a full resynchronization with # its master, the content of the whole database is removed in order to # load the RDB file just transfered. # # In all the above cases the default is to delete objects in a blocking way, # like if DEL was called. However you can configure each case specifically # in order to instead release memory in a non-blocking way like if UNLINK # was called, using the following configuration directives: lazyfree-lazy-eviction no lazyfree-lazy-expire no lazyfree-lazy-server-del no slave-lazy-flush no APPEND ONLY MODE ############################## APPEND ONLY MODE ############################### # By default Redis asynchronously dumps the dataset on disk. This mode is # good enough in many applications, but an issue with the Redis process or # a power outage may result into a few minutes of writes lost (depending on # the configured save points). # # The Append Only File is an alternative persistence mode that provides # much better durability. For instance using the default data fsync policy # (see later in the config file) Redis can lose just one second of writes in a # dramatic event like a server power outage, or a single write if something # wrong with the Redis process itself happens, but the operating system is # still running correctly. # # AOF and RDB persistence can be enabled at the same time without problems. # If the AOF is enabled on startup Redis will load the AOF, that is the file # with the better durability guarantees. # # Please check http://redis.io/topics/persistence for more information. appendonly no # The name of the append only file (default: \"appendonly.aof\") appendfilename \"appendonly.aof\" # The fsync() call tells the Operating System to actually write data on disk # instead of waiting for more data in the output buffer. Some OS will really flush # data on disk, some other OS will just try to do it ASAP. # # Redis supports three different modes: # # no: don't fsync, just let the OS flush the data when it wants. Faster. # always: fsync after every write to the append only log. Slow, Safest. # everysec: fsync only one time every second. Compromise. # # The default is \"everysec\", as that's usually the right compromise between # speed and data safety. It's up to you to understand if you can relax this to # \"no\" that will let the operating system flush the output buffer when # it wants, for better performances (but if you can live with the idea of # some data loss consider the default persistence mode that's snapshotting), # or on the contrary, use \"always\" that's very slow but a bit safer than # everysec. # # More details please check the following article: # http://antirez.com/post/redis-persistence-demystified.html # # If unsure, use \"everysec\". # appendfsync always appendfsync everysec # appendfsync no # When the AOF fsync policy is set to always or everysec, and a background # saving process (a background save or AOF log background rewriting) is # performing a lot of I/O against the disk, in some Linux configurations # Redis may block too long on the fsync() call. Note that there is no fix for # this currently, as even performing fsync in a different thread will block # our synchronous write(2) call. # # In order to mitigate this problem it's possible to use the following option # that will prevent fsync() from being called in the main process while a # BGSAVE or BGREWRITEAOF is in progress. # # This means that while another child is saving, the durability of Redis is # the same as \"appendfsync none\". In practical terms, this means that it is # possible to lose up to 30 seconds of log in the worst scenario (with the # default Linux settings). # # If you have latency problems turn this to \"yes\". Otherwise leave it as # \"no\" that is the safest pick from the point of view of durability. no-appendfsync-on-rewrite no # Automatic rewrite of the append only file. # Redis is able to automatically rewrite the log file implicitly calling # BGREWRITEAOF when the AOF log size grows by the specified percentage. # # This is how it works: Redis remembers the size of the AOF file after the # latest rewrite (if no rewrite has happened since the restart, the size of # the AOF at startup is used). # # This base size is compared to the current size. If the current size is # bigger than the specified percentage, the rewrite is triggered. Also # you need to specify a minimal size for the AOF file to be rewritten, this # is useful to avoid rewriting the AOF file even if the percentage increase # is reached but it is still pretty small. # # Specify a percentage of zero in order to disable the automatic AOF # rewrite feature. auto-aof-rewrite-percentage 100 auto-aof-rewrite-min-size 64mb # An AOF file may be found to be truncated at the end during the Redis # startup process, when the AOF data gets loaded back into memory. # This may happen when the system where Redis is running # crashes, especially when an ext4 filesystem is mounted without the # data=ordered option (however this can't happen when Redis itself # crashes or aborts but the operating system still works correctly). # # Redis can either exit with an error when this happens, or load as much # data as possible (the default now) and start if the AOF file is found # to be truncated at the end. The following option controls this behavior. # # If aof-load-truncated is set to yes, a truncated AOF file is loaded and # the Redis server starts emitting a log to inform the user of the event. # Otherwise if the option is set to no, the server aborts with an error # and refuses to start. When the option is set to no, the user requires # to fix the AOF file using the \"redis-check-aof\" utility before to restart # the server. # # Note that if the AOF file will be found to be corrupted in the middle # the server will still exit with an error. This option only applies when # Redis will try to read more data from the AOF file but not enough bytes # will be found. aof-load-truncated yes # When rewriting the AOF file, Redis is able to use an RDB preamble in the # AOF file for faster rewrites and recoveries. When this option is turned # on the rewritten AOF file is composed of two different stanzas: # # [RDB file][AOF tail] # # When loading Redis recognizes that the AOF file starts with the \"REDIS\" # string and loads the prefixed RDB file, and continues loading the AOF # tail. # # This is currently turned off by default in order to avoid the surprise # of a format change, but will at some point be used as the default. aof-use-rdb-preamble no LUA SCRIPTING ################################ LUA SCRIPTING ############################### # Max execution time of a Lua script in milliseconds. # # If the maximum execution time is reached Redis will log that a script is # still in execution after the maximum allowed time and will start to # reply to queries with an error. # # When a long running script exceeds the maximum execution time only the # SCRIPT KILL and SHUTDOWN NOSAVE commands are available. The first can be # used to stop a script that did not yet called write commands. The second # is the only way to shut down the server in the case a write command was # already issued by the script but the user doesn't want to wait for the natural # termination of the script. # # Set it to 0 or a negative value for unlimited execution without warnings. lua-time-limit 5000 REDIS CLUSTER ################################ REDIS CLUSTER ############################### # # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ # WARNING EXPERIMENTAL: Redis Cluster is considered to be stable code, however # in order to mark it as \"mature\" we need to wait for a non trivial percentage # of users to deploy it in production. # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ # # Normal Redis instances can't be part of a Redis Cluster; only nodes that are # started as cluster nodes can. In order to start a Redis instance as a # cluster node enable the cluster support uncommenting the following: # # cluster-enabled yes # Every cluster node has a cluster configuration file. This file is not # intended to be edited by hand. It is created and updated by Redis nodes. # Every Redis Cluster node requires a different cluster configuration file. # Make sure that instances running in the same system do not have # overlapping cluster configuration file names. # # cluster-config-file nodes-6379.conf # Cluster node timeout is the amount of milliseconds a node must be unreachable # for it to be considered in failure state. # Most other internal time limits are multiple of the node timeout. # # cluster-node-timeout 15000 # A slave of a failing master will avoid to start a failover if its data # looks too old. # # There is no simple way for a slave to actually have an exact measure of # its \"data age\", so the following two checks are performed: # # 1) If there are multiple slaves able to failover, they exchange messages # in order to try to give an advantage to the slave with the best # replication offset (more data from the master processed). # Slaves will try to get their rank by offset, and apply to the start # of the failover a delay proportional to their rank. # # 2) Every single slave computes the time of the last interaction with # its master. This can be the last ping or command received (if the master # is still in the \"connected\" state), or the time that elapsed since the # disconnection with the master (if the replication link is currently down). # If the last interaction is too old, the slave will not try to failover # at all. # # The point \"2\" can be tuned by user. Specifically a slave will not perform # the failover if, since the last interaction with the master, the time # elapsed is greater than: # # (node-timeout * slave-validity-factor) + repl-ping-slave-period # # So for example if node-timeout is 30 seconds, and the slave-validity-factor # is 10, and assuming a default repl-ping-slave-period of 10 seconds, the # slave will not try to failover if it was not able to talk with the master # for longer than 310 seconds. # # A large slave-validity-factor may allow slaves with too old data to failover # a master, while a too small value may prevent the cluster from being able to # elect a slave at all. # # For maximum availability, it is possible to set the slave-validity-factor # to a value of 0, which means, that slaves will always try to failover the # master regardless of the last time they interacted with the master. # (However they'll always try to apply a delay proportional to their # offset rank). # # Zero is the only value able to guarantee that when all the partitions heal # the cluster will always be able to continue. # # cluster-slave-validity-factor 10 # Cluster slaves are able to migrate to orphaned masters, that are masters # that are left without working slaves. This improves the cluster ability # to resist to failures as otherwise an orphaned master can't be failed over # in case of failure if it has no working slaves. # # Slaves migrate to orphaned masters only if there are still at least a # given number of other working slaves for their old master. This number # is the \"migration barrier\". A migration barrier of 1 means that a slave # will migrate only if there is at least 1 other working slave for its master # and so forth. It usually reflects the number of slaves you want for every # master in your cluster. # # Default is 1 (slaves migrate only if their masters remain with at least # one slave). To disable migration just set it to a very large value. # A value of 0 can be set but is useful only for debugging and dangerous # in production. # # cluster-migration-barrier 1 # By default Redis Cluster nodes stop accepting queries if they detect there # is at least an hash slot uncovered (no available node is serving it). # This way if the cluster is partially down (for example a range of hash slots # are no longer covered) all the cluster becomes, eventually, unavailable. # It automatically returns available as soon as all the slots are covered again. # # However sometimes you want the subset of the cluster which is working, # to continue to accept queries for the part of the key space that is still # covered. In order to do so, just set the cluster-require-full-coverage # option to no. # # cluster-require-full-coverage yes # In order to setup your cluster make sure to read the documentation # available at http://redis.io web site. CLUSTER DOCKER/NAT support ########################## CLUSTER DOCKER/NAT support ######################## # In certain deployments, Redis Cluster nodes address discovery fails, because # addresses are NAT-ted or because ports are forwarded (the typical case is # Docker and other containers). # # In order to make Redis Cluster working in such environments, a static # configuration where each node knows its public address is needed. The # following two options are used for this scope, and are: # # * cluster-announce-ip # * cluster-announce-port # * cluster-announce-bus-port # # Each instruct the node about its address, client port, and cluster message # bus port. The information is then published in the header of the bus packets # so that other nodes will be able to correctly map the address of the node # publishing the information. # # If the above options are not used, the normal Redis Cluster auto-detection # will be used instead. # # Note that when remapped, the bus port may not be at the fixed offset of # clients port + 10000, so you can specify any port and bus-port depending # on how they get remapped. If the bus-port is not set, a fixed offset of # 10000 will be used as usually. # # Example: # # cluster-announce-ip 10.1.1.5 # cluster-announce-port 6379 # cluster-announce-bus-port 6380 SLOW LOG ################################## SLOW LOG ################################### # The Redis Slow Log is a system to log queries that exceeded a specified # execution time. The execution time does not include the I/O operations # like talking with the client, sending the reply and so forth, # but just the time needed to actually execute the command (this is the only # stage of command execution where the thread is blocked and can not serve # other requests in the meantime). # # You can configure the slow log with two parameters: one tells Redis # what is the execution time, in microseconds, to exceed in order for the # command to get logged, and the other parameter is the length of the # slow log. When a new command is logged the oldest one is removed from the # queue of logged commands. # The following time is expressed in microseconds, so 1000000 is equivalent # to one second. Note that a negative number disables the slow log, while # a value of zero forces the logging of every command. slowlog-log-slower-than 10000 # There is no limit to this length. Just be aware that it will consume memory. # You can reclaim memory used by the slow log with SLOWLOG RESET. slowlog-max-len 128 LATENCY MONITOR ################################ LATENCY MONITOR ############################## # The Redis latency monitoring subsystem samples different operations # at runtime in order to collect data related to possible sources of # latency of a Redis instance. # # Via the LATENCY command this information is available to the user that can # print graphs and obtain reports. # # The system only logs operations that were performed in a time equal or # greater than the amount of milliseconds specified via the # latency-monitor-threshold configuration directive. When its value is set # to zero, the latency monitor is turned off. # # By default latency monitoring is disabled since it is mostly not needed # if you don't have latency issues, and collecting data has a performance # impact, that while very small, can be measured under big load. Latency # monitoring can easily be enabled at runtime using the command # \"CONFIG SET latency-monitor-threshold \" if needed. latency-monitor-threshold 0 EVENT NOTIFICATION ############################# EVENT NOTIFICATION ############################## # Redis can notify Pub/Sub clients about events happening in the key space. # This feature is documented at http://redis.io/topics/notifications # # For instance if keyspace events notification is enabled, and a client # performs a DEL operation on key \"foo\" stored in the Database 0, two # messages will be published via Pub/Sub: # # PUBLISH __keyspace@0__:foo del # PUBLISH __keyevent@0__:del foo # # It is possible to select the events that Redis will notify among a set # of classes. Every class is identified by a single character: # # K Keyspace events, published with __keyspace@__ prefix. # E Keyevent events, published with __keyevent@__ prefix. # g Generic commands (non-type specific) like DEL, EXPIRE, RENAME, ... # $ String commands # l List commands # s Set commands # h Hash commands # z Sorted set commands # x Expired events (events generated every time a key expires) # e Evicted events (events generated when a key is evicted for maxmemory) # A Alias for g$lshzxe, so that the \"AKE\" string means all the events. # # The \"notify-keyspace-events\" takes as argument a string that is composed # of zero or multiple characters. The empty string means that notifications # are disabled. # # Example: to enable list and generic events, from the point of view of the # event name, use: # # notify-keyspace-events Elg # # Example 2: to get the stream of the expired keys subscribing to channel # name __keyevent@0__:expired use: # # notify-keyspace-events Ex # # By default all notifications are disabled because most users don't need # this feature and the feature has some overhead. Note that if you don't # specify at least one of K or E, no events will be delivered. notify-keyspace-events \"\" ADVANCED CONFIG ############################### ADVANCED CONFIG ############################### # Hashes are encoded using a memory efficient data structure when they have a # small number of entries, and the biggest entry does not exceed a given # threshold. These thresholds can be configured using the following directives. hash-max-ziplist-entries 512 hash-max-ziplist-value 64 # Lists are also encoded in a special way to save a lot of space. # The number of entries allowed per internal list node can be specified # as a fixed maximum size or a maximum number of elements. # For a fixed maximum size, use -5 through -1, meaning: # -5: max size: 64 Kb node->node->...->node->[tail] # [head], [tail] will always be uncompressed; inner nodes will compress. # 2: [head]->[next]->node->node->...->node->[prev]->[tail] # 2 here means: don't compress head or head->next or tail->prev or tail, # but compress all nodes between them. # 3: [head]->[next]->[next]->node->node->...->node->[prev]->[prev]->[tail] # etc. list-compress-depth 0 # Sets have a special encoding in just one case: when a set is composed # of just strings that happen to be integers in radix 10 in the range # of 64 bit signed integers. # The following configuration setting sets the limit in the size of the # set in order to use this special memory saving encoding. set-max-intset-entries 512 # Similarly to hashes and lists, sorted sets are also specially encoded in # order to save a lot of space. This encoding is only used when the length and # elements of a sorted set are below the following limits: zset-max-ziplist-entries 128 zset-max-ziplist-value 64 # HyperLogLog sparse representation bytes limit. The limit includes the # 16 bytes header. When an HyperLogLog using the sparse representation crosses # this limit, it is converted into the dense representation. # # A value greater than 16000 is totally useless, since at that point the # dense representation is more memory efficient. # # The suggested value is ~ 3000 in order to have the benefits of # the space efficient encoding without slowing down too much PFADD, # which is O(N) with the sparse encoding. The value can be raised to # ~ 10000 when CPU is not a concern, but space is, and the data set is # composed of many HyperLogLogs with cardinality in the 0 - 15000 range. hll-sparse-max-bytes 3000 # Active rehashing uses 1 millisecond every 100 milliseconds of CPU time in # order to help rehashing the main Redis hash table (the one mapping top-level # keys to values). The hash table implementation Redis uses (see dict.c) # performs a lazy rehashing: the more operation you run into a hash table # that is rehashing, the more rehashing \"steps\" are performed, so if the # server is idle the rehashing is never complete and some more memory is used # by the hash table. # # The default is to use this millisecond 10 times every second in order to # actively rehash the main dictionaries, freeing memory when possible. # # If unsure: # use \"activerehashing no\" if you have hard latency requirements and it is # not a good thing in your environment that Redis can reply from time to time # to queries with 2 milliseconds delay. # # use \"activerehashing yes\" if you don't have such hard requirements but # want to free memory asap when possible. activerehashing yes # The client output buffer limits can be used to force disconnection of clients # that are not reading data from the server fast enough for some reason (a # common reason is that a Pub/Sub client can't consume messages as fast as the # publisher can produce them). # # The limit can be set differently for the three different classes of clients: # # normal -> normal clients including MONITOR clients # slave -> slave clients # pubsub -> clients subscribed to at least one pubsub channel or pattern # # The syntax of every client-output-buffer-limit directive is the following: # # client-output-buffer-limit # # A client is immediately disconnected once the hard limit is reached, or if # the soft limit is reached and remains reached for the specified number of # seconds (continuously). # So for instance if the hard limit is 32 megabytes and the soft limit is # 16 megabytes / 10 seconds, the client will get disconnected immediately # if the size of the output buffers reach 32 megabytes, but will also get # disconnected if the client reaches 16 megabytes and continuously overcomes # the limit for 10 seconds. # # By default normal clients are not limited because they don't receive data # without asking (in a push way), but just after a request, so only # asynchronous clients may create a scenario where data is requested faster # than it can read. # # Instead there is a default limit for pubsub and slave clients, since # subscribers and slaves receive data in a push fashion. # # Both the hard or the soft limit can be disabled by setting them to zero. client-output-buffer-limit normal 0 0 0 client-output-buffer-limit slave 256mb 64mb 60 client-output-buffer-limit pubsub 32mb 8mb 60 # Client query buffers accumulate new commands. They are limited to a fixed # amount by default in order to avoid that a protocol desynchronization (for # instance due to a bug in the client) will lead to unbound memory usage in # the query buffer. However you can configure it here if you have very special # needs, such us huge multi/exec requests or alike. # # client-query-buffer-limit 1gb # In the Redis protocol, bulk requests, that are, elements representing single # strings, are normally limited ot 512 mb. However you can change this limit # here. # # proto-max-bulk-len 512mb # Redis calls an internal function to perform many background tasks, like # closing connections of clients in timeout, purging expired keys that are # never requested, and so forth. # # Not all tasks are performed with the same frequency, but Redis checks for # tasks to perform according to the specified \"hz\" value. # # By default \"hz\" is set to 10. Raising the value will use more CPU when # Redis is idle, but at the same time will make Redis more responsive when # there are many keys expiring at the same time, and timeouts may be # handled with more precision. # # The range is between 1 and 500, however a value over 100 is usually not # a good idea. Most users should use the default of 10 and raise this up to # 100 only in environments where very low latency is required. hz 10 # When a child rewrites the AOF file, if the following option is enabled # the file will be fsync-ed every 32 MB of data generated. This is useful # in order to commit the file to the disk more incrementally and avoid # big latency spikes. aof-rewrite-incremental-fsync yes # Redis LFU eviction (see maxmemory setting) can be tuned. However it is a good # idea to start with the default settings and only change them after investigating # how to improve the performances and how the keys LFU change over time, which # is possible to inspect via the OBJECT FREQ command. # # There are two tunable parameters in the Redis LFU implementation: the # counter logarithm factor and the counter decay time. It is important to # understand what the two parameters mean before changing them. # # The LFU counter is just 8 bits per key, it's maximum value is 255, so Redis # uses a probabilistic increment with logarithmic behavior. Given the value # of the old counter, when a key is accessed, the counter is incremented in # this way: # # 1. A random number R between 0 and 1 is extracted. # 2. A probability P is calculated as 1/(old_value*lfu_log_factor+1). # 3. The counter is incremented only if R ACTIVE DEFRAGMENTATION ########################### ACTIVE DEFRAGMENTATION ####################### # # WARNING THIS FEATURE IS EXPERIMENTAL. However it was stress tested # even in production and manually tested by multiple engineers for some # time. # # What is active defragmentation? # ------------------------------- # # Active (online) defragmentation allows a Redis server to compact the # spaces left between small allocations and deallocations of data in memory, # thus allowing to reclaim back memory. # # Fragmentation is a natural process that happens with every allocator (but # less so with Jemalloc, fortunately) and certain workloads. Normally a server # restart is needed in order to lower the fragmentation, or at least to flush # away all the data and create it again. However thanks to this feature # implemented by Oran Agra for Redis 4.0 this process can happen at runtime # in an \"hot\" way, while the server is running. # # Basically when the fragmentation is over a certain level (see the # configuration options below) Redis will start to create new copies of the # values in contiguous memory regions by exploiting certain specific Jemalloc # features (in order to understand if an allocation is causing fragmentation # and to allocate it in a better place), and at the same time, will release the # old copies of the data. This process, repeated incrementally for all the keys # will cause the fragmentation to drop back to normal values. # # Important things to understand: # # 1. This feature is disabled by default, and only works if you compiled Redis # to use the copy of Jemalloc we ship with the source code of Redis. # This is the default with Linux builds. # # 2. You never need to enable this feature if you don't have fragmentation # issues. # # 3. Once you experience fragmentation, you can enable this feature when # needed with the command \"CONFIG SET activedefrag yes\". # # The configuration parameters are able to fine tune the behavior of the # defragmentation process. If you are not sure about what they mean it is # a good idea to leave the defaults untouched. # Enabled active defragmentation # activedefrag yes # Minimum amount of fragmentation waste to start active defrag # active-defrag-ignore-bytes 100mb # Minimum percentage of fragmentation to start active defrag # active-defrag-threshold-lower 10 # Maximum percentage of fragmentation at which we use maximum effort # active-defrag-threshold-upper 100 # Minimal effort for defrag in CPU percentage # active-defrag-cycle-min 25 # Maximal effort for defrag in CPU percentage # active-defrag-cycle-max 75 Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2023-03-06 10:57:30 "},"memcached/memcached.html":{"url":"memcached/memcached.html","title":"Memcached的使用","keywords":"","body":"1. Memcached简介1.1. 特征2. 安装与运行2.1. 自动安装2.2. 源码安装2.3. 验证2.4. 运行2.4.1. 前台运行2.4.2. 后台运行2.5. 连接3. Memcached运行参数1. Memcached简介 Memcached是一个开源的，高性能，分布式内存对象缓存系统。 Memcached是一种基于内存的key-value存储，用来存储小块的任意数据（字符串、对象）。这些数据可以是数据库调用、API调用或者是页面渲染的结果。 一般的使用目的是，通过缓存数据库查询结果，减少数据库访问次数，以提高动态Web应用的速度、提高可扩展性。 1.1. 特征 memcached作为高速运行的分布式缓存服务器，具有以下的特点。 协议简单 基于libevent的事件处理 内置内存存储方式 memcached不互相通信的分布式 2. 安装与运行 2.1. 自动安装 # For Redhat/Fedora yum install -y memcached # For Debian or Ubuntu apt-get install memcached 2.2. 源码安装 安装指定版本的Memcached可以从 https://github.com/memcached/memcached/wiki/ReleaseNotes 地址下载。 # Memcached depends on libevent yum install libevent-devel # install wget https://memcached.org/latest [you might need to rename the file] tar -zxf memcached-1.x.x.tar.gz cd memcached-1.x.x ./configure --prefix=/usr/local/memcached make && make test && sudo make install 问题 如遇以下报错，可再执行make install。 Signal handled: Interrupt. ok 51 - shutdown ok 52 - stop_server /bin/sh:行3: prove: 未找到命令 make: *** [test] Error 127 2.3. 验证 确认是否安装成功，可执行以下命令 /usr/local/memcached/bin/memcached -h 2.4. 运行 2.4.1. 前台运行 /usr/local/memcached/bin/memcached -p 11211 -m 64m -vv 2.4.2. 后台运行 /usr/local/memcached/bin/memcached -p 11211 -m 64m -d -c 102400 -t 8 -P /tmp/memcached.pid 2.5. 连接 $ telnet 127.0.0.1 11211 Trying 127.0.0.1... Connected to 127.0.0.1. Escape character is '^]'. set foo 0 0 3 保存命令 bar 数据 STORED 结果 get foo 取得命令 VALUE foo 0 3 数据 bar 数据 END 结束行 quit 退出 3. Memcached运行参数 # /usr/local/memcached/bin/memcached -h memcached 1.5.12 -p, --port= TCP port to listen on (default: 11211) -U, --udp-port= UDP port to listen on (default: 0, off) -s, --unix-socket= UNIX socket to listen on (disables network support) -A, --enable-shutdown enable ascii \"shutdown\" command -a, --unix-mask= access mask for UNIX socket, in octal (default: 0700) -l, --listen= interface to listen on (default: INADDR_ANY) -d, --daemon run as a daemon -r, --enable-coredumps maximize core file limit -u, --user= assume identity of (only when run as root) -m, --memory-limit= item memory in megabytes (default: 64 MB) -M, --disable-evictions return error on memory exhausted instead of evicting -c, --conn-limit= max simultaneous connections (default: 1024) -k, --lock-memory lock down all paged memory -v, --verbose verbose (print errors/warnings while in event loop) -vv very verbose (also print client commands/responses) -vvv extremely verbose (internal state transitions) -h, --help print this help and exit -i, --license print memcached and libevent license -V, --version print version and exit -P, --pidfile= save PID in , only used with -d option -f, --slab-growth-factor= chunk size growth factor (default: 1.25) -n, --slab-min-size= min space used for key+value+flags (default: 48) -L, --enable-largepages try to use large memory pages (if available) -D Use as the delimiter between key prefixes and IDs. This is used for per-prefix stats reporting. The default is \":\" (colon). If this option is specified, stats collection is turned on automatically; if not, then it may be turned on by sending the \"stats detail on\" command to the server. -t, --threads= number of threads to use (default: 4) -R, --max-reqs-per-event maximum number of requests per event, limits the requests processed per connection to prevent starvation (default: 20) -C, --disable-cas disable use of CAS -b, --listen-backlog= set the backlog queue limit (default: 1024) -B, --protocol= protocol - one of ascii, binary, or auto (default) -I, --max-item-size= adjusts max item size (default: 1mb, min: 1k, max: 128m) -F, --disable-flush-all disable flush_all command -X, --disable-dumping disable stats cachedump and lru_crawler metadump -o, --extended comma separated list of extended options most options have a 'no_' prefix to disable - maxconns_fast: immediately close new connections after limit - hashpower: an integer multiplier for how large the hash table should be. normally grows at runtime. set based on \"STAT hash_power_level\" - tail_repair_time: time in seconds for how long to wait before forcefully killing LRU tail item. disabled by default; very dangerous option. - hash_algorithm: the hash table algorithm default is murmur3 hash. options: jenkins, murmur3 - lru_crawler: enable LRU Crawler background thread - lru_crawler_sleep: microseconds to sleep between items default is 100. - lru_crawler_tocrawl: max items to crawl per slab per run default is 0 (unlimited) - lru_maintainer: enable new LRU system + background thread - hot_lru_pct: pct of slab memory to reserve for hot lru. (requires lru_maintainer) - warm_lru_pct: pct of slab memory to reserve for warm lru. (requires lru_maintainer) - hot_max_factor: items idle > cold lru age * drop from hot lru. - warm_max_factor: items idle > cold lru age * this drop from warm. - temporary_ttl: TTL's below get separate LRU, can't be evicted. (requires lru_maintainer) - idle_timeout: timeout for idle connections - slab_chunk_max: (EXPERIMENTAL) maximum slab size. use extreme care. - watcher_logbuf_size: size in kilobytes of per-watcher write buffer. - worker_logbuf_size: size in kilobytes of per-worker-thread buffer read by background thread, then written to watchers. - track_sizes: enable dynamic reports for 'stats sizes' command. - no_inline_ascii_resp: save up to 24 bytes per item. small perf hit in ASCII, no perf difference in binary protocol. speeds up all sets. - no_hashexpand: disables hash table expansion (dangerous) - modern: enables options which will be default in future. currently: nothing - no_modern: uses defaults of previous major version (1.4.x) 常用参数： -d是启动一个守护进程； -m是分配给Memcache使用的内存数量，单位是MB； -u是运行Memcache的用户； -l是监听的服务器IP地址，可以有多个地址； -p是设置Memcache监听的端口，，最好是1024以上的端口； -c是最大运行的并发连接数，默认是1024； -t是线程数，默认为4； -P是设置保存Memcache的pid文件。 参考文章： https://github.com/memcached/memcached/wiki/Overview https://github.com/memcached/memcached/wiki/Install http://www.runoob.com/memcached/memcached-tutorial.html Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2023-03-06 10:57:30 "},"memcached/memcached-cmd.html":{"url":"memcached/memcached-cmd.html","title":"Memcached命令","keywords":"","body":"1. Memcached 命令1.1. 存储命令1.1.1. 常用命令1.1.2. cas命令1.2. 查找命令1.3. 统计命令1. Memcached 命令 1.1. 存储命令 1.1.1. 常用命令 命令 说明 set 新增或更新 add 新增 replace 替换 append 在后面追加 prepend 在前面追加 cas 检查并设置 以上几个命令语法格式相似，以set为例： set key flags exptime bytes [noreply] value 参数说明如下： key：键值 key-value 结构中的 key，用于查找缓存值。 flags：可以包括键值对的整型参数，客户机使用它存储关于键值对的额外信息 。 exptime：在缓存中保存键值对的时间长度（以秒为单位，0 表示永远） bytes：在缓存中存储的字节数 noreply（可选）： 该参数告知服务器不需要返回数据 value：存储的值（始终位于第二行）（可直接理解为key-value结构中的value） 实例： key → runoob flag → 0 exptime → 900 (以秒为单位) bytes → 9 (数据存储的字节数) value → memcached set runoob 0 900 9 memcached STORED get runoob VALUE runoob 0 9 memcached END 输出： 如果数据设置成功，则输出： STORED 输出信息说明： STORED：保存成功后输出。 ERROR：在保存失败后输出。 1.1.2. cas命令 Memcached CAS（Check-And-Set 或 Compare-And-Swap） 命令用于执行一个\"检查并设置\"的操作。 它仅在当前客户端最后一次取值后，该key 对应的值没有被其他客户端修改的情况下， 才能够将值写入。 检查是通过cas_token参数进行的， 这个参数是Memcach指定给已经存在的元素的一个唯一的64位值。 语法： 比以上命令多了一个unique_cas_token cas key flags exptime bytes unique_cas_token [noreply] value 参数说明如下： key：键值 key-value 结构中的 key，用于查找缓存值。 flags：可以包括键值对的整型参数，客户机使用它存储关于键值对的额外信息 。 exptime：在缓存中保存键值对的时间长度（以秒为单位，0 表示永远） bytes：在缓存中存储的字节数 unique_cas_token通过 gets 命令获取的一个唯一的64位值。 noreply（可选）： 该参数告知服务器不需要返回数据 value：存储的值（始终位于第二行）（可直接理解为key-value结构中的value） unique_cas_token通过gets命令获取。 1.2. 查找命令 命令 说明 get 获取一个或多个key gets 获取一个或多个cas token delete 删除已存在的key incr/decr 对已存在的 key(键) 的数字值进行自增或自减操作 1.3. 统计命令 命令 说明 stats 用于返回统计信息例如 PID(进程号)、版本号、连接数等。 stats items 用于显示各个 slab 中 item 的数目和存储时长(最后一次访问距离现在的秒数)。 stats slabs 用于显示各个slab的信息，包括chunk的大小、数目、使用情况等。 stats sizes 用于显示所有item的大小和个数。 flush_all 用于清理缓存中的所有 key=>value(键=>值) 对。 参考文章： http://www.runoob.com/memcached/memcached-tutorial.html Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2023-03-06 10:57:30 "},"nginx/install-nginx.html":{"url":"nginx/install-nginx.html","title":"Nginx安装与配置","keywords":"","body":"1. 部署1.1. 使用安装包的方式1.2. 使用源代码安装1.2.1. 下载源码包1.2.2. 创建临时目录并解压源码包1.2.3. 编译并安装1.2.4. 配置项2. 配置2.1. 基本配置格式2.2. Nginx全局配置参数2.3. 使用include文件2.4. 配置说明2.4.1. main模块2.4.2. events模块2.4.3. http模块2.4.4. mail模块1. 部署 1.1. 使用安装包的方式 rpm -ivh nginx-xxx.rpm 1.2. 使用源代码安装 1.2.1. 下载源码包 wget http://blob.wae.haplat.net/nginx/nginx-1.9.13.tar.gz 1.2.2. 创建临时目录并解压源码包 mkdir $HOME/build cd $HOME/build && tar zxvf nginx-.tar.gz 1.2.3. 编译并安装 cd $HOME/build/nginx- ./configure \\ --prefix=/etc/nginx \\ --sbin-path=/usr/sbin/nginx \\ --conf-path=/etc/nginx/nginx.conf \\ ... # make && make install 1.2.4. 配置项 1.2.4.1. 通用配置项 配置选项 说明 --prefix= nginx安装的根路径，所有其他的路径都要依赖与该选项 --sbin-path= nginx二进制文件的路径，如果没有指定则会依赖于--prefix --conf-path= 如果在命令行中没有指定配置文件，则通过该配置项去查找配置文件 --error-log-path= 指定错误文件的路径 --pid-path= 指定的文件将会写入nginx master进程的pid，通常在/var/run下 --lock-path= 共享存储器互斥锁文件的路径 --user= worker进程运行的用户 --group= worker进程运行的组 --with-file-aio 启动异步I/O --with-debug 启用调试日志，生产环境不推荐配置 1.2.4.2. 优化配置项 配置选项 说明 --with-cc= 如果想设置一个不在默认PATH下的C编译器 --with-cpp= 设置C预处理器的相应路径 --with-cc-opt= 指定必要的include文件路径 --with-ld-opt= 包含连接器库的路径和运行路径 --with-cpu-opt= 通过该选项为特定的CPU构建nginx 1.2.4.3. http模块的配置项 配置选项 说明 --without-http-cache 在使用upstream模块时，nginx能够配置本地缓存内容，该选项可以禁用缓存 --with-http_perl_module nginx配置能够扩展使用perl代码。该项启用这个模块，但会降低性能 --with-perl_modules_path= 对于额外嵌入的perl模块，该选项指定该perl解析器的路径 --with-perl= 如果在默认的路径中找不到perl则指定perl（5.6版本以上）的路径 --http-log-path= http访问日志的默认路径 --http-client-body-temp-path= 从客户端收到请求后，该项用于作为请求体临时存放的目录 --http-proxy-temp-path= 在使用代理后，通过该项设置存放临时文件路径 --http-fastcgi-temp-path= 设置FastCGI临时文件的目录 --http-uwsgi-temp-path= 设置uWSGI临时文件的目录 --http-scgi-temp-path= 设置SCGI临时文件的目录 1.2.4.4. 其他模块额外配置项 默认没有安装这些模块，可以通过--with-_module来启用相应的模块功能。 配置选项 说明 --with-http_ssl_module 如果需要对流量进行加密，可以使用该选项，再URLs中开始部分将会是https(需要OpenSSL库) --with-http_realip_module 如果nginx在七层负载均衡器或者其他设备之后，它们将Http头中的客户端IP地址传递，则需要启用该模块，再多个客户处于一个IP地址的情况下使用 --with-http_addition_module 该模块作为输出过滤器，使能够在请求经过一个location前或后时在该location本身添加内容 --with-http_xslt_module 该模块用于处理XML响应转换，基于一个或多个XSLT格式 --with-http_image_filter_module 该模块被作为图像过滤器使用，在将图像投递到客户之前进行处理（需要libgd库） --with-http_geoip_module 使用该模块，能够设置各种变量以便在配置文件中的区段使用，基于地理位置查找客户端IP地址 --with-http_sub_module 该模块实现替代过滤，在响应中用一个字符串替代另一个字符串 --with-heep_dav_module 启用这个模块将激活使用WebDAV的配置指令。 --with-http_flv_module 如果需要提供Flash流媒体视频文件，那么该模块将会提供伪流媒体 --with-http_mp4_module 这个模块支持H.264/AAC文件伪流媒体 --with-http_gzip_static_module 当被调用的资源没有.gz结尾格式的文件时，如果想支持发送预压缩版本的静态文件，那么使用该模块 --with-http_gunzip_module 对于不支持gzip编码的客户，该模块用于为客户解压缩预压缩内容 --with-http_random_index_module 如果你想提供从一个目录中随机选择文件的索引文件，那么该模块需要激活 --with-http_secure_link_module 该模块提供一种机制，它会将一个哈希值链接到一个URL中，因此只有那些使用正确密码能够计算链接 --with-http_stub_status_module 启用这个模块后会收集Nginx自身的状态信息。输出的状态信息可以使用RRDtool或类似的东西绘制成图 2. 配置 配置文件一般为/etc/nginx/nginx.conf或/usr/local/nginx/conf/nginx.conf。 2.1. 基本配置格式 { ; } 每一个指令行由分号结束，大括号{}表示一个新的上下文。 2.2. Nginx全局配置参数 全局配置指令 模块 配置项 说明 main模块 user 配置worker进程的用户和组，如果忽略group，则group等于指定的用户的所属组 worker_processes 指定worker进程的启动数量，可将其设置为可用的CPU内核数，若为auto为自动检测 error_log 所有错误的写入文件，第二个参数指定错误的级别（debug，info，notice，warn，error，crit，alert，emerg） pid 设置主进程IP的文件 events模块 use 用于设置使用什么样的连接方法 worker_connections 用于配置一个工作进程能够接受的并发连接最大数。包括客户连接和向上游服务器的连接。 2.3. 使用include文件 include文件可以在任何地方以增强配置文件的可读性，使用include文件要确保被包含文件自身正确的nginx语法，即配置指令和块，然后指定这些文件的路径。 include /etc/nginx/mime.types; 若使用通配符则表示通配的多个文件，若没有给定全路径则依据主配置文件路径进行搜索。 include /etc/nginx/conf.d/*.conf 测试配置文件(包括include的配置文件)语法： nginx -t -c {path-to-nginx.conf} 2.4. 配置说明 2.4.1. main模块 #main模块类似main函数包含其他子模块，非模块配置项(包括模块内)分号结尾，子模块配置花括号结尾 user nobady; #一般按默认设置 pid /var/run/nginx.pid; #进程标识符存放路径，一般按默认设置 worker_processes auto; #nginx对外提供web服务时的worder进程数，可将其设置为可用的CPU内核数，auto为自动检测 worker_rlimit_nofile 100000; # 更改worker进程的最大打开文件数限制 error_log logs/error.log info; #错误日志存放路径 keepalive_timeout 60; #keepalive_timeout 60; events{ #见events模块 } http{ #见http模块 server{ ... location /{ } } } mail{ #见mail模块 } 2.4.2. events模块 events { worker_connections 2048; #设置可由一个worker进程同时打开的最大连接数 multi_accept on; #告诉nginx收到一个新连接通知后接受尽可能多的连接 use epoll; #设置用于复用客户端线程的轮询方法。Linux 2.6+：使用epoll；*BSD：使用kqueue。 } 2.4.3. http模块 http { #http模块 server { #server模块，http服务上的虚拟主机， server 当做对应一个域名进行的配置 listen 80; #配置监听端口 server_name www.linuxidc.com; #配置访问域名 access_log logs/linuxidc.access.log main; #指定日志文件的存放路径 index index.html; #默认访问页面 root /var/www/androidj.com/htdocs; # root 是指将本地的一个文件夹作为所有 url 请求的根路径 upstream backend { #反向代理的后端机器，实现负载均衡 ip_hash; #指明了我们均衡的方式是按照用户的 ip 地址进行分配 server backend1.example.com; server backend2.example.com; server backend3.example.com; server backend4.example.com; } location / { #location 是在一个域名下对更精细的路径进行配置 proxy_pass http://backend; #反向代理到后端机器 } } server { listen 80; server_name www.Androidj.com; access_log logs/androidj.access.log main; location / { index index.html; root /var/www/androidj.com/htdocs; } } } 2.4.4. mail模块 mail { auth_http 127.0.0.1:80/auth.php; pop3_capabilities \"TOP\" \"USER\"; imap_capabilities \"IMAP4rev1\" \"UIDPLUS\"; server { listen 110; protocol pop3; proxy on; } server { listen 25; protocol smtp; proxy on; smtp_auth login plain; xclient off; } } Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2023-03-06 10:57:30 "},"nginx/nginx-proxy.html":{"url":"nginx/nginx-proxy.html","title":"Nginx作为反向代理","keywords":"","body":"1. 反向代理简介1.1. 代理模块指令1.2. upstream模块1.2.1. 负载均衡算法2. Upstream服务器类型2.1. 单个upstream服务器2.2. 多个upstream服务器3. 负载均衡特别说明1. 反向代理简介 Nginx可以作为反向代理，接收客户端的请求，并向上游服务器发起新的请求。该请求可以根据客户端请求的URI，客户机参数或其他逻辑进行拆分，原始URL中的任何部分可以以这种方式进行转换。 1.1. 代理模块指令 指令 说明 proxy_connect_timeout Nginx从接受到请求到连接至上游服务器的最长等待时间 proxy_cookie_domain 替代从上游服务器来的Set-Cookie头的域domain proxy_cookie_path 替代从上游服务器来的Set-Cookie头的path属性 proxy_headers_hash_bucket_size 头名字的最大值 proxy_headers_hash_max_size 从上游服务器接收到头的总大小 proxy_hide_header 不应该传递给客户端头的列表 proxy_http_version 用于通上游服务器通信的Http协议版本 proxy_ignore_client_abort 如果设置为ON，那么客户端放弃连接后，nginx将不会放弃同上游服务器的连接 proxy_ignore_headers 当处理来自上游服务器的响应时，设置哪些头可以被忽略 proxy_intercept_errors 如果启用该选项，Nginx将会显示配置的error_page错误，而不是来自于上游服务器的直接响应 proxy_max_temp_file_size 在写入内存缓冲区时响应与内存不匹配时使用时，给出溢出文件的最大值 proxy_pass 指定请求被传递到的上游服务器，格式为URL proxy_pass_header 覆盖掉在proxy_hide_header指令中设置的头，允许这些头传递到客户端 proxy_pass_request_body 如果设置为off，将会阻止请求体传递到客户端 proxy_pass_request_headers 如果设置为on,则阻止请求头发送到上游服务器 proxy_read_timeout 给出连接关闭前从上游服务器两次成功的读操作耗时，如果上游服务器处理请求比较慢，那么该值需设置较高些 proxy_redirect 重写来自于上游服务器的Location和Refresh头 proxy_send_timeout 给出连接关闭前从上游服务器两次成功的写操作耗时，如果上游服务器处理请求比较慢，那么该值需设置较高些 proxy_set_body 发送到上游服务器的请求体可能会被该指令的设置值修改 proxy_set_header 重写发送到上游服务器头的内容，也可以通过将某种头的值设置为空字符，而不是发送某种头的方法实现 proxy_temp_file_write_size 在同一时间内限制缓冲到一个临时文件的数据量，以使得Nginx不会过长地阻止单个请求 proxy_temp_path 设定临时文件的缓冲，用于缓冲从上游服务器来的文件，可以设定目录的层次 1.2. upstream模块 upstream指令将会启用一个新的配置区域，在该区域定义了一组上游服务器，这些服务器可以被设置为不同的权重（权重高的服务器将会被Nginx传递越多的连接）。 指令 说明 ip_hash 通过IP地址的哈希值确保客户端均匀地连接所有的服务器，键值基于C类地址 keepalive 每一个worker进程缓存的到上游服务器的连接数。再使用Http连接时，proxy_http_verison设置1.1，并将proxy_set_header设置为Connection \"\" least_conn 激活负载均衡算法，将请求发送到活跃连接数最少的那台服务器 server 为upstream定义一个服务器地址（带有端口号的域名、IP地址，或者是UNIX套接字）和一个可选的参数，参数如下：weight：设置一个服务器的优先级优于其他服务器。max_fails：设置在fail_timeout时间之内尝试对一个服务器连接的最大次数，如果超过这个次数，那么就会被标记为down。fail_timeout：在这个指定的时间内服务器必须提供响应，如果在这个时间内没有收到响应，那么服务器就会被标记为down状态。backup：一旦其他服务器宕机，那么有该标记的机器就会接收请求。down：标记为一个服务器不再接受任何请求。 1.2.1. 负载均衡算法 upstream模块能够使用轮询、IP hash和最少连接数三种负载均衡算法之一来选择哪个上游服务器将会被在下一步中连接。 1.2.1.1. 轮询 默认情况使用轮询，不需要配置指令来设置，该算法选择下一个服务器，基于先前选择，再配置文件中哪一个是下一个服务器，以及每个服务器的负载。轮询算法是基于在队列中谁是下一个的原理确保将访问量均匀的分配给每一个上游服务器。 1.2.1.2. IP 哈希 通过ip_hash指令激活使用，从而将某些IP地址映射到同一个上游服务器。 1.2.1.3. 最少连接数 通过least_conn指令启用，该算法通过选择一个活跃的最少连接数服务器，然后将负载均匀分配给上游服务器。如果上游服务器的处理器能力不同，那么可以为server指令使用weight来指示说明。该算法将考虑到不同服务器的加权最小连接数。 2. Upstream服务器类型 上游服务器是Ngixn代理连接的一个服务器，可以是物理机或虚拟机。 2.1. 单个upstream服务器 指令try_files(包括http core模块内)意味着按顺序尝试，直到找到一个匹配为止。Nginx将会投递与客户端给定URI匹配的任何文件，如果没有找到任何配置文件，将会把请求代理到Apache作进一步处理。 2.2. 多个upstream服务器 Nginx将会通过轮询的方式将连续请求传递给3个上游服务器。这样应用程序不会过载。 图片 - 这里写图片描述 3. 负载均衡特别说明 如果客户端希望总是访问同一个上游服务器，可以使用ip_hash指令； 如果请求响应时间长短不一，可以使用least_conn指令； 默认为轮询。 Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2023-03-06 10:57:30 "},"nginx/nginx-http.html":{"url":"nginx/nginx-http.html","title":"Nginx http服务器","keywords":"","body":"1. Nginx的系统架构2. Http核心模块2.1.1. server2.1.2. 日志格式1. Nginx的系统架构 Nginx包含一个单一的master进程和多个worker进程，每个进程都是单进程，并且设计为同时处理成千上万个连接。 worker进程是处理连接的地方，Nginx使用了操作系统事件机制来快速响应这些请求。 master进程负责读取配置文件、处理套接字、派生worker进程、打开日志文件和编译嵌入式的perl脚本。master进程是一个可以通过处理信号量来管理请求的进程。 worker进程运行在一个忙碌的事件循环处理中，用于处理进入的连接。每一个nginx模块被构筑在worker中。任何请求处理、过滤、处理代理的连接和更多操作都在worker中完成。 如果没有阻塞worker进程的进程（例如磁盘I/O），那么需要配置的worker进程要多于CPU内核数，以便处理负载。 2. Http核心模块 2.1.1. server 指令server开始一个新的上下文（context）。 http server指令 指令 说明 port_in_redirect 确认nginx是否对端口指定重定向 server 创建一个新的配置区域，定义一个虚拟主机。listen指令指定IP和端口；server_name列举用于匹配的Host头值 server_name 配置用于响应请求的虚拟主机名称 server_name_in_redirect server_tokens 在错误信息中禁止发送nginx的版本号和server响应头 2.1.2. 日志格式 参数 说明 示例 $remote_addr 客户端地址 211.28.65.253 $remote_user 客户端用户名称 -- $time_local 访问时间和时区 18/Jul/2012:17:00:01 +0800 $request 请求的URI和HTTP协议 \"GET /article-10000.html HTTP/1.1\" $http_host 请求地址，即浏览器中你输入的地址（IP或域名） www.it300.com192.168.100.100 $status HTTP请求状态 200 $upstream_status upstream状态 200 $body_bytes_sent 发送给客户端文件内容大小 1547 $http_referer url跳转来源 https://www.baidu.com/ $http_user_agent 用户终端浏览器等信息 \"Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0; SV1; GTB7.0; .NET4.0C; $ssl_protocol SSL协议版本 TLSv1 $ssl_cipher 交换数据中的算法 RC4-SHA $upstream_addr 后台upstream的地址，即真正提供服务的主机地址 10.10.10.100:80 $request_time 整个请求的总时间 0.205 $upstream_response_time 请求过程中，upstream响应时间 0.002 日志切割 # vim /etc/logrotate.d/nginx /usr/local/nginx/logs/*.log{ #指定转储周期为每天 daily #保留30个备份 rotate 30 #需要压缩 delaycompress #YYYYMMDD日期格式 dateext #忽略错误 missingok #如果日志为空则不做轮询 notifempty #只为整个日志组运行一次的脚本 sharedscripts #日志轮询后执行的脚本 postrotate service nginx reload endscript } Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2023-03-06 10:57:30 "},"keepalived/keepalived-introduction.html":{"url":"keepalived/keepalived-introduction.html","title":"Keepalived简介","keywords":"","body":"1. Keepalived简介1.1. 概述1.2. keepalived的作用2. 如何实现Keepalived2.1. 基于VRRP协议的理解2.2. 基于TCP/IP协议的理解3. Keepalived选举策略3.1. 选举策略3.2. priority和weight的设定1. Keepalived简介 1.1. 概述 Keepalived一个基于VRRP协议来实现的LVS服务高可用方案，可以利用其来避免单点故障。一个LVS服务会有2台服务器运行Keepalived，一台为主服务器（MASTER），一台为备份服务器（BACKUP），但是对外表现为一个虚拟IP，主服务器会发送特定的消息给备份服务器，当备份服务器收不到这个消息的时候，即主服务器宕机的时候， 备份服务器就会接管虚拟IP，继续提供服务，从而保证了高可用性。 1.2. keepalived的作用 Keepalived的作用是检测服务器的状态，如果有一台web服务器死机，或工作出现故障，Keepalived将检测到，并将有故障的服务器从系统中剔除，同时使用其他服务器代替该服务器的工作，当服务器工作正常后Keepalived自动将服务器加入到服务器群中。 2. 如何实现Keepalived 2.1. 基于VRRP协议的理解 Keepalived是以VRRP协议为实现基础的，VRRP全称Virtual Router Redundancy Protocol，即虚拟路由冗余协议。 虚拟路由冗余协议，可以认为是实现路由器高可用的协议，即将N台提供相同功能的路由器组成一个路由器组，这个组里面有一个master和多个backup，master上面有一个对外提供服务的vip（该路由器所在局域网内其他机器的默认路由为该vip），master会发组播，当backup收不到vrrp包时就认为master宕掉了，这时就需要根据VRRP的优先级来选举一个backup当master。这样的话就可以保证路由器的高可用了。 keepalived主要有三个模块，分别是core、check和vrrp。core模块为keepalived的核心，负责主进程的启动、维护以及全局配置文件的加载和解析。check负责健康检查，包括常见的各种检查方式。vrrp模块是来实现VRRP协议的。 2.2. 基于TCP/IP协议的理解 Layer3,4&7工作在IP/TCP协议栈的IP层，TCP层，及应用层,原理分别如下： Layer3： Keepalived使用Layer3的方式工作式时，Keepalived会定期向服务器群中的服务器发送一个ICMP的数据包（既我们平时用的Ping程序）,如果发现某台服务的IP地址没有激活，Keepalived便报告这台服务器失效，并将它从服务器群中剔除，这种情况的典型例子是某台服务器被非法关机。Layer3的方式是以服务器的IP地址是否有效作为服务器工作正常与否的标准。 Layer4: 如果您理解了Layer3的方式，Layer4就容易了。Layer4主要以TCP端口的状态来决定服务器工作正常与否。如web server的服务端口一般是80，如果Keepalived检测到80端口没有启动，则Keepalived将把这台服务器从服务器群中剔除。 Layer7： Layer7就是工作在具体的应用层了，比Layer3,Layer4要复杂一点，在网络上占用的带宽也要大一些。Keepalived将根据用户的设定检查服务器程序的运行是否正常，如果与用户的设定不相符，则Keepalived将把服务器从服务器群中剔除。 3. Keepalived选举策略 3.1. 选举策略 首先，每个节点有一个初始优先级，由配置文件中的priority配置项指定，MASTER节点的priority应比BAKCUP高。运行过程中keepalived根据vrrp_script的weight设定，增加或减小节点优先级。规则如下： “weight”值为正时,脚本检测成功时”weight”值会加到”priority”上,检测失败是不加 主失败: 主priority 主成功: 主priority+weight之和>备priority+weight之和时,主依然为主,即不发生切换 “weight”为负数时,脚本检测成功时”weight”不影响”priority”,检测失败时,Master节点的权值将是“priority“值与“weight”值之差 主失败: 主priotity-abs(weight) 主成功: 主priority > 备priority 不切换 当两个节点的优先级相同时，以节点发送VRRP通告的IP作为比较对象，IP较大者为MASTER。 3.2. priority和weight的设定 主从的优先级初始值priority和变化量weight设置非常关键，配错的话会导致无法进行主从切换。比如，当MASTER初始值定得太高，即使script脚本执行失败，也比BACKUP的priority + weight大，就没法进行VIP漂移了。 所以priority和weight值的设定应遵循: abs(MASTER priority - BAKCUP priority) Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2023-03-06 10:57:30 "},"keepalived/install-keepalived.html":{"url":"keepalived/install-keepalived.html","title":"Keepalived的安装与配置","keywords":"","body":"1. Keepalived的安装1.1. yum install方式1.2. 安装包编译方式2. 常用配置2.1. MASTER（主机配置）2.2. BACKUP（备机配置）3. 注意事项4. 常用脚本4.1. Nginx健康检测脚本4.1.1. 检查接口调用是否为2004.1.2. 检查Nginx进程是否运行4.2. Keepalived状态通知脚本1. Keepalived的安装 1.1. yum install方式 yum install -y keepalived 1.2. 安装包编译方式 更多安装包参考：http://www.keepalived.org/download.html wget http://www.keepalived.org/software/keepalived-2.0.7.tar.gz tar zxvf keepalived-2.0.7.tar.gz cd keepalived-2.0.7 ./configure --bindir=/usr/bin --sbindir=/usr/sbin --sysconfdir=/etc --mandir=/usr/share make && make install 2. 常用配置 keepalived配置文件路径：/etc/keepalived/keepalived。 2.1. MASTER（主机配置） global_defs { router_id proxy-keepalived } vrrp_script check_nginx { script \"/etc/keepalived/scripts/check_nginx.sh\" interval 3 weight 2 } vrrp_instance VI_1 { state BACKUP interface eth2 virtual_router_id 15 priority 100 advert_int 1 authentication { auth_type PASS auth_pass xxx } track_script { check_nginx } virtual_ipaddress { 180.101.115.139 218.98.38.29 } nopreempt notify_master \"/etc/keepalived/keepalived_notify.sh master\" notify_backup \"/etc/keepalived/keepalived_notify.sh backup\" notify_fault \"/etc/keepalived/keepalived_notify.sh fault\" notify_stop \"/etc/keepalived/keepalived_notify.sh stop\" } 2.2. BACKUP（备机配置） global_defs { router_id proxy-keepalived } vrrp_script check_nginx { script \"/etc/keepalived/scripts/check_nginx.sh\" interval 3 weight 2 } vrrp_instance VI_1 { state BACKUP interface eth2 virtual_router_id 15 priority 99 advert_int 1 authentication { auth_type PASS auth_pass xxx } track_script { check_nginx } virtual_ipaddress { 180.101.115.139 218.98.38.29 } nopreempt notify_master \"/etc/keepalived/keepalived_notify.sh master\" notify_backup \"/etc/keepalived/keepalived_notify.sh backup\" notify_fault \"/etc/keepalived/keepalived_notify.sh fault\" notify_stop \"/etc/keepalived/keepalived_notify.sh stop\" } 3. 注意事项 1、指定Nginx健康检测脚本：/etc/keepalived/scripts/check_nginx.sh 2、主备配置差别主要为（建议这么配置）： 以下两种方式的配置，当其中一台机器keepalived挂掉后会自动VIP切到另一台机器，当挂掉机器keepalived恢复后不会抢占VIP，该方式可以避免机器恢复再次切VIP所带来的影响。 主机:(state BACKUP;priority 100) 备机：(state BACKUP;priority 99) 非抢占：nopreempt 或者： 主机:(state MASTER;priority 100) 备机：(state BACKUP;priority 100) 默认抢占 3、指定VIP virtual_ipaddress { 180.101.115.139 218.98.38.29 } 4、可以指定为非抢占：nopreempt，即priority高不会抢占已经绑定VIP的机器。 5、制定绑定IP的网卡： interface eth2 6、可以指定keepalived状态变化通知 notify_master \"/etc/keepalived/keepalived_notify.sh master\" notify_backup \"/etc/keepalived/keepalived_notify.sh backup\" notify_fault \"/etc/keepalived/keepalived_notify.sh fault\" notify_stop \"/etc/keepalived/keepalived_notify.sh stop\" 7、virtual_router_id 15值，主备值一致，但建议不应与集群中其他Nginx机器上的相同，如果同一个网段配置的virtual_router_id 重复则会报错，选择一个不重复的0~255之间的值，可以用以下命令查看已存在的vrid。 tcpdump -nn -i any net 224.0.0.0/8 4. 常用脚本 4.1. Nginx健康检测脚本 在Nginx配置目录下（/etc/nginx/conf.d/）增加health.conf的配置文件,该配置文件用于配置Nginx health的接口。 server { listen 80 default_server; server_name localhost; default_type text/html; return 200 'Health'; } Nginx健康检测脚本：/etc/keepalived/scripts/check_nginx.sh 4.1.1. 检查接口调用是否为200 #!/bin/sh set -x timeout=30 #指定默认30秒没返回200则为非健康，该值可根据实际调整 if [ -n ${timeout} ];then httpcode=`curl -sL -w %{http_code} -m ${timeout} http://localhost -o /dev/null` else httpcode=`curl -sL -w %{http_code} http://localhost -o /dev/null` fi if [ ${httpcode} -ne 200 ];then echo `date`': nginx is not healthy, return http_code is '${httpcode} >> /etc/keeperalived/keepalived.log killall keepalived exit 1 else exit 0 fi 4.1.2. 检查Nginx进程是否运行 #!/bin/sh if [ `ps -C nginx --no-header |wc -l` -eq 0 ];then echo \"$(date) nginx pid not found\">>/etc/keepalived/keepalived.log killall keepalived fi 4.2. Keepalived状态通知脚本 #!/bin/bash set -x warn_receiver=$1 ip=$(ifconfig bond0|grep inet |awk '{print $2}') warningInfo=\"${ip}_keepalived_changed_status_to_$1\" warn-report --user admin --key=xxxx --target=${warn_receiver} ${warningInfo} echo $(date) $1 >> /etc/keepalived/status 说明： ip获取本机IP，本例中IP获取是bond0的IP，不同机器网卡名称不同需要修改为对应网卡名称。 告警工具根据自己指定。 Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2023-03-06 10:57:30 "},"keepalived/keepalived-operation.html":{"url":"keepalived/keepalived-operation.html","title":"Keepalived的相关操作","keywords":"","body":"1. 常用命令1.1. 查看当前VIP在哪个节点上1.2. 查看keepalived的日志1.3. 抓包命令1.4. VIP操作1.5. keepalived 切 VIP1.5.1. 停止keepalived服务1.5.2. 查看日志2. 指定keepalived的输出日志文件2.1. 修改 /etc/sysconfig/keepalived2.2. 修改rsyslog的配置 /etc/rsyslog.conf2.3. 重启rsyslog和keepalived3. Troubleshooting3.1. virtual_router_id 同网段重复3.2. Operation not permitted1. 常用命令 1.1. 查看当前VIP在哪个节点上 # 查看VIP是否在筛选结果中 ip addr show|grep \"scope global\" # 或者 ip addr show|grep {vip} 1.2. 查看keepalived的日志 tail /var/log/messages 1.3. 抓包命令 # 抓包 tcpdump -nn vrrp # 可以用这条命令来查看该网络中所存在的vrid tcpdump -nn -i any net 224.0.0.0/8 # tcpdump -nn -i any net 224.0.0.0/8 # tcpdump -nn vrrp tcpdump: verbose output suppressed, use -v or -vv for full protocol decode listening on eth0, link-type EN10MB (Ethernet), capture size 262144 bytes 14:40:00.576387 IP 192.168.98.57 > 224.0.0.18: VRRPv2, Advertisement, vrid 9, prio 99, authtype simple, intvl 1s, length 20 14:40:01.577605 IP 192.168.98.57 > 224.0.0.18: VRRPv2, Advertisement, vrid 9, prio 99, authtype simple, intvl 1s, length 20 14:40:02.578429 IP 192.168.98.57 > 224.0.0.18: VRRPv2, Advertisement, vrid 9, prio 99, authtype simple, intvl 1s, length 20 14:40:03.579605 IP 192.168.98.57 > 224.0.0.18: VRRPv2, Advertisement, vrid 9, prio 99, authtype simple, intvl 1s, length 20 14:40:04.580443 IP 192.168.98.57 > 224.0.0.18: VRRPv2, Advertisement, vrid 9, prio 99, authtype simple, intvl 1s, length 20 1.4. VIP操作 # 解绑VIP ip addr del dev # 绑定VIP ip addr add dev 1.5. keepalived 切 VIP 例如将 A 机器上的 VIP 迁移到B 机器上。 1.5.1. 停止keepalived服务 停止被迁移的机器（A机器）的keepalived服务。 systemctl stop keepalived 1.5.2. 查看日志 解绑 A机器 VIP的日志 Sep 19 14:28:09 localhost systemd: Stopping LVS and VRRP High Availability Monitor... Sep 19 14:28:09 localhost Keepalived[45705]: Stopping Sep 19 14:28:09 localhost Keepalived_vrrp[45707]: VRRP_Instance(twemproxy) sent 0 priority Sep 19 14:28:09 localhost Keepalived_vrrp[45707]: VRRP_Instance(twemproxy) removing protocol VIPs. Sep 19 14:28:09 localhost Keepalived_healthcheckers[45706]: Stopped Sep 19 14:28:10 localhost Keepalived_vrrp[45707]: Stopped Sep 19 14:28:10 localhost Keepalived[45705]: Stopped Keepalived v1.3.5 (03/19,2017), git commit v1.3.5-6-g6fa32f2 Sep 19 14:28:10 localhost systemd: Stopped LVS and VRRP High Availability Monitor. Sep 19 14:28:10 localhost ntpd[1186]: Deleting interface #10 bond0, 192.168.99.9#123, interface stats: received=0, sent=0, dropped=0, active_time=6755768 secs 绑定 B 机器 VIP的日志 Sep 17 17:20:25 localhost systemd: Starting LVS and VRRP High Availability Monitor... Sep 17 17:20:26 localhost Keepalived[34566]: Starting Keepalived v1.3.5 (03/19,2017), git commit v1.3.5-6-g6fa32f2 Sep 17 17:20:26 localhost Keepalived[34566]: Opening file '/etc/keepalived/keepalived.conf'. Sep 17 17:20:26 localhost Keepalived[34568]: Starting Healthcheck child process, pid=34569 Sep 17 17:20:26 localhost Keepalived[34568]: Starting VRRP child process, pid=34570 Sep 17 17:20:26 localhost Keepalived_vrrp[34570]: Registering Kernel netlink reflector Sep 17 17:20:26 localhost Keepalived_vrrp[34570]: Registering Kernel netlink command channel Sep 17 17:20:26 localhost Keepalived_vrrp[34570]: Registering gratuitous ARP shared channel Sep 17 17:20:26 localhost Keepalived_vrrp[34570]: Opening file '/etc/keepalived/keepalived.conf'. Sep 17 17:20:26 localhost Keepalived_vrrp[34570]: Truncating auth_pass to 8 characters Sep 17 17:20:26 localhost Keepalived_vrrp[34570]: VRRP_Instance(twemproxy) removing protocol VIPs. Sep 17 17:20:26 localhost Keepalived_vrrp[34570]: Using LinkWatch kernel netlink reflector... Sep 17 17:20:26 localhost Keepalived_vrrp[34570]: VRRP_Instance(twemproxy) Entering BACKUP STATE Sep 17 17:20:26 localhost Keepalived_vrrp[34570]: VRRP sockpool: [ifindex(4), proto(112), unicast(0), fd(10,11)] Sep 17 17:20:26 localhost systemd: Started LVS and VRRP High Availability Monitor. Sep 17 17:20:26 localhost kernel: IPVS: Registered protocols (TCP, UDP, SCTP, AH, ESP) Sep 17 17:20:26 localhost kernel: IPVS: Connection hash table configured (size=4096, memory=64Kbytes) Sep 17 17:20:26 localhost kernel: IPVS: Creating netns size=2192 id=0 Sep 17 17:20:26 localhost kernel: IPVS: Creating netns size=2192 id=1 Sep 17 17:20:26 localhost kernel: IPVS: ipvs loaded. Sep 17 17:20:26 localhost Keepalived_healthcheckers[34569]: Opening file '/etc/keepalived/keepalived.conf'. 2. 指定keepalived的输出日志文件 2.1. 修改 /etc/sysconfig/keepalived 将KEEPALIVED_OPTIONS=\"-D\"改为KEEPALIVED_OPTIONS=\"-D -d -S 0\"。 # Options for keepalived. See `keepalived --help' output and keepalived(8) and # keepalived.conf(5) man pages for a list of all options. Here are the most # common ones : # # --vrrp -P Only run with VRRP subsystem. # --check -C Only run with Health-checker subsystem. # --dont-release-vrrp -V Dont remove VRRP VIPs & VROUTEs on daemon stop. # --dont-release-ipvs -I Dont remove IPVS topology on daemon stop. # --dump-conf -d Dump the configuration data. # --log-detail -D Detailed log messages. # --log-facility -S 0-7 Set local syslog facility (default=LOG_DAEMON) # KEEPALIVED_OPTIONS=\"-D -d -S 0\" 2.2. 修改rsyslog的配置 /etc/rsyslog.conf 在/etc/rsyslog.conf 添加 keepalived的日志路径 vi /etc/rsyslog.conf ... # keepalived log local0.* /etc/keepalived/keepalived.log 2.3. 重启rsyslog和keepalived # 重启rsyslog systemctl restart rsyslog # 重启keepalived systemctl restart keepalived 3. Troubleshooting 3.1. virtual_router_id 同网段重复 日志报错如下： Mar 09 21:28:28 k8s4 Keepalived_vrrp[8548]: bogus VRRP packet received on eth0 !!! Mar 09 21:28:28 k8s4 Keepalived_vrrp[8548]: VRRP_Instance(VI-kube-master) ignoring received advertisment... Mar 09 21:28:43 k8s4 Keepalived_vrrp[8548]: ip address associated with VRID not present in received packet : 192.168.1.10 Mar 09 21:28:43 k8s4 Keepalived_vrrp[8548]: one or more VIP associated with VRID mismatch actual MASTER advert 解决方法: 同一网段内LB节点配置的 virtual_router_id 值有重复了，选择一个不重复的0~255之间的值，可以用以下命令查看已存在的vrid。 tcpdump -nn -i any net 224.0.0.0/8 3.2. Operation not permitted 问题： 两台主备机器都绑定了VIP，查看日志如下： Sep 28 14:28:37 node Keepalived_vrrp[1686]: (VI_1): send advert error 1 (Operation not permitted) Sep 28 14:28:39 node Keepalived_vrrp[1686]: (VI_1): send advert error 1 (Operation not permitted) 原因： 由于iptables vrrp协议没有放通，导致keepalived直接无法互相探测选主。 解决方法： 添加iptabels vrrp协议规则 iptables -A INPUT -p vrrp -j ACCEPT iptables -A OUTPUT -p vrrp -j ACCEPT 持久化iptables规则，添加规则到文件中/etc/sysconfig/iptables # vi /etc/sysconfig/iptables -A INPUT -p vrrp -j ACCEPT -A OUTPUT -p vrrp -j ACCEPT Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2023-03-06 10:57:30 "},"keepalived/keepalived-conf.html":{"url":"keepalived/keepalived-conf.html","title":"Keepalived的配置详解","keywords":"","body":"详细配置说明1. global_defs区域2. static_ipaddress和static_routes区域[可忽略]3. vrrp_script区域4. vrrp_instance和vrrp_sync_group区域5. virtual_server_group和virtual_server区域详细配置说明 keepalived只有一个配置文件/etc/keepalived/keepalived.conf。 里面主要包括以下几个配置区域，分别是: global_defs static_ipaddress static_routes vrrp_script vrrp_instance virtual_server 1. global_defs区域 主要是配置故障发生时的通知对象以及机器标识。 global_defs { notification_email { # notification_email 故障发生时给谁发邮件通知 a@abc.com b@abc.com ... } notification_email_from alert@abc.com # notification_email_from 通知邮件从哪个地址发出 smtp_server smtp.abc.com # smpt_server 通知邮件的smtp地址 smtp_connect_timeout 30 # smtp_connect_timeout 连接smtp服务器的超时时间 enable_traps # enable_traps 开启SNMP陷阱（Simple Network Management Protocol） router_id host163 # router_id 标识本节点的字条串，通常为hostname，但不一定非得是hostname。故障发生时，邮件通知会用到。 } 2. static_ipaddress和static_routes区域[可忽略] static_ipaddress和static_routes区域配置的是是本节点的IP和路由信息。如果你的机器上已经配置了IP和路由，那么这两个区域可以不用配置。其实，一般情况下你的机器都会有IP地址和路由信息的，因此没必要再在这两个区域配置。 static_ipaddress { 10.210.214.163/24 brd 10.210.214.255 dev eth0 ... } static_routes { 10.0.0.0/8 via 10.210.214.1 dev eth0 ... } 3. vrrp_script区域 用来做健康检查的，当时检查失败时会将vrrp_instance的priority减少相应的值。 vrrp_script chk_http_port { script \" 4. vrrp_instance和vrrp_sync_group区域 vrrp_instance用来定义对外提供服务的VIP区域及其相关属性。 vrrp_rsync_group用来定义vrrp_intance组，使得这个组内成员动作一致。 vrrp_sync_group VG_1 { #监控多个网段的实例 group { inside_network # name of vrrp_instance (below) outside_network # One for each moveable IP. ... } notify_master /path/to_master.sh # notify_master表示切换为主机执行的脚本 notify_backup /path/to_backup.sh # notify_backup表示切换为备机师的脚本 notify_fault \"/path/fault.sh VG_1\" # notify_fault表示出错时执行的脚本 notify /path/notify.sh # notify表示任何一状态切换时都会调用该脚本，且在以上三个脚本执行完成之后进行调用 smtp_alert # smtp_alert 表示是否开启邮件通知（用全局区域的邮件设置来发通知） } vrrp_instance VI_1 { state MASTER # state MASTER或BACKUP，当其他节点keepalived启动时会将priority比较大的节点选举为MASTER，因此该项其实没有实质用途。 interface eth0 # interface 节点固有IP（非VIP）的网卡，用来发VRRP包 use_vmac dont_track_primary # use_vmac 是否使用VRRP的虚拟MAC地址，dont_track_primary 忽略VRRP网卡错误（默认未设置） track_interface {# track_interface 监控以下网卡，如果任何一个不通就会切换到FALT状态。（可选项） eth0 eth1 } #mcast_src_ip 修改vrrp组播包的源地址，默认源地址为master的IP mcast_src_ip lvs_sync_daemon_interface eth1 #lvs_sync_daemon_interface 绑定lvs syncd的网卡 garp_master_delay 10 # garp_master_delay 当切为主状态后多久更新ARP缓存，默认5秒 virtual_router_id 1 # virtual_router_id 取值在0-255之间，用来区分多个instance的VRRP组播， 同一网段中virtual_router_id的值不能重复，否则会出错 priority 100 #priority用来选举master的，根据服务是否可用，以weight的幅度来调整节点的priority，从而选取priority高的为master，该项取值范围是1-255（在此范围之外会被识别成默认值100） advert_int 1 # advert_int 发VRRP包的时间间隔，即多久进行一次master选举（可以认为是健康查检时间间隔） authentication { # authentication 认证区域，认证类型有PASS和HA（IPSEC），推荐使用PASS（密码只识别前8位） auth_type PASS #认证方式 auth_pass 12345678 #认证密码 } virtual_ipaddress { # 设置vip 10.210.214.253/24 brd 10.210.214.255 dev eth0 192.168.1.11/24 brd 192.168.1.255 dev eth1 } virtual_routes { # virtual_routes 虚拟路由，当IP漂过来之后需要添加的路由信息 172.16.0.0/12 via 10.210.214.1 192.168.1.0/24 via 192.168.1.1 dev eth1 default via 202.102.152.1 } track_script { chk_http_port } nopreempt # nopreempt 允许一个priority比较低的节点作为master，即使有priority更高的节点启动 preempt_delay 300 # preempt_delay master启动多久之后进行接管资源（VIP/Route信息等），并提是没有nopreempt选项 debug notify_master| notify_backup| notify_fault| notify| smtp_alert } 5. virtual_server_group和virtual_server区域 virtual_server_group一般在超大型的LVS中用到，一般LVS用不到这东西。 virtual_server IP Port { delay_loop # delay_loop 延迟轮询时间（单位秒） lb_algo rr|wrr|lc|wlc|lblc|sh|dh # lb_algo 后端调试算法（load balancing algorithm） lb_kind NAT|DR|TUN # lb_kind LVS调度类型NAT/DR/TUN persistence_timeout #会话保持时间 persistence_granularity #lvs会话保持粒度 protocol TCP #使用的协议 ha_suspend virtualhost # virtualhost 用来给HTTP_GET和SSL_GET配置请求header的 alpha omega quorum hysteresis quorum_up| quorum_down| sorry_server #备用机，所有realserver失效后启用 real_server{ # real_server 真正提供服务的服务器 weight 1 # 默认为1,0为失效 inhibit_on_failure #在服务器健康检查失效时，将其设为0，而不是直接从ipvs中删除 notify_up| # real server宕掉时执行的脚本 notify_down| # real server启动时执行的脚本 # HTTP_GET|SSL_GET|TCP_CHECK|SMTP_CHECK|MISC_CHECK TCP_CHECK { connect_timeout 3 #连接超时时间 nb_get_retry 3 #重连次数 delay_before_retry 3 #重连间隔时间 connect_port 23 #健康检查的端口的端口 bindto } HTTP_GET|SSL_GET { url {# 检查url，可以指定多个 path # path 请求real serserver上的路径 digest # 用genhash算出的摘要信息 status_code # 检查的http状态码 } connect_port # connect_port 健康检查，如果端口通则认为服务器正常 connect_timeout # 超时时长 nb_get_retry # 重试次数 delay_before_retry # 下次重试的时间延迟 } SMTP_CHECK { host { connect_ip connect_port #默认检查25端口 bindto } connect_timeout 5 retry 3 delay_before_retry 2 helo_name | #smtp helo请求命令参数，可选 } MISC_CHECK { misc_path | #外部脚本路径 misc_timeout #脚本执行超时时间 misc_dynamic #如设置该项，则退出状态码会用来动态调整服务器的权重，返回0 正常，不修改；返回1， #检查失败，权重改为0；返回2-255，正常，权重设置为：返回状态码-2 } } } Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2023-03-06 10:57:30 "},"keymap/vscode-keymap.html":{"url":"keymap/vscode-keymap.html","title":"vscode快捷键","keywords":"","body":"vscode快捷键1. 基本快捷键1.1. VsCode 快捷键有五种组合方式1.2. Ctrl+P 模式2. 快捷键分类2.1. 通用快捷键2.2. 基础编辑2.3. 导航2.4. 查询与替换2.5. 多行光标操作于选择2.6. 丰富的语言操作2.7. 编辑器管理2.8. 文件管理2.9. 显示2.10. 调试2.11. 集成终端vscode快捷键 1. 基本快捷键 1.1. VsCode 快捷键有五种组合方式 Ctrl + Shift + ? : 这种常规组合按钮 Ctrl + V Ctrl +V : 同时依赖一个按键的组合 Shift + V c : 先组合后单键的输入 Ctrl + Click: 键盘 + 鼠标点击 Ctrl + DragMouse : 键盘 + 鼠标拖动 1.2. Ctrl+P 模式 在Ctrl+P下输入>又可以回到主命令框 Ctrl+Shift+P模式。 在Ctrl+P窗口下还可以 直接输入文件名，快速打开文件 ? 列出当前可执行的动作 ! 显示Errors或Warnings，也可以Ctrl+Shift+M : 跳转到行数，也可以Ctrl+G直接进入 @ 跳转到symbol（搜索变量或者函数），也可以Ctrl+Shift+O直接进入 @:根据分类跳转symbol，查找属性或函数，也可以Ctrl+Shift+O后输入:进入 # 根据名字查找symbol，也可以Ctrl+T 2. 快捷键分类 2.1. 通用快捷键 快捷键 作用 Ctrl+Shift+P,F1 展示全局命令面板 Ctrl+P 快速打开最近打开的文件 Ctrl+Shift+N 打开新的编辑器窗口 Ctrl+Shift+W 关闭编辑器 2.2. 基础编辑 快捷键 作用 Ctrl + X 剪切 Ctrl + C 复制 Alt + up/down 移动行上下 Shift + Alt up/down 在当前行上下复制当前行 Ctrl + Shift + K 删除行 Ctrl + Enter 在当前行下插入新的一行 Ctrl + Shift + Enter 在当前行上插入新的一行 Ctrl + Shift + \\ 匹配花括号的闭合处，跳转 Ctrl + ] / [ 行缩进 Home 光标跳转到行头 End 光标跳转到行尾 Ctrl + Home 跳转到页头 Ctrl + End 跳转到页尾 Ctrl + up/down 行视图上下偏移 Alt + PgUp/PgDown 屏视图上下偏移 Ctrl + Shift + [ 折叠区域代码 Ctrl + Shift + ] 展开区域代码 Ctrl + K Ctrl + [ 折叠所有子区域代码 Ctrl + k Ctrl + ] 展开所有折叠的子区域代码 Ctrl + K Ctrl + 0 折叠所有区域代码 Ctrl + K Ctrl + J 展开所有折叠区域代码 Ctrl + K Ctrl + C 添加行注释 Ctrl + K Ctrl + U 删除行注释 Ctrl + / 添加关闭行注释 Shift + Alt +A 块区域注释 Alt + Z 添加关闭词汇包含 2.3. 导航 快捷键 作用 Ctrl + T 列出所有符号 Ctrl + G 跳转行 Ctrl + P 跳转文件 Ctrl + Shift + O 跳转到符号处 Ctrl + Shift + M 打开问题展示面板 F8 跳转到下一个错误或者警告 Shift + F8 跳转到上一个错误或者警告 Ctrl + Shift + Tab 切换到最近打开的文件 Alt + left / right 向后、向前 Ctrl + M 进入用Tab来移动焦点 2.4. 查询与替换 快捷键 作用 Ctrl + F 查询 Ctrl + H 替换 F3 / Shift + F3 查询下一个/上一个 Alt + Enter 选中所有出现在查询中的 Ctrl + D 匹配当前选中的词汇或者行，再次选中-可操作 Ctrl + K Ctrl + D 移动当前选择到下个匹配选择的位置(光标选定) Alt + C / R / W 2.5. 多行光标操作于选择 快捷键 作用 Alt + Click 插入光标-支持多个 Ctrl + Alt + up/down 上下插入光标-支持多个 Ctrl + U 撤销最后一次光标操作 Shift + Alt + I 插入光标到选中范围内所有行结束符 Ctrl + I 选中当前行 Ctrl + Shift + L 选择所有出现在当前选中的行-操作 Ctrl + F2 选择所有出现在当前选中的词汇-操作 Shift + Alt + right 从光标处扩展选中全行 Shift + Alt + left 收缩选择区域 Shift + Alt + (drag mouse) 鼠标拖动区域，同时在多个行结束符插入光标 Ctrl + Shift + Alt + (Arrow Key) 也是插入多行光标的[方向键控制] Ctrl + Shift + Alt + PgUp/PgDown 也是插入多行光标的[整屏生效] Shift + Alt +鼠标选择块 多行 块选择编辑 2.6. 丰富的语言操作 快捷键 作用 Ctrl + Space 输入建议[智能提示] Ctrl + Shift + Space 参数提示 Tab Emmet指令触发/缩进 Shift + Alt + F 格式化代码 Ctrl + K Ctrl + F 格式化选中部分的代码 F12 跳转到定义处 ctrl + - 跳回原处（跳转前位置） Alt + F12 代码片段显示定义 Ctrl + K F12 在其他窗口打开定义处 Ctrl + . 快速修复部分可以修复的语法错误 Shift + F12 显示所有引用 F2 重命名符号 Ctrl + Shift + . / , 替换下个值 Ctrl + K Ctrl + X 移除空白字符 Ctrl + K M 更改页面文档格式 2.7. 编辑器管理 快捷键 作用 Ctrl + F4, Ctrl + W 关闭编辑器 Ctrl + k F 关闭当前打开的文件夹 Ctrl + \\ 切割编辑窗口 Ctrl + 1/2/3 切换焦点在不同的切割窗口 Ctrl + K Ctrl 切换焦点在不同的切割窗口 Ctrl + Shift + PgUp/PgDown 切换标签页的位置 Ctrl + K 切割窗口位置调换 2.8. 文件管理 快捷键 作用 Ctrl + N 新建文件 Ctrl + O 打开文件 Ctrl + S 保存文件 Ctrl + Shift + S 另存为 Ctrl + K S 保存所有当前已经打开的文件 Ctrl + F4 关闭当前编辑窗口 Ctrl + K Ctrl + W 关闭所有编辑窗口 Ctrl + Shift + T 撤销最近关闭的一个文件编辑窗口 Ctrl + K Enter 保持开启 Ctrl + Shift + Tab 调出最近打开的文件列表，重复按会切换 Ctrl + Tab 与上面一致，顺序不一致 Ctrl + K P 复制当前打开文件的存放路径 Ctrl + K R 打开当前编辑文件存放位置【文件管理器】 Ctrl + K O 在新的编辑器中打开当前编辑的文件 2.9. 显示 快捷键 作用 F11 切换全屏模式 Shift + Alt + 1 切换编辑布局【目前无效】 Ctrl + =/- 放大 / 缩小 Ctrl + B 侧边栏显示隐藏 Ctrl + Shift + E 资源视图和编辑视图的焦点切换 Ctrl + Shift + F 打开全局搜索 Ctrl + Shift + G 打开Git可视管理 Ctrl + Shift + D 打开DeBug面板 Ctrl + Shift + X 打开插件市场面板 Ctrl + Shift + H 在当前文件替换查询替换 Ctrl + Shift + J 开启详细查询 Ctrl + Shift + V 预览Markdown文件【编译后】 Ctrl + K v 在边栏打开渲染后的视图【新建】 2.10. 调试 快捷键 作用 F9 添加解除断点 F5 启动调试、继续 F11 / Shift + F11 单步进入 / 单步跳出 F10 单步跳过 Ctrl + K Ctrl + I 显示悬浮 2.11. 集成终端 快捷键 作用 Ctrl + ` 打开集成终端 Ctrl + Shift + ` 创建一个新的终端 Ctrl + Shift + C 复制所选 Ctrl + Shift + V 复制到当前激活的终端 Shift + PgUp / PgDown 页面上下翻屏 Ctrl + Home / End 滚动到页面头部或尾部 Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2023-03-06 10:57:30 "},"keymap/eclipse-keymap.html":{"url":"keymap/eclipse-keymap.html","title":"eclipse快捷键","keywords":"","body":"eclipse快捷键1. 快捷键1.1. 编辑1.2. 查看1.3. 窗口1.4. 导航2. 搜索3. 文本编辑4. 文件5. 项目5.1. 源代码5.2. 运行5.3. 重构eclipse快捷键 1. 快捷键 1.1. 编辑 作用域 功能 快捷键 全局 查找并替换 Ctrl+F 文本编辑器 查找上一个 Ctrl+Shift+K 文本编辑器 查找下一个 Ctrl+K 文本编辑器 删除当前行 Ctrl+D 文本编辑器 当前行的下一行插入空行 Shift+Enter 文本编辑器 当前行插入空行 Ctrl+Shift+Enter 文本编辑器 定位到最后编辑的位置 Ctrl+Q 全局 恢复上一个选择 Alt+Shift+↓ 全局 快速修正 Ctrl+1 全局 内容辅助（代码提示） Alt+/ 全局 全部选中 Ctrl+A 全局 删除 Delete 全局 上下文信息 Alt+/Alt+Shift+?Ctrl+Shift+Space Java编辑器 显示工具提示描述 F2 Java编辑器 选择封装元素 Alt+Shift+↑ Java编辑器 增量选择上一个同级元素 Alt+Shift+← Java编辑器 增量选择下一个同级元素 Alt+Shift+→ 文本编辑器 增量查找 Ctrl+J 文本编辑器 增量逆向查找 Ctrl+Shift+J java编辑器 自动生成get set方法 Alt+Shift+s 再按 r java编辑器 列出所有实现此接口的类 ctrl+T 1.2. 查看 作用域 功能 快捷键 全局 放大 Ctrl+= 全局 缩小 Ctrl+- 1.3. 窗口 作用域 功能 快捷键 全局 激活编辑器 F12 全局 关闭所有编辑器 Ctrl+Shift+W 全局 上一个编辑器 Ctrl+Shift+F6 全局 上一个视图 Ctrl+Shift+F7 全局 上一个透视图 Ctrl+Shift+F8 全局 下一个编辑器 Ctrl+F6 全局 下一个视图 Ctrl+F7 全局 下一个透视图 Ctrl+F8 文本编辑器 关闭当前窗口 Ctrl+W 全局 显示视图菜单 Ctrl+F10 全局 显示系统菜单 Alt+- 1.4. 导航 作用域 功能 快捷键 Java编辑器 打开结构 Ctrl+F3 全局 打开类型 Ctrl+Shift+T 全局 打开类型层次结构 F4 全局 打开声明 F3 全局 打开外部javadoc Shift+F2 全局 打开资源 Ctrl+Shift+R 全局 后退历史记录 Alt+← 全局 前进历史记录 Alt+→ 全局 上一个 Ctrl+, 全局 下一个 Ctrl+. Java编辑器 显示大纲 Ctrl+O 全局 在层次结构中打开类型 Ctrl+Shift+H 全局 转至匹配的括号 Ctrl+Shift+P 全局 转至上一个编辑位置 Ctrl+Q Java编辑器 转至上一个成员 Ctrl+Shift+↑ Java编辑器 转至下一个成员 Ctrl+Shift+↓ 文本编辑器 转至行 Ctrl+L 2. 搜索 作用域 功能 快捷键 全局 出现在文件中 Ctrl+Shift+U 全局 查找目标文件 ctrl+shift+R 全局 打开搜索对话框 Ctrl+H 全局 工作区中的声明 Ctrl+G 全局 工作区中的引用 Ctrl+Shift+G 工作区域的类 查看某一个类的继承类或者实现类 ctrl+T 3. 文本编辑 作用域 功能 快捷键 文本编辑器 改写切换 Insert 文本编辑器 上滚行 Ctrl+↑ 文本编辑器 下滚行 Ctrl+↓ 4. 文件 作用域 功能 快捷键 全局 保存 Ctrl+S 全局 打印 Ctrl+P 全局 关闭 Ctrl+F4 全局 全部保存 Ctrl+Shift+S 全局 全部关闭 Ctrl+Shift+F4 全局 属性 Alt+Enter 全局 新建 Ctrl+N 5. 项目 作用域 功能 快捷键 全局 全部构建 Ctrl+B 5.1. 源代码 作用域 功能 快捷键 Java编辑器 格式化 Ctrl+Shift+F Java编辑器 添加/取消注释 Ctrl+/ Java编辑器 添加导入 Ctrl+Shift+M Java编辑器 组织导入 Ctrl+Shift+O Java编辑器 使用try/catch块来包围 未设置，太常用了，所以在这里列出，建议自己设置。也可以使用Ctrl+1自动修正。Alt+Shift+z（就可以吧） Java编辑器 将所选区域字母设置为小写 Ctrl+Shift+Y Java编辑器 将所选区域字母设置为大写 Ctrl+Shift+X Java编辑器 方法添加注释 Alt+Shift+J 5.2. 运行 作用域 功能 快捷键 全局 单步返回 F7 全局 单步执行 F6 全局 单步跳入 F5 全局 单步跳入选择 Ctrl+F5 全局 调试上次启动 F11 全局 继续 F8 全局 使用过滤器单步执行 Shift+F5 全局 添加/去除断点 Ctrl+Shift+B 全局 显示 Ctrl+D 全局 运行上次启动 Ctrl+F11 全局 运行至行 Ctrl+R 全局 执行 Ctrl+U 5.3. 重构 作用域 功能 快捷键 全局 撤销重构 Alt+Shift+Z 全局 抽取方法 Alt+Shift+M 全局 抽取局部变量 Alt+Shift+L 全局 内联 Alt+Shift+I 全局 移动 Alt+Shift+V 全局 重命名 Alt+Shift+R 全局 重做 Alt+Shift+Y Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2023-03-06 10:57:30 "},"keymap/chrome-keymap.html":{"url":"keymap/chrome-keymap.html","title":"chrome快捷键","keywords":"","body":"chrome快捷键1. 标签页和窗口快捷键2. Google Chrome 功能快捷键3. 地址栏快捷键4. 网页快捷键5. 鼠标快捷键chrome快捷键 1. 标签页和窗口快捷键 操作 快捷键 打开新窗口 ⌘ + n 在无痕模式下打开新窗口 ⌘ + Shift + n 打开新的标签页，并跳转到该标签页 ⌘ + t 重新打开最后关闭的标签页，并跳转到该标签页 ⌘ + Shift + t 跳转到下一个打开的标签页 ⌘ + Option + 向右箭头键 跳转到上一个打开的标签页 ⌘ + Option + 向左箭头键 跳转到特定标签页 ⌘ + 1 到 ⌘ + 8 顺序切换标签页 Ctrl + Tab 跳转到最后一个标签页 ⌘ + 9 打开当前标签页浏览记录中记录的上一个页面 ⌘ + [ 或 ⌘ + 向左箭头键 打开当前标签页浏览记录中记录的下一个页面 ⌘ + ] 或 ⌘ + 向右箭头键 关闭当前标签页或弹出式窗口 ⌘ + w 关闭当前窗口 ⌘ + Shift + w 最小化窗口 ⌘ + m 隐藏 Google Chrome ⌘ + h 退出 Google Chrome ⌘ + q 2. Google Chrome 功能快捷键 操作 快捷键 显示或隐藏书签栏 ⌘ + Shift + b 打开书签管理器 ⌘ + Option + b 在新标签页中打开“设置”页 ⌘ + , 在新标签页中打开“历史记录”页 ⌘ + y 在新标签页中打开“下载内容”页 ⌘ + Shift + j 打开查找栏搜索当前网页 ⌘ + f 跳转到与查找栏中搜索字词相匹配的下一条内容 ⌘ + g 跳转到与查找栏中搜索字词相匹配的上一条内容 ⌘ + Shift + g 打开查找栏后，搜索选定文本 ⌘ + e 打开“开发者工具” ⌘ + Option + i 打开“清除浏览数据”选项 ⌘ + Shift + Delete 使用其他帐号登录或以访客身份浏览 ⌘ + Shift + m 3. 地址栏快捷键 在地址栏中可使用以下快捷键： 操作 快捷键 使用默认搜索引擎进行搜索 输入搜索字词并按 Enter 键 使用其他搜索引擎进行搜索 输入搜索引擎名称并按 Tab 键 为网站名称添加 www. 和 .com，并在当前标签页中打开该网站 输入网站名称并按 Control + Enter 键 为网站名称添加 www. 和 .com，并在新标签页中打开该网站 输入网站名称并按 Control + Shift + Enter 键 在新的后台标签页中打开网站 输入网址并按 ⌘ + Enter 键 跳转到地址栏 ⌘ + l 从地址栏中移除联想查询内容 按向下箭头键以突出显示相应内容，然后按 Shift + fn + Delete 4. 网页快捷键 操作 快捷键 打开选项以打印当前网页 ⌘ + p 打开选项以保存当前网页 ⌘ + s 打开“页面设置”对话框 ⌘ + Option + p 通过电子邮件发送当前网页 ⌘ + Shift + i 重新加载当前网页 ⌘ + r 重新加载当前网页（忽略缓存的内容） ⌘ + Shift + r 停止加载网页 Esc 浏览下一个可点击项 Tab 浏览上一个可点击项 Shift + Tab 使用 Google Chrome 打开计算机中的文件 按住 ⌘ + o 键并选择文件 显示当前网页的 HTML 源代码（不可修改） ⌘ + Option + u 打开 JavaScript 控制台 ⌘ + Option + j 将当前网页保存为书签 ⌘ + d 将所有打开的标签页以书签的形式保存在新文件夹中 ⌘ + Shift + d 开启或关闭全屏模式 ⌘ + Ctrl + f 放大网页上的所有内容 ⌘ 和 + 缩小网页上的所有内容 ⌘ 和 - 将网页上的所有内容恢复到默认大小 ⌘ + 0 向下滚动网页，一次一个屏幕 空格键 向上滚动网页，一次一个屏幕 Shift + 空格键 搜索网络 ⌘ + Option + f 将光标移到文本字段中的上一个字词前面 Option + 向左箭头键 将光标移到文本字段中的上一个字词后面 Option + 向右箭头键 删除文本字段中的上一个字词 Option + Delete 在当前标签页中打开主页 ⌘ + Shift + h 5. 鼠标快捷键 以下快捷键要求您使用鼠标： 操作 快捷键 在当前标签页中打开链接（仅限鼠标） 将链接拖到标签页中 在新的后台标签页中打开链接 按住 ⌘ 键的同时点击链接 打开链接，并跳转到该链接 按住 ⌘ + Shift 键的同时点击链接 打开链接，并跳转到该链接（仅使用鼠标） 将链接拖到标签栏的空白区域 在新窗口中打开链接 按住 Shift 键的同时点击链接 在新窗口中打开标签页（仅使用鼠标） 将标签页拖出标签栏 将标签页移至当前窗口（仅限鼠标） 将标签页拖到现有窗口中 将标签页移回其原始位置 拖动标签页的同时按 Esc 将当前网页保存为书签 将相应网址拖动到书签栏中 下载链接目标 按住 Option 键的同时点击链接 显示浏览记录 右键点击“后退”箭头 或“前进”箭头 ，或者左键点击（并按住鼠标左键不放）“后退”箭头或“前进”箭头 将窗口高度最大化 双击标签栏的空白区域 来自：https://support.google.com/chrome/answer/157179?hl=zh-Hans Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2023-03-06 10:57:30 "},"keymap/tmux-keymap.html":{"url":"keymap/tmux-keymap.html","title":"tmux快捷键","keywords":"","body":"1. 安装tmux2. tmux常用命令2.1. 进入tmux2.2. 退出tmux，程序后台运行2.3. 重回上次tmux窗口2.4. 结束tmux窗口运行的进程3. 更多快捷键说明1. 安装tmux # linux yum install -y tmux # mac brew install tmux 2. tmux常用命令 2.1. 进入tmux tmux 2.2. 退出tmux，程序后台运行 按ctrl + b 进入控制台，再按 d 2.3. 重回上次tmux窗口 ctrl + a 2.4. 结束tmux窗口运行的进程 ctrl + d 3. 更多快捷键说明 类别 快捷键 说明 进入控制台 Ctrl+b 激活控制台；此时以下按键生效 系统操作 ? 列出所有快捷键；按q返回 d 脱离当前会话；这样可以暂时返回Shell界面，输入tmux attach能够重新进入之前的会话 D 选择要脱离的会话；在同时开启了多个会话时使用 Ctrl+z 挂起当前会话 r 强制重绘未脱离的会话 s 选择并切换会话；在同时开启了多个会话时使用 : 进入命令行模式；此时可以输入支持的命令，例如kill-server可以关闭服务器 [ 进入复制模式；此时的操作与vi/emacs相同，按q/Esc退出 ~ 列出提示信息缓存；其中包含了之前tmux返回的各种提示信息 窗口操作 c 创建新窗口 & 关闭当前窗口 数字键 切换至指定窗口 p 切换至上一窗口 n 切换至下一窗口 l 在前后两个窗口间互相切换 w 通过窗口列表切换窗口 , 重命名当前窗口；这样便于识别 . 修改当前窗口编号；相当于窗口重新排序 f 在所有窗口中查找指定文本 面板操作 ” 将当前面板平分为上下两块 % 将当前面板平分为左右两块 x 关闭当前面板 ! 将当前面板置于新窗口；即新建一个窗口，其中仅包含当前面板 Ctrl+方向键 以1个单元格为单位移动边缘以调整当前面板大小 Alt+方向键 以5个单元格为单位移动边缘以调整当前面板大小 Space 在预置的面板布局中循环切换；依次包括even-horizontal、even-vertical、main-horizontal、main-vertical、tiled q 显示面板编号 o 在当前窗口中选择下一面板 方向键 移动光标以选择面板 { 向前置换当前面板 } 向后置换当前面板 Alt+o 逆时针旋转当前窗口的面板 Ctrl+o 顺时针旋转当前窗口的面板 快捷键列表原文：https://blog.csdn.net/hcx25909/article/details/7602935 Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2023-03-06 10:57:30 "},"keymap/iterm2-rzsz.html":{"url":"keymap/iterm2-rzsz.html","title":"iterm2 rzsz的使用","keywords":"","body":"iterm2 rz与sz的功能1. 安装lrzsz2. 安装执行脚本3. 赋予这两个文件可执行权限4. 设置Iterm2的Tirgger特性5. 使用 本文由网络文章整理备份。 iterm2 rz与sz的功能 本文主要介绍mac环境下使用iterm2的rz sz功能的安装流程。 1. 安装lrzsz brew install lrzsz 2. 安装执行脚本 将iterm2-send-zmodem.sh和iterm2-recv-zmodem.sh保存到/usr/local/bin目录下。 iterm2-send-zmodem.sh #!/bin/bash # Author: Matt Mastracci (matthew@mastracci.com) # AppleScript from http://stackoverflow.com/questions/4309087/cancel-button-on-osascript-in-a-bash-script # licensed under cc-wiki with attribution required # Remainder of script public domain osascript -e 'tell application \"iTerm2\" to version' > /dev/null 2>&1 && NAME=iTerm2 || NAME=iTerm if [[ $NAME = \"iTerm\" ]]; then FILE=$(osascript -e 'tell application \"iTerm\" to activate' -e 'tell application \"iTerm\" to set thefile to choose file with prompt \"Choose a file to send\"' -e \"do shell script (\\\"echo \\\"&(quoted form of POSIX path of thefile as Unicode text)&\\\"\\\")\") else FILE=$(osascript -e 'tell application \"iTerm2\" to activate' -e 'tell application \"iTerm2\" to set thefile to choose file with prompt \"Choose a file to send\"' -e \"do shell script (\\\"echo \\\"&(quoted form of POSIX path of thefile as Unicode text)&\\\"\\\")\") fi if [[ $FILE = \"\" ]]; then echo Cancelled. # Send ZModem cancel echo -e \\\\x18\\\\x18\\\\x18\\\\x18\\\\x18 sleep 1 echo echo \\# Cancelled transfer else /usr/local/bin/sz \"$FILE\" --escape --binary --bufsize 4096 sleep 1 echo echo \\# Received \"$FILE\" fi iterm2-recv-zmodem.sh #!/bin/bash # Author: Matt Mastracci (matthew@mastracci.com) # AppleScript from http://stackoverflow.com/questions/4309087/cancel-button-on-osascript-in-a-bash-script # licensed under cc-wiki with attribution required # Remainder of script public domain osascript -e 'tell application \"iTerm2\" to version' > /dev/null 2>&1 && NAME=iTerm2 || NAME=iTerm if [[ $NAME = \"iTerm\" ]]; then FILE=$(osascript -e 'tell application \"iTerm\" to activate' -e 'tell application \"iTerm\" to set thefile to choose folder with prompt \"Choose a folder to place received files in\"' -e \"do shell script (\\\"echo \\\"&(quoted form of POSIX path of thefile as Unicode text)&\\\"\\\")\") else FILE=$(osascript -e 'tell application \"iTerm2\" to activate' -e 'tell application \"iTerm2\" to set thefile to choose folder with prompt \"Choose a folder to place received files in\"' -e \"do shell script (\\\"echo \\\"&(quoted form of POSIX path of thefile as Unicode text)&\\\"\\\")\") fi if [[ $FILE = \"\" ]]; then echo Cancelled. # Send ZModem cancel echo -e \\\\x18\\\\x18\\\\x18\\\\x18\\\\x18 sleep 1 echo echo \\# Cancelled transfer else cd \"$FILE\" /usr/local/bin/rz --rename --escape --binary --bufsize 4096 sleep 1 echo echo echo \\# Sent \\-\\> $FILE fi 3. 赋予这两个文件可执行权限 chmod 777 /usr/local/bin/iterm2-* 4. 设置Iterm2的Tirgger特性 设置Iterm2的Tirgger特性，profiles->default->editProfiles->Advanced中的Tirgger 添加两条trigger，分别设置 Regular expression，Action，Parameters，Instant如下： Regular expression: rz waiting to receive.\\*\\*B0100 Action: Run Silent Coprocess Parameters: /usr/local/bin/iterm2-send-zmodem.sh Instant: checked Regular expression: \\*\\*B00000000000000 Action: Run Silent Coprocess Parameters: /usr/local/bin/iterm2-recv-zmodem.sh Instant: checked 示例图： 5. 使用 上传文件：rz 下载文件：sz + file 参考： https://www.robberphex.com/use-zmodem-at-macos/ https://github.com/RobberPhex/iterm2-zmodem Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2023-03-06 10:57:30 "},"ide/vscode.html":{"url":"ide/vscode.html","title":"vscode配置","keywords":"","body":"1. 本地文件传远程开发2. 远程文件传本地开发1. 本地文件传远程开发 安装sftp插件 创建sftp配置文件 创建.vscode目录，在目录下创建sftp.json文件，内容如下： { \"name\": \"ip\", \"host\": \"ip\", \"protocol\": \"sftp\", \"port\": 22, \"username\": \"root\", \"privateKeyPath\": \"~/.ssh/id_rsa\", \"remotePath\": \"/home/go/src/projectname\", \"uploadOnSave\": true, \"ignore\": [ \".git\", \".vscode\", \".idea\", \".DS_Store\", \"node_modules\" ], \"watcher\": { \"files\": \"/home/go/src/github.com/projectname/*\", \"autoUpload\": true, \"autoDelete\": true } } 2. 远程文件传本地开发 在远程设备上git clone代码，然后在vscode上安装remote的插件，通过remote插件连接远程开发机并打开代码目录。即可进行远程开发。 通过vscode在远程机器上面安装相关的代码插件即可实现代码跳转等操作。 Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2023-03-06 10:57:30 "},"ide/goland.html":{"url":"ide/goland.html","title":"Goland配置","keywords":"","body":"Goland配置引用mod目录索引Goland配置引用mod目录索引 在preferences-Go-Go module下，启用go模块集成，配置环境变量如下，点击应用。 GOPROXY=https://goproxy.cn,direct Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2023-03-06 10:57:30 "},"vim/vim-keymap.html":{"url":"vim/vim-keymap.html","title":"vim命令","keywords":"","body":"1. vi的模式1.1. 普通模式1.2. 编辑模式1.3. 命令模式2. vim命令汇总3. vim命令分类3.1. 基础编辑、移动光标3.2. 操作和重复操作3.3. 复制 和 粘贴3.4. 搜索3.5. 标记 和 宏3.6. 高级移动3.7. 高级指令1. vi的模式 1.1. 普通模式 由Shell进入vi编辑器时，首先进入普通模式。在普通模式下，从键盘输入任何字符都被当作命令来解释。普通模式下没有任何提示符，输入命令后立即执行，不需要回车，而且输入的字符不会在屏幕上显示出来。 1.2. 编辑模式 编辑模式主要用于文本的编辑。该模式下用户输入的任何字符都被作为文件的内容保存起来，并在屏幕上显示出来。 1.3. 命令模式 命令模式下，用户可以对文件进行一些高级处理。尽管普通模式下的命令可以完成很多功能，但要执行一些如字符串查找、替换、显示行号等操作还是必须要进入命令模式。 也有文章称为两种工作模式，即把命令模式合并到普通模式。 如果不确定当前处于哪种模式，按两次 Esc 键将回到普通模式。 2. vim命令汇总 高级汇总 3. vim命令分类 3.1. 基础编辑、移动光标 指令 解释 $ 行尾 ^ 行首 w 下一个单词 (词首） e 下一个单词（词尾） b 前一个单词 x del 删除后一个字符 X backspace 删除前一个字符 u 撤销 ctrl + r 重做 k 上 h 下 g 左 l 右 i 插入，开始写东西 s 覆盖 esc 退出输入模式，进入普通模式，可执行各种命令 3.2. 操作和重复操作 指令 解释 f 查找字符，按f后再按需要移动到的字符，光标就会移动到那 f; 就会移动到下一个 ;的位置 F 反向查找字符 . 重复上一个操作 v 选择模式，用上下左右选择文本，按相应的指令直接执行，如：选中后执行 d 就直接删除选中的文本 ctrl + v 块状选择模式，可以纵向选择文本块，而非以行的形式 d 高级删除指令： dw 删除一个单词 df( 配合 f ，删除从光标处到 ( 的字符，单行操作 dd 删除当前行 d2w 删除两个单词 d2t, 删除当前位置到后面第二个 , 之间的内容，不包含 , （t = to） 3.3. 复制 和 粘贴 指令 解释 y 复制 yy 复制当前行 p 粘贴到后面 P 粘贴到前面 o 在当前行的下一行添加空行并开始输入 O 在当前行的上一行添加空行并开始输入 所有经过 d x e 处理的字符串都已经复制到了粘贴板上。 3.4. 搜索 指令 解释 / 从当前位置向后搜索 ？ 从当前位置后前搜索 n 搜索完之后，如果有多个结果，跳到 下一个匹 配项 N 跳到 上一个 匹配项 * 直接匹配当前光标下面的字符串，移到下一个匹配项，跟/ ? 没有关系 # 上一个匹配项 3.5. 标记 和 宏 标记 m 后跟 a - z 任意字符来设置一个标记 ` 后跟 字符来跳到这个标记点 大写 A - Z 是全局的，小写 a - z '. 代表最后编辑位置 宏 q 后接 a - z 开始录制宏 q 结束宏的录制 @ 后接 a - z 读取宏 @@ 代表最后一个宏 3.6. 高级移动 % 在配对的 () [] 之间移动 H M L 移动到编辑器可视范围的头部，中间，尾部 G 到文件的尾部，前面添加数字再按 G 跳到输入的行，写行号的时候是看不见的 - + 跳到上一行，下一行 ( ) 跳到当前句子的 首 / 尾 { } 跳到 前一个 / 后一个 空行 [[ jumps to the previous { in column 0 ]] jumps to the next } column 0 3.7. 高级指令 J 合并当前行与下一行。合并已选中的所有行。 r 替换当前字符到下一个输入的字符。如： r 后接 4 会把当前字符替换成 4 C 是 c$ 的缩写：修改从光标到结尾 D 是 d$ 的缩写：删除从光标到结尾 Y 是 yy 的缩写：复制当前行 s 删除光标下字符，并开始编辑 S 删除当前行，并开始编辑 向前缩进，一行，或多行，范围设置在前面提到了，t等等 > 向后缩进，一行，或多行 = 格式化，一行，或多行 ~ 切换光标下的字符大小写 参考： 本文由以下文章整理得 http://www.viemu.com/a_vi_vim_graphical_cheat_sheet_tutorial.html https://segmentfault.com/a/1190000016056004#articleHeader15 Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2023-03-06 10:57:30 "},"vim/vimrc-cn.html":{"url":"vim/vimrc-cn.html","title":"vimrc配置","keywords":"","body":"vimrc 中文版vimrc 中文版 由 https://blog.51cto.com/zpf666/2335640 转载 \"~/.vimrc \"vim config file \"date 2018-12-26 \"Created by bert \"blog:https://blog.51cto.com/zpf666 \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \"\"\"=>全局配置字体和颜色代码折叠功能 @=((foldclosed(line('.')) \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \"\"\"=>文字处理>命令移动时的宽度为4\" set shiftwidth=4 \"使得按退格键时可以一次删除4个空格\" set softtabstop=4 set smarttab \"缩进，自动缩进（继承前一行的缩进）\" \"set autoindent 命令打开自动缩进，是下面配置的缩写 \"可使用autoindent命令的简写，即“:set ai”和“:set noai” \"还可以使用“:set ai sw=4”在一个命令中打开缩进并设置缩进级别 set ai set cindent \"智能缩进\" set si \"自动换行” set wrap \"设置软宽度\" set sw=4 \"行内替换\" set gdefault \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \"\"\"=>Vim 界面,h,l \"设置魔术\" set magic \"关闭遇到错误时的声音提示\" \"关闭错误信息响铃\" set noerrorbells \"关闭使用可视响铃代替呼叫\" set novisualbell \"高亮显示匹配的括号([{和}])\" set showmatch \"匹配括号高亮的时间（单位是十分之一秒）\" set mat=2 \"光标移动到buffer的顶部和底部时保持3行距离\" set scrolloff=3 \"搜索逐字符高亮\" set hlsearch set incsearch \"搜索时不区分大小写\" \"还可以使用简写（“:set ic”和“:set noic”）\" set ignorecase \"用浅色高亮显示当前行\" autocmd InsertLeave * se nocul autocmd InsertEnter * se cul \"输入的命令显示出来，看的清楚\" set showcmd \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \"\"\"=>编码设置其他设置 \"设置背景颜色\" set background=dark \"文件类型自动检测，代码智能补全\" set completeopt=longest,preview,menu \"共享剪切板\" set clipboard+=unnamed \"从不备份\" set nobackup set noswapfile \"自动保存\" set autowrite \"显示中文帮助\" if version >= 603 set helplang=cn set encoding=utf-8 endif \"设置高亮相关项\" highlight Search ctermbg=black ctermfg=white guifg=white guibg=black \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \"\"\"=>在shell脚本开头自动增加解释器以及作者等版权信息 Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2023-03-06 10:57:30 "},"vim/basic-vimrc.html":{"url":"vim/basic-vimrc.html","title":"basic vimrc","keywords":"","body":"basic vimrc 以下转自https://github.com/amix/vimrc/blob/master/vimrcs/basic.vim basic vimrc \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \" Maintainer: \" Amir Salihefendic — @amix3k \" \" Awesome_version: \" Get this config, nice color schemes and lots of plugins! \" \" Install the awesome version from: \" \" https://github.com/amix/vimrc \" \" Sections: \" -> General \" -> VIM user interface \" -> Colors and Fonts \" -> Files and backups \" -> Text, tab and indent related \" -> Visual mode related \" -> Moving around, tabs and buffers \" -> Status line \" -> Editing mappings \" -> vimgrep searching and cope displaying \" -> Spell checking \" -> Misc \" -> Helper functions \" \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \" => General \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \" Sets how many lines of history VIM has to remember set history=500 \" Enable filetype plugins filetype plugin on filetype indent on \" Set to auto read when a file is changed from the outside set autoread \" With a map leader it's possible to do extra key combinations \" like w saves the current file let mapleader = \",\" \" Fast saving nmap w :w! \" :W sudo saves the file \" (useful for handling the permission-denied error) command W w !sudo tee % > /dev/null \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \" => VIM user interface \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \" Set 7 lines to the cursor - when moving vertically using j/k set so=7 \" Avoid garbled characters in Chinese language windows OS let $LANG='en' set langmenu=en source $VIMRUNTIME/delmenu.vim source $VIMRUNTIME/menu.vim \" Turn on the Wild menu set wildmenu \" Ignore compiled files set wildignore=*.o,*~,*.pyc if has(\"win16\") || has(\"win32\") set wildignore+=.git\\*,.hg\\*,.svn\\* else set wildignore+=*/.git/*,*/.hg/*,*/.svn/*,*/.DS_Store endif \"Always show current position set ruler \" Height of the command bar set cmdheight=2 \" A buffer becomes hidden when it is abandoned set hid \" Configure backspace so it acts as it should act set backspace=eol,start,indent set whichwrap+=,h,l \" Ignore case when searching set ignorecase \" When searching try to be smart about cases set smartcase \" Highlight search results set hlsearch \" Makes search act like search in modern browsers set incsearch \" Don't redraw while executing macros (good performance config) set lazyredraw \" For regular expressions turn magic on set magic \" Show matching brackets when text indicator is over them set showmatch \" How many tenths of a second to blink when matching brackets set mat=2 \" No annoying sound on errors set noerrorbells set novisualbell set t_vb= set tm=500 \" Properly disable sound on errors on MacVim if has(\"gui_macvim\") autocmd GUIEnter * set vb t_vb= endif \" Add a bit extra margin to the left set foldcolumn=1 \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \" => Colors and Fonts \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \" Enable syntax highlighting syntax enable \" Enable 256 colors palette in Gnome Terminal if $COLORTERM == 'gnome-terminal' set t_Co=256 endif try colorscheme desert catch endtry set background=dark \" Set extra options when running in GUI mode if has(\"gui_running\") set guioptions-=T set guioptions-=e set t_Co=256 set guitablabel=%M\\ %t endif \" Set utf8 as standard encoding and en_US as the standard language set encoding=utf8 \" Use Unix as the standard file type set ffs=unix,dos,mac \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \" => Files, backups and undo \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \" Turn backup off, since most stuff is in SVN, git et.c anyway... set nobackup set nowb set noswapfile \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \" => Text, tab and indent related \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \" Use spaces instead of tabs set expandtab \" Be smart when using tabs ;) set smarttab \" 1 tab == 4 spaces set shiftwidth=4 set tabstop=4 \" Linebreak on 500 characters set lbr set tw=500 set ai \"Auto indent set si \"Smart indent set wrap \"Wrap lines \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \" => Visual mode related \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \" Visual mode pressing * or # searches for the current selection \" Super useful! From an idea by Michael Naumann vnoremap * :call VisualSelection('', '')/=@/ vnoremap # :call VisualSelection('', '')?=@/ \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \" => Moving around, tabs, windows and buffers \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \" Map to / (search) and Ctrl- to ? (backwards search) map / map ? \" Disable highlight when is pressed map :noh \" Smart way to move between windows map j map k map h map l \" Close the current buffer map bd :Bclose:tabclosegT \" Close all the buffers map ba :bufdo bd map l :bnext map h :bprevious \" Useful mappings for managing tabs map tn :tabnew map to :tabonly map tc :tabclose map tm :tabmove map t :tabnext \" Let 'tl' toggle between this and the last accessed tab let g:lasttab = 1 nmap tl :exe \"tabn \".g:lasttab au TabLeave * let g:lasttab = tabpagenr() \" Opens a new tab with the current buffer's path \" Super useful when editing files in the same directory map te :tabedit =expand(\"%:p:h\")/ \" Switch CWD to the directory of the open buffer map cd :cd %:p:h:pwd \" Specify the behavior when switching between buffers try set switchbuf=useopen,usetab,newtab set stal=2 catch endtry \" Return to last edit position when opening files (You want this!) au BufReadPost * if line(\"'\\\"\") > 1 && line(\"'\\\"\") Status line \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \" Always show the status line set laststatus=2 \" Format the status line set statusline=\\ %{HasPaste()}%F%m%r%h\\ %w\\ \\ CWD:\\ %r%{getcwd()}%h\\ \\ \\ Line:\\ %l\\ \\ Column:\\ %c \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \" => Editing mappings \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \" Remap VIM 0 to first non-blank character map 0 ^ \" Move a line of text using ALT+[jk] or Command+[jk] on mac nmap mz:m+`z nmap mz:m-2`z vmap :m'>+`mzgv`yo`z vmap :m'`>my` nmap vmap vmap endif \" Delete trailing white space on save, useful for some filetypes ;) fun! CleanExtraSpaces() let save_cursor = getpos(\".\") let old_query = getreg('/') silent! %s/\\s\\+$//e call setpos('.', save_cursor) call setreg('/', old_query) endfun if has(\"autocmd\") autocmd BufWritePre *.txt,*.js,*.py,*.wiki,*.sh,*.coffee :call CleanExtraSpaces() endif \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \" => Spell checking \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \" Pressing ,ss will toggle and untoggle spell checking map ss :setlocal spell! \" Shortcuts using map sn ]s map sp [s map sa zg map s? z= \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \" => Misc \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \" Remove the Windows ^M - when the encodings gets messed up noremap m mmHmt:%s///ge'tzt'm \" Quickly open a buffer for scribble map q :e ~/buffer \" Quickly open a markdown buffer for scribble map x :e ~/buffer.md \" Toggle paste mode on and off map pp :setlocal paste! \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \" => Helper functions \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \" Returns true if paste mode is enabled function! HasPaste() if &paste return 'PASTE MODE ' endif return '' endfunction \" Don't close window, when deleting a buffer command! Bclose call BufcloseCloseIt() function! BufcloseCloseIt() let l:currentBufNum = bufnr(\"%\") let l:alternateBufNum = bufnr(\"#\") if buflisted(l:alternateBufNum) buffer # else bnext endif if bufnr(\"%\") == l:currentBufNum new endif if buflisted(l:currentBufNum) execute(\"bdelete! \".l:currentBufNum) endif endfunction function! CmdLine(str) call feedkeys(\":\" . a:str) endfunction function! VisualSelection(direction, extra_filter) range let l:saved_reg = @\" execute \"normal! vgvy\" let l:pattern = escape(@\", \"\\\\/.*'$^~[]\") let l:pattern = substitute(l:pattern, \"\\n$\", \"\", \"\") if a:direction == 'gv' call CmdLine(\"Ack '\" . l:pattern . \"' \" ) elseif a:direction == 'replace' call CmdLine(\"%s\" . '/'. l:pattern . '/') endif let @/ = l:pattern let @\" = l:saved_reg endfunction 参考 https://github.com/amix/vimrc Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2023-03-06 10:57:30 "}}